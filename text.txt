1
c h a p t e r
introduction
an operating system is a program that manages a computers hardware. it
also provides a basis for application programs and acts as an intermediary
between the computer user and the computer hardware. an amazing aspect of
operating systems is how they vary in accomplishing these tasks. mainframe
operating systems are designed primarily to optimize utilization of hardware.
personal computer (pc) operating systems support complex games, business
applications, and everything in between. operating systems for mobile com-
puters provide an environment in which a user can easily interface with the
computer to execute programs. thus, some operating systems are designed to
be convenient, others to be efcient, and others to be some combination of the
two.
before we can explore the details of computer system operation, we need to
know something about system structure. we thus discuss the basic functions
of system startup, i/o, and storage early in this chapter. we also describe
the basic computer architecture that makes it possible to write a functional
operating system.
because an operating system is large and complex, it must be created
piece by piece. each of these pieces should be a well-delineated portion of the
system, with carefully dened inputs, outputs, and functions. in this chapter,
we provide a general overview of the major components of a contemporary
computer system as well as the functions provided by the operating system.
additionally, we cover several other topics to help set the stage for the
remainder of this text: data structures used in operating systems, computing
environments, and open-source operating systems.
chapter objectives
to describe the basic organization of computer systems.
to provide a grand tour of the major components of operating systems.
to give an overview of the many types of computing environments.
to explore several open-source operating systems.
3
4
chapter 1
introduction
user
1
user
2
user
3
computer hardware
operating system
system and application programs
compiler
assembler
text editor
database
system
user
n
figure 1.1
abstract view of the components of a computer system.
1.1
what operating systems do
we begin our discussion by looking at the operating systems role in the
overall computer system. a computer system can be divided roughly into four
components: the hardware, the operating system, the application programs,
and the users (figure 1.1).
the hardwarethe central processing unit (cpu), the memory, and the
input/output (i/o) devicesprovides the basic computing resources for the
system. the application programssuch as word processors, spreadsheets,
compilers, and web browsersdene the ways in which these resources are
used to solve users computing problems. the operating system controls the
hardware and coordinates its use among the various application programs for
the various users.
we can also view a computer system as consisting of hardware, software,
and data. the operating system provides the means for proper use of these
resources in the operation of the computer system. an operating system is
similar to a government. like a government, it performs no useful function by
itself. it simply provides an environment within which other programs can do
useful work.
to understand more fully the operating systems role, we next explore
operating systems from two viewpoints: that of the user and that of the system.
1.1.1
user view
the users view of the computer varies according to the interface being
used. most computer users sit in front of a pc, consisting of a monitor,
keyboard, mouse, and system unit. such a system is designed for one user
1.1
what operating systems do
5
to monopolize its resources. the goal is to maximize the work (or play) that
the user is performing. in this case, the operating system is designed mostly
for ease of use, with some attention paid to performance and none paid
to resource utilizationhow various hardware and software resources are
shared. performance is, of course, important to the user; but such systems
are optimized for the single-user experience rather than the requirements of
multiple users.
in other cases, a user sits at a terminal connected to a mainframe or a
minicomputer. other users are accessing the same computer through other
terminals. these users share resources and may exchange information. the
operating system in such cases is designed to maximize resource utilization
to assure that all available cpu time, memory, and i/o are used efciently and
that no individual user takes more than her fair share.
in still other cases, users sit at workstations connected to networks of
other workstations and servers. these users have dedicated resources at
their disposal, but they also share resources such as networking and servers,
including le, compute, and print servers. therefore, their operating system is
designed to compromise between individual usability and resource utilization.
recently, many varieties of mobile computers, such as smartphones and
tablets, have come into fashion. most mobile computers are standalone units for
individual users. quite often, they are connected to networks through cellular
or other wireless technologies. increasingly, these mobile devices are replacing
desktop and laptop computers for people who are primarily interested in
using computers for e-mail and web browsing. the user interface for mobile
computers generally features a touch screen, where the user interacts with the
system by pressing and swiping ngers across the screen rather than using a
physical keyboard and mouse.
some computers have little or no user view. for example, embedded
computers in home devices and automobiles may have numeric keypads and
may turn indicator lights on or off to show status, but they and their operating
systems are designed primarily to run without user intervention.
1.1.2
system view
from the computers point of view, the operating system is the program
most intimately involved with the hardware. in this context, we can view
an operating system as a resource allocator. a computer system has many
resources that may be required to solve a problem: cpu time, memory space,
le-storage space, i/o devices, and so on. the operating system acts as the
manager of these resources. facing numerous and possibly conicting requests
for resources, the operating system must decide how to allocate them to specic
programs and users so that it can operate the computer system efciently and
fairly. as we have seen, resource allocation is especially important where many
users access the same mainframe or minicomputer.
a slightly different view of an operating system emphasizes the need to
control the various i/o devices and user programs. an operating system is a
control program. a control program manages the execution of user programs
to prevent errors and improper use of the computer. it is especially concerned
with the operation and control of i/o devices.
6
chapter 1
introduction
1.1.3
dening operating systems
by now, you can probably see that the term operating system covers many roles
and functions. that is the case, at least in part, because of the myriad designs
and uses of computers. computers are present within toasters, cars, ships,
spacecraft, homes, and businesses. they are the basis for game machines, music
players, cable tv tuners, and industrial control systems. although computers
have a relatively short history, they have evolved rapidly. computing started
as an experiment to determine what could be done and quickly moved to
xed-purpose systems for military uses, such as code breaking and trajectory
plotting, and governmental uses, such as census calculation. those early
computers evolved into general-purpose, multifunction mainframes, and
thats when operating systems were born. in the 1960s, moores law predicted
that the number of transistors on an integrated circuit would double every
eighteen months, and that prediction has held true. computers gained in
functionality and shrunk in size, leading to a vast number of uses and a vast
number and variety of operating systems. (see chapter 20 for more details on
the history of operating systems.)
how, then, can we dene what an operating system is? in general, we have
no completely adequate denition of an operating system. operating systems
exist because they offer a reasonable way to solve the problem of creating a
usable computing system. the fundamental goal of computer systems is to
execute user programs and to make solving user problems easier. computer
hardware is constructed toward this goal. since bare hardware alone is not
particularly easy to use, application programs are developed. these programs
require certain common operations, such as those controlling the i/o devices.
the common functions of controlling and allocating resources are then brought
together into one piece of software: the operating system.
in addition, we have no universally accepted denition of what is part of the
operating system. a simple viewpoint is that it includes everything a vendor
ships when you order the operating system. the features included, however,
vary greatly across systems. some systems take up less than a megabyte of
space and lack even a full-screen editor, whereas others require gigabytes of
space and are based entirely on graphical windowing systems. a more common
denition, and the one that we usually follow, is that the operating system
is the one program running at all times on the computerusually called
the kernel. (along with the kernel, there are two other types of programs:
system programs, which are associated with the operating system but are not
necessarily part of the kernel, and application programs, which include all
programs not associated with the operation of the system.)
the matter of what constitutes an operating system became increasingly
important as personal computers became more widespread and operating
systems grew increasingly sophisticated. in 1998, the united states department
of justice led suit against microsoft, in essence claiming that microsoft
included too much functionality in its operating systems and thus prevented
application vendors from competing. (for example, a web browser was an
integral part of the operating systems.) as a result, microsoft was found guilty
of using its operating-system monopoly to limit competition.
today, however, if we look at operating systems for mobile devices, we
see that once again the number of features constituting the operating system
1.2
computer-system organization
7
is increasing. mobile operating systems often include not only a core kernel
but also middlewarea set of software frameworks that provide additional
services to application developers. for example, each of the two most promi-
nent mobile operating systemsapples ios and googles androidfeatures
a core kernel along with middleware that supports databases, multimedia, and
graphics (to name a only few).
1.2
computer-system organization
before we can explore the details of how computer systems operate, we need
general knowledge of the structure of a computer system. in this section,
we look at several parts of this structure. the section is mostly concerned
with computer-system organization, so you can skim or skip it if you already
understand the concepts.
1.2.1
computer-system operation
a modern general-purpose computer system consists of one or more cpus
and a number of device controllers connected through a common bus that
provides access to shared memory (figure 1.2). each device controller is in
charge of a specic type of device (for example, disk drives, audio devices,
or video displays). the cpu and the device controllers can execute in parallel,
competing for memory cycles. to ensure orderly access to the shared memory,
a memory controller synchronizes access to the memory.
for a computer to start runningfor instance, when it is powered up or
rebootedit needs to have an initial program to run. this initial program,
or bootstrap program, tends to be simple. typically, it is stored within
the computer hardware in read-only memory (rom) or electrically erasable
programmable read-only memory (eeprom), known by the general term
rmware. it initializes all aspects of the system, from cpu registers to device
controllers to memory contents. the bootstrap program must know how to load
the operating system and how to start executing that system. to accomplish
usb controller
keyboard
printer
mouse
monitor
disks
graphics
adapter
disk
controller
memory
cpu
on-line
figure 1.2
a modern computer system.
8
chapter 1
introduction
user
process
executing
cpu
i/o interrupt
processing
i/o
request
transfer
done
i/o
request
transfer
done
i/o
device
idle
transferring
figure 1.3
interrupt timeline for a single process doing output.
this goal, the bootstrap program must locate the operating-system kernel and
load it into memory.
once the kernel is loaded and executing, it can start providing services to
the system and its users. some services are provided outside of the kernel, by
system programs that are loaded into memory at boot time to become system
processes, or system daemons that run the entire time the kernel is running.
on unix, the rst system process is init, and it starts many other daemons.
once this phase is complete, the system is fully booted, and the system waits
for some event to occur.
the occurrence of an event is usually signaled by an interrupt from either
the hardware or the software. hardware may trigger an interrupt at any time
by sending a signal to the cpu, usually by way of the system bus. software
may trigger an interrupt by executing a special operation called a system call
(also called a monitor call).
when the cpu is interrupted, it stops what it is doing and immediately
transfers execution to a xed location. the xed location usually contains
the starting address where the service routine for the interrupt is located.
the interrupt service routine executes; on completion, the cpu resumes the
interrupted computation. a timeline of this operation is shown in figure 1.3.
interrupts are an important part of a computer architecture. each computer
design has its own interrupt mechanism, but several functions are common.
the interrupt must transfer control to the appropriate interrupt service routine.
the straightforward method for handling this transfer would be to invoke
a generic routine to examine the interrupt information. the routine, in turn,
would call the interrupt-specic handler. however, interrupts must be handled
quickly. since only a predened number of interrupts is possible, a table of
pointers to interrupt routines can be used instead to provide the necessary
speed. the interrupt routine is called indirectly through the table, with no
intermediate routine needed. generally, the table of pointers is stored in low
memory (the rst hundred or so locations). these locations hold the addresses
of the interrupt service routines for the various devices. this array, or interrupt
vector, of addresses is then indexed by a unique device number, given with
the interrupt request, to provide the address of the interrupt service routine for
1.2
computer-system organization
9
storage definitions and notation
the basic unit of computer storage is the bit. a bit can contain one of two
values, 0 and 1. all other storage in a computer is based on collections of bits.
given enough bits, it is amazing how many things a computer can represent:
numbers, letters, images, movies, sounds, documents, and programs, to name
a few. a byte is 8 bits, and on most computers it is the smallest convenient
chunk of storage. for example, most computers dont have an instruction to
move a bit but do have one to move a byte. a less common term is word,
which is a given computer architectures native unit of data. a word is made
up of one or more bytes. for example, a computer that has 64-bit registers and
64-bit memory addressing typically has 64-bit (8-byte) words. a computer
executes many operations in its native word size rather than a byte at a time.
computer storage, along with most computer throughput, is generally
measured and manipulated in bytes and collections of bytes. a kilobyte, or
kb, is 1,024 bytes; a megabyte, or mb, is 1,0242 bytes; a gigabyte, or gb, is
1,0243 bytes; a terabyte, or tb, is 1,0244 bytes; and a petabyte, or pb, is 1,0245
bytes. computer manufacturers often round off these numbers and say that
a megabyte is 1 million bytes and a gigabyte is 1 billion bytes. networking
measurements are an exception to this general rule; they are given in bits
(because networks move data a bit at a time).
the interrupting device. operating systems as different as windows and unix
dispatch interrupts in this manner.
the interrupt architecture must also save the address of the interrupted
instruction. many old designs simply stored the interrupt address in a
xed location or in a location indexed by the device number. more recent
architectures store the return address on the system stack. if the interrupt
routine needs to modify the processor statefor instance, by modifying
register valuesit must explicitly save the current state and then restore that
state before returning. after the interrupt is serviced, the saved return address
is loaded into the program counter, and the interrupted computation resumes
as though the interrupt had not occurred.
1.2.2
storage structure
the cpu can load instructions only from memory, so any programs to run must
be stored there. general-purpose computers run most of their programs from
rewritable memory, called main memory (also called random-access memory,
or ram). main memory commonly is implemented in a semiconductor
technology called dynamic random-access memory (dram).
computers use other forms of memory as well. we have already mentioned
read-only memory, rom) and electrically erasable programmable read-only
memory, eeprom). because rom cannot be changed, only static programs, such
as the bootstrap program described earlier, are stored there. the immutability
of rom is of use in game cartridges. eeprom can be changed but cannot
be changed frequently and so contains mostly static programs. for example,
smartphones have eeprom to store their factory-installed programs.
10
chapter 1
introduction
all forms of memory provide an array of bytes. each byte has its
own address. interaction is achieved through a sequence of load or store
instructions to specic memory addresses. the load instruction moves a byte
or word from main memory to an internal register within the cpu, whereas the
store instruction moves the content of a register to main memory. aside from
explicit loads and stores, the cpu automatically loads instructions from main
memory for execution.
a typical instructionexecution cycle, as executed on a system with a von
neumann architecture, rst fetches an instruction from memory and stores
that instruction in the instruction register. the instruction is then decoded
and may cause operands to be fetched from memory and stored in some
internal register. after the instruction on the operands has been executed, the
result may be stored back in memory. notice that the memory unit sees only
a stream of memory addresses. it does not know how they are generated (by
the instruction counter, indexing, indirection, literal addresses, or some other
means) or what they are for (instructions or data). accordingly, we can ignore
how a memory address is generated by a program. we are interested only in
the sequence of memory addresses generated by the running program.
ideally, we want the programs and data to reside in main memory
permanently. this arrangement usually is not possible for the following two
reasons:
1. main memory is usually too small to store all needed programs and data
permanently.
2. main memory is a volatile storage device that loses its contents when
power is turned off or otherwise lost.
thus, most computer systems provide secondary storage as an extension of
main memory. the main requirement for secondary storage is that it be able to
hold large quantities of data permanently.
the most common secondary-storage device is a magnetic disk, which
provides storage for both programs and data. most programs (system and
application) are stored on a disk until they are loaded into memory. many
programs then use the disk as both the source and the destination of their
processing. hence, the proper management of disk storage is of central
importance to a computer system, as we discuss in chapter 10.
in a larger sense, however, the storage structure that we have described
consisting of registers, main memory, and magnetic disksis only one of many
possible storage systems. others include cache memory, cd-rom, magnetic
tapes, and so on. each storage system provides the basic functions of storing
a datum and holding that datum until it is retrieved at a later time. the main
differences among the various storage systems lie in speed, cost, size, and
volatility.
the wide variety of storage systems can be organized in a hierarchy (figure
1.4) according to speed and cost. the higher levels are expensive, but they are
fast. as we move down the hierarchy, the cost per bit generally decreases,
whereas the access time generally increases. this trade-off is reasonable; if a
given storage system were both faster and less expensive than anotherother
properties being the samethen there would be no reason to use the slower,
more expensive memory. in fact, many early storage devices, including paper
1.2
computer-system organization
11
registers
cache
main memory
solid-state disk
magnetic disk
optical disk
magnetic tapes
figure 1.4
storage-device hierarchy.
tape and core memories, are relegated to museums now that magnetic tape and
semiconductor memory have become faster and cheaper. the top four levels
of memory in figure 1.4 may be constructed using semiconductor memory.
in addition to differing in speed and cost, the various storage systems are
either volatile or nonvolatile. as mentioned earlier, volatile storage loses its
contents when the power to the device is removed. in the absence of expensive
battery and generator backup systems, data must be written to nonvolatile
storage for safekeeping. in the hierarchy shown in figure 1.4, the storage
systems above the solid-state disk are volatile, whereas those including the
solid-state disk and below are nonvolatile.
solid-state disks have several variants but in general are faster than
magnetic disks and are nonvolatile. one type of solid-state disk stores data in a
large dram array during normal operation but also contains a hidden magnetic
hard disk and a battery for backup power. if external power is interrupted, this
solid-state disks controller copies the data from ram to the magnetic disk.
when external power is restored, the controller copies the data back into ram.
another form of solid-state disk is ash memory, which is popular in cameras
and personal digital assistants (pdas), in robots, and increasingly for storage
on general-purpose computers. flash memory is slower than dram but needs
no power to retain its contents. another form of nonvolatile storage is nvram,
which is dram with battery backup power. this memory can be as fast as
dram and (as long as the battery lasts) is nonvolatile.
the design of a complete memory system must balance all the factors just
discussed: it must use only as much expensive memory as necessary while
providing as much inexpensive, nonvolatile memory as possible. caches can
12
chapter 1
introduction
be installed to improve performance where a large disparity in access time or
transfer rate exists between two components.
1.2.3
i/o structure
storage is only one of many types of i/o devices within a computer. a large
portion of operating system code is dedicated to managing i/o, both because
of its importance to the reliability and performance of a system and because of
the varying nature of the devices. next, we provide an overview of i/o.
a general-purpose computer system consists of cpus and multiple device
controllers that are connected through a common bus. each device controller
is in charge of a specic type of device. depending on the controller, more
than one device may be attached. for instance, seven or more devices can be
attached to the small computer-systems interface (scsi) controller. a device
controller maintains some local buffer storage and a set of special-purpose
registers. the device controller is responsible for moving the data between
the peripheral devices that it controls and its local buffer storage. typically,
operating systems have a device driver for each device controller. this device
driver understands the device controller and provides the rest of the operating
system with a uniform interface to the device.
to start an i/o operation, the device driver loads the appropriate registers
within the device controller. the device controller, in turn, examines the
contents of these registers to determine what action to take (such as read
a character from the keyboard). the controller starts the transfer of data from
the device to its local buffer. once the transfer of data is complete, the device
controller informs the device driver via an interrupt that it has nished its
operation. the device driver then returns control to the operating system,
possibly returning the data or a pointer to the data if the operation was a read.
for other operations, the device driver returns status information.
this form of interrupt-driven i/o is ne for moving small amounts of data
but can produce high overhead when used for bulk data movement such as disk
i/o. to solve this problem, direct memory access (dma) is used. after setting
up buffers, pointers, and counters for the i/o device, the device controller
transfers an entire block of data directly to or from its own buffer storage to
memory, with no intervention by the cpu. only one interrupt is generated per
block, to tell the device driver that the operation has completed, rather than
the one interrupt per byte generated for low-speed devices. while the device
controller is performing these operations, the cpu is available to accomplish
other work.
some high-end systems use switch rather than bus architecture. on these
systems, multiple components can talk to other components concurrently,
rather than competing for cycles on a shared bus. in this case, dma is even
more effective. figure 1.5 shows the interplay of all components of a computer
system.
1.3
computer-system architecture
in section 1.2, we introduced the general structure of a typical computer system.
a computer system can be organized in a number of different ways, which we
1.3
computer-system architecture
13
thread of execution
instructions
and
data
instruction execution
cycle
data movement
dma
memory
interrupt
cache
data
i/o request
cpu (*n)
device
(*m)
figure 1.5
how a modern computer system works.
can categorize roughly according to the number of general-purpose processors
used.
1.3.1
single-processor systems
until recently, most computer systems used a single processor. on a single-
processor system, there is one main cpucapable of executing a general-purpose
instruction set, including instructions from user processes. almost all single-
processor systems have other special-purpose processors as well. they may
come in the form of device-specic processors, such as disk, keyboard, and
graphics controllers; or, on mainframes, they may come in the form of more
general-purpose processors, such as i/o processors that move data rapidly
among the components of the system.
all of these special-purpose processors run a limited instruction set and
do not run user processes. sometimes, they are managed by the operating
system, in that the operating system sends them information about their next
task and monitors their status. for example, a disk-controller microprocessor
receives a sequence of requests from the main cpu and implements its own disk
queue and scheduling algorithm. this arrangement relieves the main cpu of
the overhead of disk scheduling. pcs contain a microprocessor in the keyboard
to convert the keystrokes into codes to be sent to the cpu. in other systems
or circumstances, special-purpose processors are low-level components built
into the hardware. the operating system cannot communicate with these
processors; they do their jobs autonomously. the use of special-purpose
microprocessors is common and does not turn a single-processor system into
14
chapter 1
introduction
a multiprocessor. if there is only one general-purpose cpu, then the system is
a single-processor system.
1.3.2
multiprocessor systems
within the past several years, multiprocessor systems (also known as parallel
systems or multicore systems) have begun to dominate the landscape of
computing. such systems have two or more processors in close communication,
sharing the computer bus and sometimes the clock, memory, and peripheral
devices. multiprocessor systems rst appeared prominently appeared in
servers and have since migrated to desktop and laptop systems. recently,
multiple processors have appeared on mobile devices such as smartphones
and tablet computers.
multiprocessor systems have three main advantages:
1. increased throughput. by increasing the number of processors, we expect
to get more work done in less time. the speed-up ratio with n processors
is not n, however; rather, it is less than n. when multiple processors
cooperate on a task, a certain amount of overhead is incurred in keeping
all the parts working correctly. this overhead, plus contention for shared
resources, lowers the expected gain from additional processors. similarly,
n programmers working closely together do not produce n times the
amount of work a single programmer would produce.
2. economy of scale. multiprocessor systems can cost less than equivalent
multiple single-processor systems, because they can share peripherals,
mass storage, and power supplies. if several programs operate on the
same set of data, it is cheaper to store those data on one disk and to have
all the processors share them than to have many computers with local
disks and many copies of the data.
3. increased reliability. if functions can be distributed properly among
several processors, then the failure of one processor will not halt the
system, only slow it down. if we have ten processors and one fails, then
each of the remaining nine processors can pick up a share of the work of
the failed processor. thus, the entire system runs only 10 percent slower,
rather than failing altogether.
increased reliability of a computer system is crucial in many applications.
the ability to continue providing service proportional to the level of surviving
hardware is called graceful degradation. some systems go beyond graceful
degradation and are called fault tolerant, because they can suffer a failure of
any single component and still continue operation. fault tolerance requires
a mechanism to allow the failure to be detected, diagnosed, and, if possible,
corrected. the hp nonstop (formerly tandem) system uses both hardware and
software duplication to ensure continued operation despite faults. the system
consists of multiple pairs of cpus, working in lockstep. both processors in the
pair execute each instruction and compare the results. if the results differ, then
one cpu of the pair is at fault, and both are halted. the process that was being
executed is then moved to another pair of cpus, and the instruction that failed
1.3
computer-system architecture
15
is restarted. this solution is expensive, since it involves special hardware and
considerable hardware duplication.
the multiple-processor systems in use today are of two types. some
systems use asymmetric multiprocessing, in which each processor is assigned
a specic task. a boss processor controls the system; the other processors either
look to the boss for instruction or have predened tasks. this scheme denes
a bossworker relationship. the boss processor schedules and allocates work
to the worker processors.
the most common systems use symmetric multiprocessing (smp), in
which each processor performs all tasks within the operating system. smp
means that all processors are peers; no bossworker relationship exists
between processors. figure 1.6 illustrates a typical smp architecture. notice
that each processor has its own set of registers, as well as a privateor local
cache. however, all processors share physical memory. an example of an
smp system is aix, a commercial version of unix designed by ibm. an aix
system can be congured to employ dozens of processors. the benet of this
model is that many processes can run simultaneouslyn processes can run
if there are n cpuswithout causing performance to deteriorate signicantly.
however, we must carefully control i/o to ensure that the data reach the
appropriate processor. also, since the cpus are separate, one may be sitting
idle while another is overloaded, resulting in inefciencies. these inefciencies
can be avoided if the processors share certain data structures. a multiprocessor
system of this form will allow processes and resourcessuch as memory
to be shared dynamically among the various processors and can lower the
variance among the processors. such a system must be written carefully, as
we shall see in chapter 5. virtually all modern operating systemsincluding
windows, mac os x, and linuxnow provide support for smp.
the difference between symmetric and asymmetric multiprocessing may
result from either hardware or software. special hardware can differentiate the
multiple processors, or the software can be written to allow only one boss and
multiple workers. for instance, sun microsystems operating system sunos
version 4 provided asymmetric multiprocessing, whereas version 5 (solaris) is
symmetric on the same hardware.
multiprocessing adds cpus to increase computing power. if the cpu has an
integrated memory controller, then adding cpus can also increase the amount
cpu0
registers
cache
cpu1
registers
cache
cpu2
registers
cache
memory
figure 1.6
symmetric multiprocessing architecture.
16
chapter 1
introduction
of memory addressable in the system. either way, multiprocessing can cause
a system to change its memory access model from uniform memory access
(uma) to non-uniform memory access (numa). uma is dened as the situation
in which access to any ram from any cpu takes the same amount of time. with
numa, some parts of memory may take longer to access than other parts,
creating a performance penalty. operating systems can minimize the numa
penalty through resource management, as discussed in section 9.5.4.
a recent trend in cpu design is to include multiple computing cores
on a single chip. such multiprocessor systems are termed multicore. they
can be more efcient than multiple chips with single cores because on-chip
communication is faster than between-chip communication. in addition, one
chip with multiple cores uses signicantly less power than multiple single-core
chips.
it is important to note that while multicore systems are multiprocessor
systems, not all multiprocessor systems are multicore, as we shall see in section
1.3.3. in our coverage of multiprocessor systems throughout this text, unless
we state otherwise, we generally use the more contemporary term multicore,
which excludes some multiprocessor systems.
in figure 1.7, we show a dual-core design with two cores on the same
chip. in this design, each core has its own register set as well as its own local
cache. other designs might use a shared cache or a combination of local and
shared caches. aside from architectural considerations, such as cache, memory,
and bus contention, these multicore cpus appear to the operating system as
n standard processors. this characteristic puts pressure on operating system
designersand application programmersto make use of those processing
cores.
finally, blade servers are a relativelyrecent development in which multiple
processor boards, i/o boards, and networking boards are placed in the same
chassis. the difference between these and traditional multiprocessor systems
is that each blade-processor board boots independently and runs its own
operating system. some blade-server boards are multiprocessor as well, which
blurs the lines between types of computers. in essence, these servers consist of
multiple independent multiprocessor systems.
cpu core0
registers
cache
cpu core1
registers
cache
memory
figure 1.7
a dual-core design with two cores placed on the same chip.
1.3
computer-system architecture
17
1.3.3
clustered systems
another type of multiprocessor system is a clustered system, which gathers
together multiple cpus. clustered systems differ from the multiprocessor
systems described in section 1.3.2 in that they are composed of two or more
individual systemsor nodesjoined together. such systems are considered
loosely coupled. each node may be a single processor system or a multicore
system. we should note that the denition of clustered is not concrete; many
commercial packages wrestle to dene a clustered system and why one form
is better than another. the generally accepted denition is that clustered
computers share storage and are closely linked via a local-area network lan
(as described in chapter 17) or a faster interconnect, such as inniband.
clustering is usually used to provide high-availability servicethat is,
service will continue even if one or more systems in the cluster fail. generally,
we obtain high availability by adding a level of redundancy in the system.
a layer of cluster software runs on the cluster nodes. each node can monitor
one or more of the others (over the lan). if the monitored machine fails,
the monitoring machine can take ownership of its storage and restart the
applications that were running on the failed machine. the users and clients of
the applications see only a brief interruption of service.
clustering can be structured asymmetrically or symmetrically. in asym-
metric clustering, one machine is in hot-standby mode while the other is
running the applications. the hot-standby host machine does nothing but
monitor the active server. if that server fails, the hot-standby host becomes
the active server. in symmetric clustering, two or more hosts are running
applications and are monitoring each other. this structure is obviously more
efcient, as it uses all of the available hardware. however it does require that
more than one application be available to run.
since a cluster consists of several computer systems connected via a
network, clusters can also be used to provide high-performance computing
environments. such systems can supply signicantly greater computational
power than single-processor or even smp systems because they can run an
application concurrently on all computers in the cluster. the application must
have been written specically to take advantage of the cluster, however. this
involves a technique known as parallelization, which divides a program into
separate components that run in parallel on individual computers in the cluster.
typically, these applications are designed so that once each computing node in
the cluster has solved its portion of the problem, the results from all the nodes
are combined into a nal solution.
other forms of clusters include parallel clusters and clustering over a
wide-area network (wan) (as described in chapter 17). parallel clusters allow
multiple hosts to access the same data on shared storage. because most
operating systems lack support for simultaneous data access by multiple hosts,
parallel clusters usually require the use of special versions of software and
special releases of applications. for example, oracle real application cluster
is a version of oracles database that has been designed to run on a parallel
cluster. each machine runs oracle, and a layer of software tracks access to the
shared disk. each machine has full access to all data in the database. to provide
this shared access, the system must also supply access control and locking to
18
chapter 1
introduction
beowulf clusters
beowulf clusters are designed to solve high-performance computing tasks.
a beowulf cluster consists of commodity hardwaresuch as personal
computersconnected via a simple local-area network. no single specic
software package is required to construct a cluster. rather, the nodes use a
set of open-source software libraries to communicate with one another. thus,
there are a variety of approaches to constructing a beowulf cluster. typically,
though, beowulf computing nodes run the linux operating system. since
beowulf clusters require no special hardware and operate using open-source
software that is available free, they offer a low-cost strategy for building
a high-performance computing cluster. in fact, some beowulf clusters built
from discarded personal computers are using hundreds of nodes to solve
computationally expensive scientic computing problems.
ensure that no conicting operations occur. this function, commonly known
as a distributed lock manager (dlm), is included in some cluster technology.
cluster technology is changing rapidly. some cluster products support
dozens of systems in a cluster, as well as clustered nodes that are separated
by miles. many of these improvements are made possible by storage-area
networks (sans), as described in section 10.3.3, which allow many systems
to attach to a pool of storage. if the applications and their data are stored on
the san, then the cluster software can assign the application to run on any
host that is attached to the san. if the host fails, then any other host can take
over. in a database cluster, dozens of hosts can share the same database, greatly
increasing performance and reliability. figure 1.8 depicts the general structure
of a clustered system.
computer
interconnect
computer
interconnect
computer
storage area
network
figure 1.8
general structure of a clustered system.
1.4
operating-system structure
19
job 1
0
max
operating system
job 2
job 3
job 4
figure 1.9
memory layout for a multiprogramming system.
1.4
operating-system structure
now that we have discussed basic computer-system organization and archi-
tecture, we are ready to talk about operating systems. an operating system
provides the environment within which programs are executed. internally,
operating systems vary greatly in their makeup, since they are organized
along many different lines. there are, however, many commonalities, which
we consider in this section.
one of the most important aspects of operating systems is the ability
to multiprogram. a single program cannot, in general, keep either the cpu
or the i/o devices busy at all times. single users frequently have multiple
programsrunning.multiprogrammingincreases cpuutilizationbyorganizing
jobs (code and data) so that the cpu always has one to execute.
the idea is as follows: the operating system keeps several jobs in memory
simultaneously (figure 1.9). since, in general, main memory is too small to
accommodate all jobs, the jobs are kept initially on the disk in the job pool.
this pool consists of all processes residing on disk awaiting allocation of main
memory.
the set of jobs in memory can be a subset of the jobs kept in the job
pool. the operating system picks and begins to execute one of the jobs in
memory. eventually, the job may have to wait for some task, such as an i/o
operation, to complete. in a non-multiprogrammed system, the cpu would sit
idle. in a multiprogrammed system, the operating system simply switches to,
and executes, another job. when that job needs to wait, the cpu switches to
another job, and so on. eventually, the rst job nishes waiting and gets the
cpu back. as long as at least one job needs to execute, the cpu is never idle.
this idea is common in other life situations. a lawyer does not work for
only one client at a time, for example. while one case is waiting to go to trial
or have papers typed, the lawyer can work on another case. if he has enough
clients, the lawyer will never be idle for lack of work. (idle lawyers tend to
become politicians, so there is a certain social value in keeping lawyers busy.)
20
chapter 1
introduction
multiprogrammed systems provide an environment in which the various
system resources (for example, cpu, memory, and peripheral devices) are
utilized effectively, but they do not provide for user interaction with the
computer system. time sharing (or multitasking) is a logical extension of
multiprogramming. in time-sharing systems, the cpu executes multiple jobs
by switching among them, but the switches occur so frequently that the users
can interact with each program while it is running.
time sharing requires an interactive computer system, which provides
direct communication between the user and the system. the user gives
instructions to the operating system or to a program directly, using a input
device such as a keyboard, mouse, touch pad, or touch screen, and waits for
immediate results on an output device. accordingly, the response time should
be shorttypically less than one second.
a time-shared operating system allows many users to share the computer
simultaneously. since each action or command in a time-shared system tends
to be short, only a little cpu time is needed for each user. as the system switches
rapidly from one user to the next, each user is given the impression that the
entire computer system is dedicated to his use, even though it is being shared
among many users.
a time-shared operating system uses cpu scheduling and multiprogram-
ming to provide each user with a small portion of a time-shared computer.
each user has at least one separate program in memory. a program loaded into
memory and executing is called a process. when a process executes, it typically
executes for only a short time before it either nishes or needs to perform i/o.
i/o may be interactive; that is, output goes to a display for the user, and input
comes from a user keyboard, mouse, or other device. since interactive i/o
typically runs at people speeds, it may take a long time to complete. input,
for example, may be bounded by the users typing speed; seven characters per
second is fast for people but incredibly slow for computers. rather than let
the cpu sit idle as this interactive input takes place, the operating system will
rapidly switch the cpu to the program of some other user.
time sharing and multiprogramming require that several jobs be kept
simultaneously in memory. if several jobs are ready to be brought into memory,
and if there is not enough room for all of them, then the system must choose
among them. making this decision involves job scheduling, which we discuss
in chapter 6. when the operating system selects a job from the job pool, it loads
that job into memory for execution. having several programs in memory at
the same time requires some form of memory management, which we cover in
chapters 8 and 9. in addition, if several jobs are ready to run at the same time,
the system must choose which job will run rst. making this decision is cpu
scheduling, which is also discussed in chapter 6. finally, running multiple
jobs concurrently requires that their ability to affect one another be limited in
all phases of the operating system, including process scheduling, disk storage,
and memory management. we discuss these considerations throughout the
text.
in a time-sharing system, the operating system must ensure reasonable
response time. this goal is sometimes accomplished through swapping,
whereby processes are swapped in and out of main memory to the disk. a more
common method for ensuring reasonable response time is virtual memory, a
technique that allows the execution of a process that is not completely in
1.5
operating-system operations
21
memory (chapter 9). the main advantage of the virtual-memory scheme is that
it enables users to run programs that are larger than actual physical memory.
further, it abstracts main memory into a large, uniform array of storage,
separating logical memory as viewed by the user from physical memory.
this arrangement frees programmers from concern over memory-storage
limitations.
a time-sharing system must also provide a le system (chapters 11 and
12). the le system resides on a collection of disks; hence, disk management
must be provided (chapter 10). in addition, a time-sharing system provides
a mechanism for protecting resources from inappropriate use (chapter 14).
to ensure orderly execution, the system must provide mechanisms for job
synchronization and communication (chapter 5), and it may ensure that jobs
do not get stuck in a deadlock, forever waiting for one another (chapter 7).
1.5
operating-system operations
as mentioned earlier, modern operating systems are interrupt driven. if there
are no processes to execute, no i/o devices to service, and no users to whom
to respond, an operating system will sit quietly, waiting for something to
happen. events are almost always signaled by the occurrence of an interrupt
or a trap. a trap (or an exception) is a software-generated interrupt caused
either by an error (for example, division by zero or invalid memory access)
or by a specic request from a user program that an operating-system service
be performed. the interrupt-driven nature of an operating system denes
that systems general structure. for each type of interrupt, separate segments
of code in the operating system determine what action should be taken. an
interrupt service routine is provided to deal with the interrupt.
since the operating system and the users share the hardware and software
resources of the computer system, we need to make sure that an error in a
user program could cause problems only for the one program running. with
sharing, many processes could be adversely affected by a bug in one program.
for example, if a process gets stuck in an innite loop, this loop could prevent
the correct operation of many other processes. more subtle errors can occur
in a multiprogramming system, where one erroneous program might modify
another program, the data of another program, or even the operating system
itself.
without protection against these sorts of errors, either the computer must
execute only one process at a time or all output must be suspect. a properly
designed operating system must ensure that an incorrect (or malicious)
program cannot cause other programs to execute incorrectly.
1.5.1
dual-mode and multimode operation
in order to ensure the proper execution of the operating system, we must be
able to distinguish between the execution of operating-system code and user-
dened code. the approach taken by most computer systems is to provide
hardware support that allows us to differentiate among various modes of
execution.
22
chapter 1
introduction
user process executing
user process
kernel
calls system call
return from system call
user mode
(mode bit = 1)
trap
mode bit = 0
return
mode bit = 1
kernel mode
(mode bit = 0)
execute system call
figure 1.10
transition from user to kernel mode.
at the very least, we need two separate modes of operation: user mode
and kernel mode (also called supervisor mode, system mode, or privileged
mode). a bit, called the mode bit, is added to the hardware of the computer
to indicate the current mode: kernel (0) or user (1). with the mode bit, we can
distinguish between a task that is executed on behalf of the operating system
and one that is executed on behalf of the user. when the computer system is
executing on behalf of a user application, the system is in user mode. however,
when a user application requests a service from the operating system (via a
system call), the system must transition from user to kernel mode to fulll
the request. this is shown in figure 1.10. as we shall see, this architectural
enhancement is useful for many other aspects of system operation as well.
at system boot time, the hardware starts in kernel mode. the operating
system is then loaded and starts user applications in user mode. whenever a
trap or interrupt occurs, the hardware switches from user mode to kernel mode
(that is, changes the state of the mode bit to 0). thus, whenever the operating
system gains control of the computer, it is in kernel mode. the system always
switches to user mode (by setting the mode bit to 1) before passing control to
a user program.
the dual mode of operation provides us with the means for protecting the
operating system from errant usersand errant users from one another. we
accomplishthisprotectionbydesignatingsome ofthe machine instructionsthat
may cause harm as privileged instructions. the hardware allows privileged
instructions to be executed only in kernel mode. if an attempt is made to
execute a privileged instruction in user mode, the hardware does not execute
the instruction but rather treats it as illegal and traps it to the operating system.
the instruction to switch to kernel mode is an example of a privileged
instruction. some other examples include i/o control, timer management, and
interrupt management. as we shall see throughout the text, there are many
additional privileged instructions.
the concept of modes can be extended beyond two modes (in which case
the cpu uses more than one bit to set and test the mode). cpus that support
virtualization (section 16.1) frequently have a separate mode to indicate when
the virtual machine manager (vmm)and the virtualization management
softwareis in control of the system. in this mode, the vmm has more
privileges than user processes but fewer than the kernel. it needs that level
of privilege so it can create and manage virtual machines, changing the cpu
state to do so. sometimes, too, different modes are used by various kernel
1.5
operating-system operations
23
components. we should note that, as an alternative to modes, the cpu designer
may use other methods to differentiate operational privileges. the intel 64
family of cpus supports four privilege levels, for example, and supports
virtualization but does not have a separate mode for virtualization.
we can now see the life cycle of instruction execution in a computer system.
initial control resides in the operating system, where instructions are executed
in kernel mode. when control is given to a user application, the mode is set to
user mode. eventually, control is switched back to the operating system via an
interrupt, a trap, or a system call.
system calls provide the means for a user program to ask the operating
system to perform tasks reserved for the operating system on the user
programs behalf. a system call is invoked in a variety of ways, depending
on the functionality provided by the underlying processor. in all forms, it is the
method used by a process to request action by the operating system. a system
call usually takes the form of a trap to a specic location in the interrupt vector.
this trap can be executed by a generic trap instruction, although some systems
(such as mips) have a specic syscall instruction to invoke a system call.
when a system call is executed, it is typically treated by the hardware
as a software interrupt. control passes through the interrupt vector to a
service routine in the operating system, and the mode bit is set to kernel
mode. the system-call service routine is a part of the operating system. the
kernel examines the interrupting instruction to determine what system call
has occurred; a parameter indicates what type of service the user program is
requesting. additional information needed for the request may be passed in
registers, on the stack, or in memory (with pointers to the memory locations
passed in registers). the kernel veries that the parameters are correct and
legal, executes the request, and returns control to the instruction following the
system call. we describe system calls more fully in section 2.3.
the lack of a hardware-supported dual mode can cause serious shortcom-
ings in an operating system. for instance, ms-dos was written for the intel
8088 architecture, which has no mode bit and therefore no dual mode. a user
program running awry can wipe out the operating system by writing over it
with data; and multiple programs are able to write to a device at the same
time, with potentially disastrous results. modern versions of the intel cpu
do provide dual-mode operation. accordingly, most contemporary operating
systemssuch as microsoft windows 7, as well as unix and linuxtake
advantage of this dual-mode feature and provide greater protection for the
operating system.
once hardware protection is in place, it detects errors that violate modes.
these errors are normally handled by the operating system. if a user program
fails in some waysuch as by making an attempt either to execute an illegal
instruction or to access memory that is not in the users address spacethen
the hardware traps to the operating system. the trap transfers control through
the interrupt vector to the operating system, just as an interrupt does. when
a program error occurs, the operating system must terminate the program
abnormally. this situation is handled by the same code as a user-requested
abnormal termination. an appropriate error message is given, and the memory
of the program may be dumped. the memory dump is usually written to a
le so that the user or programmer can examine it and perhaps correct it and
restart the program.
24
chapter 1
introduction
1.5.2
timer
we must ensure that the operating system maintains control over the cpu.
we cannot allow a user program to get stuck in an innite loop or to fail
to call system services and never return control to the operating system. to
accomplish this goal, we can use a timer. a timer can be set to interrupt
the computer after a specied period. the period may be xed (for example,
1/60 second) or variable (for example, from 1 millisecond to 1 second). a
variable timer is generally implemented by a xed-rate clock and a counter.
the operating system sets the counter. every time the clock ticks, the counter
is decremented. when the counter reaches 0, an interrupt occurs. for instance,
a 10-bit counter with a 1-millisecond clock allows interrupts at intervals from
1 millisecond to 1,024 milliseconds, in steps of 1 millisecond.
before turning over control to the user, the operating system ensures
that the timer is set to interrupt. if the timer interrupts, control transfers
automatically to the operating system, which may treat the interrupt as a fatal
error or may give the program more time. clearly, instructions that modify the
content of the timer are privileged.
we can use the timer to prevent a user program from running too long.
a simple technique is to initialize a counter with the amount of time that a
program is allowed to run. a program with a 7-minute time limit, for example,
would have its counter initialized to 420. every second, the timer interrupts,
and the counter is decremented by 1. as long as the counter is positive, control
is returned to the user program. when the counter becomes negative, the
operating system terminates the program for exceeding the assigned time
limit.
1.6
process management
a program does nothing unless its instructions are executed by a cpu. a
program in execution, as mentioned, is a process. a time-shared user program
such as a compiler is a process. a word-processing program being run by an
individual user on a pc is a process. a system task, such as sending output
to a printer, can also be a process (or at least part of one). for now, you can
consider a process to be a job or a time-shared program, but later you will learn
that the concept is more general. as we shall see in chapter 3, it is possible
to provide system calls that allow processes to create subprocesses to execute
concurrently.
a process needs certain resourcesincluding cpu time, memory, les,
and i/o devicesto accomplish its task. these resources are either given to
the process when it is created or allocated to it while it is running. in addition
to the various physical and logical resources that a process obtains when it is
created, various initialization data (input) may be passed along. for example,
consider a process whose function is to display the status of a le on the screen
of a terminal. the process will be given the name of the le as an input and will
execute the appropriate instructions and system calls to obtain and display
the desired information on the terminal. when the process terminates, the
operating system will reclaim any reusable resources.
we emphasize that a program by itself is not a process. a program is a
passive entity, like the contents of a le stored on disk, whereas a process
1.7
memory management
25
is an active entity. a single-threaded process has one program counter
specifying the next instruction to execute. (threads are covered in chapter
4.) the execution of such a process must be sequential. the cpu executes one
instruction of the process after another, until the process completes. further,
at any time, one instruction at most is executed on behalf of the process. thus,
although two processes may be associated with the same program, they are
nevertheless considered two separate execution sequences. a multithreaded
process has multiple program counters, each pointing to the next instruction
to execute for a given thread.
a process is the unit of work in a system. a system consists of a collection
of processes, some of which are operating-system processes (those that execute
system code) and the rest of which are user processes (those that execute
user code). all these processes can potentially execute concurrentlyby
multiplexing on a single cpu, for example.
the operating system is responsible for the following activities in connec-
tion with process management:
scheduling processes and threads on the cpus
creating and deleting both user and system processes
suspending and resuming processes
providing mechanisms for process synchronization
providing mechanisms for process communication
we discuss process-management techniques in chapters 3 through 5.
1.7
memory management
as we discussed in section 1.2.2, the main memory is central to the operation
of a modern computer system. main memory is a large array of bytes, ranging
in size from hundreds of thousands to billions. each byte has its own address.
main memory is a repository of quickly accessible data shared by the cpu and
i/o devices. the central processor reads instructions from main memory during
the instruction-fetch cycle and both reads and writes data from main memory
during the data-fetch cycle (on a von neumann architecture). as noted earlier,
the main memory is generally the only large storage device that the cpu is able
to address and access directly. for example, for the cpu to process data from
disk, those data must rst be transferred to main memory by cpu-generated
i/o calls. in the same way, instructions must be in memory for the cpu to
execute them.
for a program to be executed, it must be mapped to absolute addresses and
loaded into memory. as the program executes, it accesses program instructions
and data from memory by generating these absolute addresses. eventually,
the program terminates, its memory space is declared available, and the next
program can be loaded and executed.
to improve both the utilization of the cpu and the speed of the computers
response to its users, general-purpose computers must keep several programs
in memory, creating a need for memory management. many different memory-
26
chapter 1
introduction
management schemes are used. these schemes reect various approaches, and
the effectiveness of any given algorithm depends on the situation. in selecting a
memory-management scheme for a specic system, we must take into account
many factorsespecially the hardware design of the system. each algorithm
requires its own hardware support.
the operating system is responsible for the following activities in connec-
tion with memory management:
keeping track of which parts of memory are currently being used and who
is using them
deciding which processes (or parts of processes) and data to move into
and out of memory
allocating and deallocating memory space as needed
memory-management techniques are discussed in chapters 8 and 9.
1.8
storage management
to make the computer system convenient for users, the operating system
provides a uniform, logical view of information storage. the operating system
abstracts from the physical properties of its storage devices to dene a logical
storage unit, the le. the operating system maps les onto physical media and
accesses these les via the storage devices.
1.8.1
file-system management
file management is one of the most visible components of an operating system.
computers can store information on several different types of physical media.
magnetic disk, optical disk, and magnetic tape are the most common. each
of these media has its own characteristics and physical organization. each
medium is controlled by a device, such as a disk drive or tape drive, that
also has its own unique characteristics. these properties include access speed,
capacity, data-transfer rate, and access method (sequential or random).
a le is a collection of related information dened by its creator. commonly,
les represent programs (both source and object forms) and data. data les may
be numeric, alphabetic, alphanumeric, or binary. files may be free-form (for
example, text les), or they may be formatted rigidly (for example, xed elds).
clearly, the concept of a le is an extremely general one.
the operating system implements the abstract concept of a le by managing
mass-storage media, such as tapes and disks, and the devices that control them.
in addition, les are normally organized into directories to make them easier
to use. finally, when multiple users have access to les, it may be desirable
to control which user may access a le and how that user may access it (for
example, read, write, append).
the operating system is responsible for the following activities in connec-
tion with le management:
creating and deleting les
1.8
storage management
27
creating and deleting directories to organize les
supporting primitives for manipulating les and directories
mapping les onto secondary storage
backing up les on stable (nonvolatile) storage media
file-management techniques are discussed in chapters 11 and 12.
1.8.2
mass-storage management
as we have already seen, because main memory is too small to accommodate
all data and programs, and because the data that it holds are lost when power
is lost, the computer system must provide secondary storage to back up main
memory. most modern computer systems use disks as the principal on-line
storage medium for both programs and data. most programsincluding
compilers, assemblers, word processors, editors, and formattersare stored
on a disk until loaded into memory. they then use the disk as both the source
and destination of their processing. hence, the proper management of disk
storage is of central importance to a computer system. the operating system is
responsible for the following activities in connection with disk management:
free-space management
storage allocation
disk scheduling
because secondary storage is used frequently, it must be used efciently. the
entire speed of operation of a computer may hinge on the speeds of the disk
subsystem and the algorithms that manipulate that subsystem.
there are, however, many uses for storage that is slower and lower in
cost (and sometimes of higher capacity) than secondary storage. backups of
disk data, storage of seldom-used data, and long-term archival storage are
some examples. magnetic tape drives and their tapes and cd and dvd drives
and platters are typical tertiary storage devices. the media (tapes and optical
platters) vary between worm (write-once, read-many-times) and rw (read
write) formats.
tertiary storage is not crucial to system performance, but it still must
be managed. some operating systems take on this task, while others leave
tertiary-storage management to application programs. some of the functions
that operating systems can provide include mounting and unmounting media
in devices, allocating and freeing the devices for exclusive use by processes,
and migrating data from secondary to tertiary storage.
techniques for secondary and tertiary storage management are discussed
in chapter 10.
1.8.3
caching
caching is an important principle of computer systems. heres how it works.
information is normally kept in some storage system (such as main memory).
as it is used, it is copied into a faster storage systemthe cacheon a
28
chapter 1
introduction
temporary basis. when we need a particular piece of information, we rst
check whether it is in the cache. if it is, we use the information directly from
the cache. if it is not, we use the information from the source, putting a copy
in the cache under the assumption that we will need it again soon.
in addition, internal programmable registers, such as index registers,
provide a high-speed cache for main memory. the programmer (or compiler)
implements the register-allocation and register-replacement algorithms to
decide which information to keep in registers and which to keep in main
memory.
other caches are implemented totally in hardware. for instance, most
systems have an instruction cache to hold the instructions expected to be
executed next. without this cache, the cpu would have to wait several cycles
while an instruction was fetched from main memory. for similar reasons, most
systems have one or more high-speed data caches in the memory hierarchy.
we are not concerned with these hardware-only caches in this text, since they
are outside the control of the operating system.
because caches have limited size, cache management is an important
design problem. careful selection of the cache size and of a replacement policy
can result in greatly increased performance. figure 1.11 compares storage
performance in large workstations and small servers. various replacement
algorithms for software-controlled caches are discussed in chapter 9.
main memory can be viewed as a fast cache for secondary storage, since
data in secondary storage must be copied into main memory for use and
data must be in main memory before being moved to secondary storage for
safekeeping. the le-system data, which resides permanently on secondary
storage, may appear on several levels in the storage hierarchy. at the highest
level, the operating system may maintain a cache of le-system data in main
memory. in addition, solid-state disks may be used for high-speed storage that
is accessed through the le-system interface. the bulk of secondary storage
is on magnetic disks. the magnetic-disk storage, in turn, is often backed up
onto magnetic tapes or removable disks to protect against data loss in case
of a hard-disk failure. some systems automatically archive old le data from
secondary storage to tertiary storage, such as tape jukeboxes, to lower the
storage cost (see chapter 10).
level
name
typical size
implementation
technology
access time (ns)
bandwidth (mb/sec)
managed by
backed by
1
registers
< 1 kb
custom memory
with multiple
ports cmos
0.25 - 0.5
20,000 - 100,000
compiler
cache
2
cache
< 16mb
on-chip or
off-chip
cmos sram
0.5 - 25
5,000 - 10,000
hardware
main memory
3
main memory
< 64gb
cmos sram
80 - 250
1,000 - 5,000
operating system
disk
4
solid state disk
< 1 tb
flash memory
25,000 - 50,000
500
operating system
disk
5
magnetic disk
< 10 tb
magnetic disk
5,000,000
20 - 150
operating system
disk or tape
figure 1.11
performance of various levels of storage.
1.8
storage management
29
a
a
a
magnetic
disk
main
memory
hardware
register
cache
figure 1.12
migration of integer a from disk to register.
the movement of information between levels of a storage hierarchy may
be either explicit or implicit, depending on the hardware design and the
controlling operating-system software. for instance, data transfer from cache
to cpu and registers is usually a hardware function, with no operating-system
intervention. in contrast, transfer of data from disk to memory is usually
controlled by the operating system.
in a hierarchical storage structure, the same data may appear in different
levels of the storage system. for example, suppose that an integer a that is to
be incremented by 1 is located in le b, and le b resides on magnetic disk.
the increment operation proceeds by rst issuing an i/o operation to copy the
disk block on which a resides to main memory. this operation is followed by
copying a to the cache and to an internal register. thus, the copy of a appears
in several places: on the magnetic disk, in main memory, in the cache, and in an
internal register (see figure 1.12). once the increment takes place in the internal
register, the value of a differs in the various storage systems. the value of a
becomes the same only after the new value of a is written from the internal
register back to the magnetic disk.
in a computing environment where only one process executes at a time,
this arrangement poses no difculties, since an access to integer a will always
be to the copy at the highest level of the hierarchy. however, in a multitasking
environment, where the cpu is switched back and forth among various
processes, extreme care must be taken to ensure that, if several processes wish
to access a, then each of these processes will obtain the most recently updated
value of a.
the situation becomes more complicated in a multiprocessor environment
where, in addition to maintaining internal registers, each of the cpus also
contains a local cache (figure 1.6). in such an environment, a copy of a may
exist simultaneously in several caches. since the various cpus can all execute
in parallel, we must make sure that an update to the value of a in one cache
is immediately reected in all other caches where a resides. this situation is
called cache coherency, and it is usually a hardware issue (handled below the
operating-system level).
in a distributed environment, the situation becomes even more complex.
in this environment, several copies (or replicas) of the same le can be kept on
different computers. since the various replicas may be accessed and updated
concurrently, some distributed systems ensure that, when a replica is updated
in one place, all other replicas are brought up to date as soon as possible. there
are various ways to achieve this guarantee, as we discuss in chapter 17.
1.8.4
i/o systems
one of the purposes of an operating system is to hide the peculiarities of specic
hardware devices from the user. for example, in unix, the peculiarities of i/o
30
chapter 1
introduction
devices are hidden from the bulk of the operating system itself by the i/o
subsystem. the i/o subsystem consists of several components:
a memory-management component that includes buffering, caching, and
spooling
a general device-driver interface
drivers for specic hardware devices
only the device driver knows the peculiarities of the specic device to which
it is assigned.
we discussed in section 1.2.3 how interrupt handlers and device drivers are
used in the construction of efcient i/o subsystems. in chapter 13, we discuss
how the i/o subsystem interfaces to the other system components, manages
devices, transfers data, and detects i/o completion.
1.9
protection and security
if a computer system has multiple users and allows the concurrent execution
of multiple processes, then access to data must be regulated. for that purpose,
mechanisms ensure that les, memory segments, cpu, and other resources can
be operated on by only those processes that have gained proper authoriza-
tion from the operating system. for example, memory-addressing hardware
ensures that a process can execute only within its own address space. the
timer ensures that no process can gain control of the cpu without eventually
relinquishing control. device-control registers are not accessible to users, so
the integrity of the various peripheral devices is protected.
protection, then, is any mechanism for controlling the access of processes
or users to the resources dened by a computer system. this mechanism must
provide means to specify the controls to be imposed and to enforce the controls.
protection can improve reliability by detecting latent errors at the interfaces
between component subsystems. early detection of interface errors can often
prevent contamination of a healthy subsystem by another subsystem that is
malfunctioning. furthermore, an unprotected resource cannot defend against
use (or misuse) by an unauthorized or incompetent user. a protection-oriented
system provides a means to distinguish between authorized and unauthorized
usage, as we discuss in chapter 14.
a system can have adequate protection but still be prone to failure and
allow inappropriate access. consider a user whose authentication information
(her means of identifying herself to the system) is stolen. her data could be
copied or deleted, even though le and memory protection are working. it is
the job of security to defend a system from external and internal attacks. such
attacks spread across a huge range and include viruses and worms, denial-of-
service attacks (which use all of a systems resources and so keep legitimate
users out of the system), identity theft, and theft of service (unauthorized
use of a system). prevention of some of these attacks is considered an
operating-system function on some systems, while other systems leave it to
policy or additional software. due to the alarming rise in security incidents,
1.10
kernel data structures
31
operating-system security features represent a fast-growing area of research
and implementation. we discuss security in chapter 15.
protection and security require the system to be able to distinguish among
all its users. most operating systems maintain a list of user names and
associated user identiers (user ids). in windows parlance, this is a security
id (sid). these numerical ids are unique, one per user. when a user logs in
to the system, the authentication stage determines the appropriate user id for
the user. that user id is associated with all of the users processes and threads.
when an id needs to be readable by a user, it is translated back to the user
name via the user name list.
in some circumstances, we wish to distinguish among sets of users rather
than individual users. for example, the owner of a le on a unix system may be
allowed to issue all operations on that le, whereas a selected set of users may
be allowed only to read the le. to accomplish this, we need to dene a group
name and the set of users belonging to that group. group functionality can
be implemented as a system-wide list of group names and group identiers.
a user can be in one or more groups, depending on operating-system design
decisions. the users group ids are also included in every associated process
and thread.
in the course of normal system use, the user id and group id for a user
are sufcient. however, a user sometimes needs to escalate privileges to gain
extra permissions for an activity. the user may need access to a device that is
restricted, for example. operating systems provide various methods to allow
privilege escalation. on unix, for instance, the setuid attribute on a program
causes that program to run with the user id of the owner of the le, rather than
the current users id. the process runs with this effective uid until it turns off
the extra privileges or terminates.
1.10
kernel data structures
we turn next to a topic central to operating-system implementation: the way
data are structured in the system. in this section, we briey describe several
fundamental data structures used extensively in operating systems. readers
who require further details on these structures, as well as others, should consult
the bibliography at the end of the chapter.
1.10.1
lists, stacks, and queues
an array is a simple data structure in which each element can be accessed
directly. for example, main memory is constructed as an array. if the data item
being stored is larger than one byte, then multiple bytes can be allocated to the
item, and the item is addressed as item number item size. but what about
storing an item whose size may vary? and what about removing an item if the
relative positions of the remaining items must be preserved? in such situations,
arrays give way to other data structures.
after arrays, lists are perhaps the most fundamental data structures in
computer science. whereas each item in an array can be accessed directly, the
items in a list must be accessed in a particular order. that is, a list represents
a collection of data values as a sequence. the most common method for
32
chapter 1
introduction
data
data
data
null
figure 1.13
singly linked list.
implementing this structure is a linked list, in which items are linked to one
another. linked lists are of several types:
in a singly linked list, each item points to its successor, as illustrated in
figure 1.13.
in a doubly linked list, a given item can refer either to its predecessor or
to its successor, as illustrated in figure 1.14.
in a circularly linked list, the last element in the list refers to the rst
element, rather than to null, as illustrated in figure 1.15.
linked lists accommodate items of varying sizes and allow easy insertion
and deletion of items. one potential disadvantage of using a list is that
performance for retrieving a specied item in a list of size n is linear o(n),
as it requires potentially traversing all n elements in the worst case. lists
are sometimes used directly by kernel algorithms. frequently, though, they
are used for constructing more powerful data structures, such as stacks and
queues.
a stack is a sequentially ordered data structure that uses the last in, rst
out (lifo) principle for adding and removing items, meaning that the last item
placed onto a stack is the rst item removed. the operations for inserting and
removing items from a stack are known as push and pop, respectively. an
operating system often uses a stack when invoking function calls. parameters,
local variables, and the return address are pushed onto the stack when a
function is called; returning from the function call pops those items off the
stack.
a queue, in contrast, is a sequentially ordered data structure that uses the
rst in, rst out (fifo) principle: items are removed from a queue in the order
in which they were inserted. there are many everyday examples of queues,
including shoppers waiting in a checkout line at a store and cars waiting in line
at a trafc signal. queues are also quite common in operating systemsjobs
that are sent to a printer are typically printed in the order in which they were
submitted, for example. as we shall see in chapter 6, tasks that are waiting to
be run on an available cpu are often organized in queues.
data null
null
data
data
data
figure 1.14
doubly linked list.
1.10
kernel data structures
33
data
data
data
data
figure 1.15
circularly linked list.
1.10.2
trees
a tree is a data structure that can be used to represent data hierarchically. data
values in a tree structure are linked through parentchild relationships. in a
general tree, a parent may have an unlimited number of children. in a binary
tree, a parent may have at most two children, which we term the left child
and the right child. a binary search tree additionally requires an ordering
between the parents two children in which le f t child <= right child. figure
1.16 provides an example of a binary search tree. when we search for an item in
a binary search tree, the worst-case performance is o(n) (consider how this can
occur). to remedy this situation, we can use an algorithm to create a balanced
binary search tree. here, a tree containing n items has at most lg n levels, thus
ensuring worst-case performance of o(lg n). we shall see in section 6.7.1 that
linux uses a balanced binary search tree as part its cpu-scheduling algorithm.
1.10.3
hash functions and maps
a hash function takes data as its input, performs a numeric operation on this
data, and returns a numeric value. this numeric value can then be used as an
index into a table (typically an array) to quickly retrieve the data. whereas
searching for a data item through a list of size n can require up to o(n)
comparisons in the worst case, using a hash function for retrieving data from
table can be as good as o(1) in the worst case, depending on implementation
details. because of this performance, hash functions are used extensively in
operating systems.
17
35
40
42
12
14
6
figure 1.16
binary search tree.
34
chapter 1
introduction
0
1
.
.
n
value
hash map
hash_function(key)
figure 1.17
hash map.
one potential difculty with hash functions is that two inputs can result
in the same output valuethat is, they can link to the same table location.
we can accommodate this hash collision by having a linked list at that table
location that contains all of the items with the same hash value. of course, the
more collisions there are, the less efcient the hash function is.
one use of a hash function is to implement a hash map, which associates
(or maps) [key:value] pairs using a hash function. for example, we can map
the key operating to the value system. once the mapping is established, we can
apply the hash function to the key to obtain the value from the hash map
(figure 1.17). for example, suppose that a user name is mapped to a password.
password authentication then proceeds as follows: a user enters his user name
and password. the hash function is applied to the user name, which is then
used to retrieve the password. the retrieved password is then compared with
the password entered by the user for authentication.
1.10.4
bitmaps
a bitmap is a string of n binary digits that can be used to represent the status of
n items. for example, suppose we have several resources, and the availability
of each resource is indicated by the value of a binary digit: 0 means that the
resource is available, while 1 indicates that it is unavailable (or vice-versa). the
value of the ith position in the bitmap is associated with the ith resource. as an
example, consider the bitmap shown below:
0 0 1 0 1 1 1 0 1
resources 2, 4, 5, 6, and 8 are unavailable; resources 0, 1, 3, and 7 are available.
the power of bitmaps becomes apparent when we consider their space
efciency. if we were to use an eight-bit boolean value instead of a single bit,
the resulting data structure would be eight times larger. thus, bitmaps are
commonly used when there is a need to represent the availability of a large
number of resources. disk drives provide a nice illustration. a medium-sized
disk drive might be divided into several thousand individual units, called disk
blocks. a bitmap can be used to indicate the availability of each disk block.
data structures are pervasive in operating system implementations. thus,
we will see the structures discussed here, along with others, throughout this
text as we explore kernel algorithms and their implementations.
1.11
computing environments
35
linux kernel data structures
the data structures used in the linux kernel are available in the kernel source
code. the include le <linux/list.h> provides details of the linked-list
data structure used throughout the kernel. a queue in linux is known as
a kfifo, and its implementation can be found in the kfifo.c le in the
kernel directory of the source code. linux also provides a balanced binary
search tree implementation using red-black trees. details can be found in the
include le <linux/rbtree.h>.
1.11
computing environments
so far, we have briey described several aspects of computer systems and the
operating systems that manage them. we turn now to a discussion of how
operating systems are used in a variety of computing environments.
1.11.1
traditional computing
as computing has matured, the lines separating many of the traditional com-
puting environments have blurred. consider the typical ofce environment.
just a few years ago, this environment consisted of pcs connected to a network,
with servers providing le and print services. remote access was awkward,
and portability was achieved by use of laptop computers. terminals attached
to mainframes were prevalent at many companies as well, with even fewer
remote access and portability options.
the current trend is toward providing more ways to access these computing
environments. web technologies and increasing wan bandwidth are stretching
the boundaries of traditional computing. companies establish portals, which
provide web accessibility to their internal servers. network computers (or
thin clients)which are essentially terminals that understand web-based
computingare used in place of traditional workstations where more security
or easier maintenance is desired. mobile computers can synchronize with pcs
to allow very portable use of company information. mobile computers can also
connect to wireless networks and cellular data networks to use the companys
web portal (as well as the myriad other web resources).
at home, most users once had a single computer with a slow modem
connection to the ofce, the internet, or both. today, network-connection
speeds once available only at great cost are relatively inexpensive in many
places, giving home users more access to more data. these fast data connections
are allowing home computers to serve up web pages and to run networks that
include printers, client pcs, and servers. many homes use rewalls to protect
their networks from security breaches.
in the latter half of the 20th century, computing resources were relatively
scarce. (before that, they were nonexistent!) for a period of time, systems
were either batch or interactive. batch systems processed jobs in bulk, with
predetermined input from les or other data sources. interactive systems
waited for input from users. to optimize the use of the computing resources,
multiple users shared time on these systems. time-sharing systems used a
36
chapter 1
introduction
timer and scheduling algorithms to cycle processes rapidly through the cpu,
giving each user a share of the resources.
today, traditional time-sharing systems are uncommon. the same schedul-
ing technique is still in use on desktop computers, laptops, servers, and even
mobile computers, but frequently all the processes are owned by the same
user (or a single user and the operating system). user processes, and system
processes that provide services to the user, are managed so that each frequently
gets a slice of computer time. consider the windows created while a user
is working on a pc, for example, and the fact that they may be performing
different tasks at the same time. even a web browser can be composed of
multiple processes, one for each website currently being visited, with time
sharing applied to each web browser process.
1.11.2
mobile computing
mobile computing refers to computing on handheld smartphones and tablet
computers. these devices share the distinguishing physical features of being
portable and lightweight. historically, compared with desktop and laptop
computers, mobile systems gave up screen size, memory capacity, and overall
functionality in return for handheld mobile access to services such as e-mail
and web browsing. over the past few years, however, features on mobile
devices have become so rich that the distinction in functionality between, say,
a consumer laptop and a tablet computer may be difcult to discern. in fact,
we might argue that the features of a contemporary mobile device allow it to
provide functionality that is either unavailable or impractical on a desktop or
laptop computer.
today, mobile systems are used not only for e-mail and web browsing but
also for playing music and video, reading digital books, taking photos, and
recording high-denition video. accordingly, tremendous growth continues
in the wide range of applications that run on such devices. many developers
are now designing applications that take advantage of the unique features of
mobile devices, such as global positioning system (gps) chips, accelerometers,
and gyroscopes. an embedded gps chip allows a mobile device to use satellites
to determine its precise location on earth. that functionality is especially useful
in designing applications that provide navigationfor example, telling users
which way to walk or drive or perhaps directing them to nearby services, such
as restaurants. an accelerometer allows a mobile device to detect its orientation
with respect to the ground and to detect certain other forces, such as tilting
and shaking. in several computer games that employ accelerometers, players
interface with the system not by using a mouse or a keyboard but rather by
tilting, rotating, and shaking the mobile device! perhaps more a practical use
of these features is found in augmented-reality applications, which overlay
information on a display of the current environment. it is difcult to imagine
how equivalent applications could be developed on traditional laptop or
desktop computer systems.
to provide access to on-line services, mobile devices typically use either
ieee standard 802.11 wireless or cellular data networks. the memory capacity
and processing speed of mobile devices, however, are more limited than those
of pcs. whereas a smartphone or tablet may have 64 gb in storage, it is not
uncommon to nd 1 tb in storage on a desktop computer. similarly, because
1.11
computing environments
37
power consumption is such a concern, mobile devices often use processors that
are smaller, are slower, and offer fewer processing cores than processors found
on traditional desktop and laptop computers.
two operating systems currently dominate mobile computing: apple ios
and google android. ios was designed to run on apple iphone and ipad
mobile devices. android powers smartphones and tablet computers available
from many manufacturers. we examine these two mobile operating systems in
further detail in chapter 2.
1.11.3
distributed systems
a distributed system is a collection of physically separate, possibly heteroge-
neous, computer systems that are networked to provide users with access to
the various resources that the system maintains. access to a shared resource
increases computation speed, functionality, data availability, and reliability.
some operating systems generalize network access as a form of le access, with
the details of networking contained in the network interfaces device driver.
others make users specically invoke network functions. generally, systems
contain a mix of the two modesfor example ftp and nfs. the protocols
that create a distributed system can greatly affect that systems utility and
popularity.
a network, in the simplest terms, is a communication path between
two or more systems. distributed systems depend on networking for their
functionality. networks vary by the protocols used, the distances between
nodes, and the transport media. tcp/ip is the most common network protocol,
and it provides the fundamental architecture of the internet. most operating
systems support tcp/ip, including all general-purpose ones. some systems
support proprietary protocols to suit their needs. to an operating system, a
network protocol simply needs an interface devicea network adapter, for
examplewith a device driver to manage it, as well as software to handle
data. these concepts are discussed throughout this book.
networks are characterized based on the distances between their nodes.
a local-area network (lan) connects computers within a room, a building,
or a campus. a wide-area network (wan) usually links buildings, cities, or
countries. a global company may have a wan to connect its ofces worldwide,
for example. these networks may run one protocol or several protocols. the
continuing advent of new technologies brings about new forms of networks.
for example, a metropolitan-area network (man) could link buildings within
a city. bluetooth and 802.11 devices use wireless technology to communicate
over a distance of several feet, in essence creating a personal-area network
(pan) between a phone and a headset or a smartphone and a desktop computer.
the media to carry networks are equally varied. they include copper wires,
ber strands, and wireless transmissions between satellites, microwave dishes,
and radios. when computing devices are connected to cellular phones, they
create a network. even very short-range infrared communication can be used
for networking. at a rudimentary level, whenever computers communicate,
they use or create a network. these networks also vary in their performance
and reliability.
some operating systems have taken the concept of networks and dis-
tributed systems further than the notion of providing network connectivity.
38
chapter 1
introduction
a network operating system is an operating system that provides features
such as le sharing across the network, along with a communication scheme
that allows different processes on different computers to exchange messages.
a computer running a network operating system acts autonomously from all
other computers on the network, although it is aware of the network and is
able to communicate with other networked computers. a distributed operating
system provides a less autonomous environment. the different computers
communicate closely enough to provide the illusion that only a single operating
system controls the network. we cover computer networks and distributed
systems in chapter 17.
1.11.4
clientserver computing
as pcs have become faster, more powerful, and cheaper, designers have shifted
away from centralized system architecture. terminals connected to centralized
systems are now being supplanted by pcs and mobile devices. correspond-
ingly, user-interface functionality once handled directly by centralized systems
is increasingly being handled by pcs, quite often through a web interface. as
a result, many of todays systems act as server systems to satisfy requests
generated by client systems. this form of specialized distributed system, called
a clientserver system, has the general structure depicted in figure 1.18.
server systems can be broadly categorized as compute servers and le
servers:
the compute-server system provides an interface to which a client can
send a request to perform an action (for example, read data). in response,
the server executes the action and sends the results to the client. a server
running a database that responds to client requests for data is an example
of such a system.
the le-server system provides a le-system interface where clients can
create, update, read, and delete les. an example of such a system is a web
server that delivers les to clients running web browsers.
server
network
client
desktop
client
laptop
client
smartphone
figure 1.18
general structure of a clientserver system.
1.11
computing environments
39
1.11.5
peer-to-peer computing
another structure for a distributed system is the peer-to-peer (p2p) system
model. in this model, clients and servers are not distinguished from one
another. instead, all nodes within the system are considered peers, and each
may act as either a client or a server, depending on whether it is requesting or
providing a service. peer-to-peer systems offer an advantage over traditional
client-server systems. in a client-server system, the server is a bottleneck; but
in a peer-to-peer system, services can be provided by several nodes distributed
throughout the network.
to participate in a peer-to-peer system, a node must rst join the network
of peers. once a node has joined the network, it can begin providing services
toand requesting services fromother nodes in the network. determining
what services are available is accomplished in one of two general ways:
when a node joins a network, it registers its service with a centralized
lookup service on the network. any node desiring a specic service rst
contacts this centralized lookup service to determine which node provides
the service. the remainder of the communication takes place between the
client and the service provider.
an alternative scheme uses no centralized lookup service. instead, a peer
acting as a client must discover what node provides a desired service by
broadcasting a request for the service to all other nodes in the network. the
node (or nodes) providing that service responds to the peer making the
request. to support this approach, a discovery protocol must be provided
that allows peers to discover services provided by other peers in the
network. figure 1.19 illustrates such a scenario.
peer-to-peer networks gained widespread popularity in the late 1990s with
several le-sharing services, such as napster and gnutella, that enabled peers
to exchange les with one another. the napster system used an approach
similar to the rst type described above: a centralized server maintained an
index of all les stored on peer nodes in the napster network, and the actual
client
client
client
client
client
figure 1.19
peer-to-peer system with no centralized service.
40
chapter 1
introduction
exchange of les took place between the peer nodes. the gnutella system used
a technique similar to the second type: a client broadcasted le requests to
other nodes in the system, and nodes that could service the request responded
directly to the client. the future of exchanging les remains uncertain because
peer-to-peer networks can be used to exchange copyrighted materials (music,
for example) anonymously, and there are laws governing the distribution of
copyrighted material. notably, napster ran into legal trouble for copyright
infringement and its services were shut down in 2001.
skype is another example of peer-to-peer computing. it allows clients to
make voice calls and video calls and to send text messages over the internet
using a technology known as voice over ip (voip). skype uses a hybrid peer-
to-peer approach. it includes a centralized login server, but it also incorporates
decentralized peers and allows two peers to communicate.
1.11.6
virtualization
virtualization is a technology that allows operating systems to run as appli-
cations within other operating systems. at rst blush, there seems to be
little reason for such functionality. but the virtualization industry is vast and
growing, which is a testament to its utility and importance.
broadly speaking, virtualization is one member of a class of software
that also includes emulation. emulation is used when the source cpu type
is different from the target cpu type. for example, when apple switched from
the ibm power cpu to the intel x86 cpu for its desktop and laptop computers,
it included an emulation facility called rosetta, which allowed applications
compiled for the ibm cpu to run on the intel cpu. that same concept can be
extended to allow an entire operating system written for one platform to run
on another. emulation comes at a heavy price, however. every machine-level
instruction that runs natively on the source system must be translated to the
equivalent function on the target system, frequently resulting in several target
instructions. if the source and target cpus have similar performance levels, the
emulated code can run much slower than the native code.
a common example of emulation occurs when a computer language is
not compiled to native code but instead is either executed in its high-level
form or translated to an intermediate form. this is known as interpretation.
some languages, such as basic, can be either compiled or interpreted. java, in
contrast, is always interpreted. interpretation is a form of emulation in that the
high-level language code is translated to native cpu instructions, emulating
not another cpu but a theoretical virtual machine on which that language could
run natively. thus, we can run java programs on java virtual machines, but
technically those virtual machines are java emulators.
with virtualization, in contrast, an operating system that is natively com-
piled for a particular cpu architecture runs within another operating system
also native to that cpu. virtualization rst came about on ibm mainframes
as a method for multiple users to run tasks concurrently. running multiple
virtual machines allowed (and still allows) many users to run tasks on a system
designed for a single user. later, in response to problems with running multiple
microsoft windows xp applications on the intel x86 cpu, vmware created a
new virtualization technology in the form of an application that ran on xp.
that application ran one or more guest copies of windows or other native
1.11
computing environments
41
(a)
processes
hardware
kernel
(b)
programming
interface
processes
processes
processes
kernel
kernel
kernel
vm2
vm1
vm3
manager
hardware
virtual machine
figure 1.20
vmware.
x86 operating systems, each running its own applications. (see figure 1.20.)
windows was the host operating system, and the vmware application was the
virtual machine manager vmm. the vmm runs the guest operating systems,
manages their resource use, and protects each guest from the others.
even though modern operating systems are fully capable of running
multiple applications reliably, the use of virtualization continues to grow. on
laptops and desktops, a vmm allows the user to install multiple operating
systems for exploration or to run applications written for operating systems
other than the native host. for example, an apple laptop running mac os
x on the x86 cpu can run a windows guest to allow execution of windows
applications. companies writing software for multiple operating systems
can use virtualization to run all of those operating systems on a single
physical server for development, testing, and debugging. within data centers,
virtualization has become a common method of executing and managing
computing environments. vmms like vmware, esx, and citrix xenserver no
longer run on host operating systems but rather are the hosts. full details of
the features and implementation of virtualization are found in chapter 16.
1.11.7
cloud computing
cloud computing is a type of computing that delivers computing, storage,
and even applications as a service across a network. in some ways, its a
logical extension of virtualization, because it uses virtualization as a base for
its functionality. for example, the amazon elastic compute cloud (ec2) facility
has thousands of servers, millions of virtual machines, and petabytes of storage
available for use by anyone on the internet. users pay per month based on how
much of those resources they use.
there are actually many types of cloud computing, including the following:
public clouda cloud available via the internet to anyone willing to pay
for the services
42
chapter 1
introduction
private clouda cloud run by a company for that companys own use
hybrid clouda cloud that includes both public and private cloud
components
software as a service (saas)one or more applications (such as word
processors or spreadsheets) available via the internet
platform as a service (paas)a software stack ready for application use
via the internet (for example, a database server)
infrastructure as a service (iaas)servers or storage available over the
internet (for example, storage available for making backup copies of
production data)
these cloud-computing types are not discrete, as a cloud computing environ-
ment may provide a combination of several types. for example, an organization
may provide both saas and iaas as a publicly available service.
certainly, there are traditional operating systems within many of the
types of cloud infrastructure. beyond those are the vmms that manage the
virtual machines in which the user processes run. at a higher level, the vmms
themselves are managed by cloud management tools, such as vware vcloud
director and the open-source eucalyptus toolset. these tools manage the
resources within a given cloud and provide interfaces to the cloud components,
making a good argument for considering them a new type of operating system.
figure 1.21 illustrates a public cloud providing iaas. notice that both the
cloud services and the cloud user interface are protected by a rewall.
firewall
cloud
customer
interface
load balancer
virtual
machines
virtual
machines
servers
servers
storage
internet
customer
requests
cloud
management
commands
cloud
managment
services
figure 1.21
cloud computing.
1.12
open-source operating systems
43
1.11.8
real-time embedded systems
embedded computers are the most prevalent form of computers in existence.
these devices are found everywhere, from car engines and manufacturing
robots to dvds and microwave ovens. they tend to have very specic tasks.
the systems they run on are usually primitive, and so the operating systems
provide limited features. usually, they have little orno user interface, preferring
to spend their time monitoring and managing hardware devices, such as
automobile engines and robotic arms.
these embedded systems vary considerably. some are general-purpose
computers, running standard operating systemssuch as linuxwith
special-purpose applications to implement the functionality. others are hard-
ware devices with a special-purpose embedded operating system providing
just the functionality desired. yet others are hardware devices with application-
specic integrated circuits (asics) that perform their tasks without an operat-
ing system.
the use of embedded systems continues to expand. the power of these
devices, both as standalone units and as elements of networks and the web,
is sure to increase as well. even now, entire houses can be computerized, so
that a central computereither a general-purpose computer or an embedded
systemcan control heating and lighting, alarm systems, and even coffee
makers. web access can enable a home owner to tell the house to heat up
before she arrives home. someday, the refrigerator can notify the grocery store
when it notices the milk is gone.
embedded systems almost always run real-time operating systems. a
real-time system is used when rigid time requirements have been placed on
the operation of a processor or the ow of data; thus, it is often used as a
control device in a dedicated application. sensors bring data to the computer.
the computer must analyze the data and possibly adjust controls to modify
the sensor inputs. systems that control scientic experiments, medical imaging
systems, industrial control systems, and certain display systems are real-
time systems. some automobile-engine fuel-injection systems, home-appliance
controllers, and weapon systems are also real-time systems.
a real-time system has well-dened, xed time constraints. processing
must be done within the dened constraints, or the system will fail. for
instance, it would not do for a robot arm to be instructed to halt after it had
smashed into the car it was building. a real-time system functions correctly
only if it returns the correct result within its time constraints. contrast this
system with a time-sharing system, where it is desirable (but not mandatory)
to respond quickly, or a batch system, which may have no time constraints at
all.
in chapter 6, we consider the scheduling facility needed to implement
real-time functionality in an operating system. in chapter 9, we describe the
design of memory management for real-time computing. finally, in chapters
18 and 19, we describe the real-time components of the linux and windows 7
operating systems.
1.12
open-source operating systems
we noted at the beginning of this chapter that the study of operating systems
has been made easier by the availability of a vast number of open-source
44
chapter 1
introduction
releases. open-source operating systems are those available in source-code
format rather than as compiled binary code. linux is the most famous open-
source operating system, while microsoft windows is a well-known example
of the opposite closed-source approach. apples mac os x and ios operating
systems comprise a hybrid approach. they contain an open-source kernel
named darwin yet include proprietary, closed-source components as well.
starting with the source code allows the programmer to produce binary
code that can be executed on a system. doing the oppositereverse engi-
neering the source code from the binariesis quite a lot of work, and useful
items such as comments are never recovered. learning operating systems by
examining the source code has other benets as well. with the source code
in hand, a student can modify the operating system and then compile and
run the code to try out those changes, which is an excellent learning tool.
this text includes projects that involve modifying operating-system source
code, while also describing algorithms at a high level to be sure all important
operating-system topics are covered. throughout the text, we provide pointers
to examples of open-source code for deeper study.
there are many benets to open-source operating systems, including a
community of interested (and usually unpaid) programmers who contribute
to the code by helping to debug it, analyze it, provide support, and suggest
changes. arguably, open-source code is more secure than closed-source code
because many more eyes are viewing the code. certainly, open-source code has
bugs, but open-source advocates argue that bugs tend to be found and xed
faster owing to the number of people using and viewing the code. companies
that earn revenue from selling their programs often hesitate to open-source
their code, but red hat and a myriad of other companies are doing just that
and showing that commercial companies benet, rather than suffer, when they
open-source their code. revenue can be generated through support contracts
and the sale of hardware on which the software runs, for example.
1.12.1
history
in the early days of modern computing (that is, the 1950s), a great deal of
software was available in open-source format. the original hackers (computer
enthusiasts) at mits tech model railroad club left their programs in drawers
for others to work on. homebrew user groups exchanged code during their
meetings. later, company-specic user groups, such as digital equipment
corporations dec, accepted contributions of source-code programs, collected
them onto tapes, and distributed the tapes to interested members.
computer and software companies eventually sought to limit the use of
their software to authorized computers and paying customers. releasing only
the binary les compiled from the source code, rather than the source code
itself, helped them to achieve this goal, as well as protecting their code and their
ideas from their competitors. another issue involved copyrighted material.
operating systems and other programs can limit the ability to play back movies
and music or display electronic books to authorized computers. such copy
protection or digital rights management (drm) would not be effective if the
source code that implemented these limits were published. laws in many
countries, including the u.s. digital millennium copyright act (dmca), make
it illegal to reverse-engineer drm code or otherwise try to circumvent copy
protection.
1.12
open-source operating systems
45
to counter the move to limit software use and redistribution, richard
stallman in 1983 started the gnu project to create a free, open-source, unix-
compatible operating system. in 1985, he published the gnu manifesto, which
argues that all software should be free and open-sourced. he also formed
the free software foundation (fsf) with the goal of encouraging the free
exchange of software source code and the free use of that software. rather than
copyright its software, the fsf copylefts the software to encourage sharing
and improvement. the gnu general public license (gpl) codies copylefting
and is a common license under which free software is released. fundamentally,
gpl requires that the source code be distributed with any binaries and that any
changes made to the source code be released under the same gpl license.
1.12.2
linux
as an example of an open-source operating system, consider gnu/linux.
the gnu project produced many unix-compatible tools, including compilers,
editors, and utilities, but never released a kernel. in 1991, a student in
finland, linus torvalds, released a rudimentary unix-like kernel using the
gnu compilers and tools and invited contributions worldwide. the advent of
the internet meant that anyone interested could download the source code,
modify it, and submit changes to torvalds. releasing updates once a week
allowed this so-called linux operating system to grow rapidly, enhanced by
several thousand programmers.
the resulting gnu/linux operating system has spawned hundreds of
unique distributions, or custom builds, of the system. major distributions
include redhat, suse, fedora, debian, slackware, and ubuntu. distributions
vary in function, utility, installed applications, hardware support, user inter-
face, and purpose. for example, redhat enterprise linux is geared to large
commercial use. pclinuxos is a livecdan operating system that can be
booted and run from a cd-rom without being installed on a systems hard
disk. one variant of pclinuxoscalled pclinuxos supergamer dvdis a
livedvd that includes graphics drivers and games. a gamer can run it on
any compatible system simply by booting from the dvd. when the gamer is
nished, a reboot of the system resets it to its installed operating system.
you can run linux on a windows system using the following simple, free
approach:
1. download the free vmware player tool from
http://www.vmware.com/download/player/
and install it on your system.
2. choose a linux version from among the hundreds of appliances, or
virtual machine images, available from vmware at
http://www.vmware.com/appliances/
these images are preinstalled with operating systems and applications
and include many avors of linux.
46
chapter 1
introduction
3. boot the virtual machine within vmware player.
with this text, we provide a virtual machine image of linux running the debian
release. this image contains the linux source code as well as tools for software
development. we cover examples involving that linux image throughout this
text, as well as in a detailed case study in chapter 18.
1.12.3
bsd unix
bsd unix has a longer and more complicated history than linux. it started in
1978 as a derivative of at&ts unix. releases from the university of california
at berkeley (ucb) came in source and binary form, but they were not open-
source because a license from at&t was required. bsd unixs development was
slowed by a lawsuit by at&t, but eventually a fully functional, open-source
version, 4.4bsd-lite, was released in 1994.
just as with linux, there are many distributions of bsd unix, including
freebsd, netbsd, openbsd, and dragonybsd. to explore the source code
of freebsd, simply download the virtual machine image of the version of
interest and boot it within vmware, as described above for linux. the source
code comes with the distribution and is stored in /usr/src/. the kernel
source code is in /usr/src/sys. for example, to examine the virtual memory
implementation code in the freebsd kernel, see the les in /usr/src/sys/vm.
darwin, the core kernel component of mac os x, is based on bsd
unix and is open-sourced as well. that source code is available from
http://www.opensource.apple.com/. every mac os x release has its open-
source components posted at that site. the name of the package that contains
the kernel begins with xnu. apple also provides extensive developer tools,
documentation, and support at http://connect.apple.com. for more informa-
tion, see appendix a.
1.12.4
solaris
solaris is the commercial unix-based operating system of sun microsystems.
originally, suns sunos operating system was based on bsd unix. sun moved
to at&ts system v unix as its base in 1991. in 2005, sun open-sourced most
of the solaris code as the opensolaris project. the purchase of sun by oracle
in 2009, however, left the state of this project unclear. the source code as it
was in 2005 is still available via a source code browser and for download at
http://src.opensolaris.org/source.
several groups interested in using opensolaris have started from that base
and expanded its features. their working set is project illumos, which has
expanded from the opensolaris base to include more features and to be the
basis for several products. illumos is available at http://wiki.illumos.org.
1.12.5
open-source systems as learning tools
the free software movement is driving legions of programmers to create
thousands of open-source projects, including operating systems. sites like
http://freshmeat.net/ and http://distrowatch.com/ provide portals to many
of these projects. as we stated earlier, open-source projects enable students to
use source code as a learning tool. they can modify programs and test them,
1.13
summary
47
help nd and x bugs, and otherwise explore mature, full-featured operating
systems, compilers, tools, user interfaces, and other types of programs. the
availability of source code for historic projects, such as multics, can help
students to understand those projects and to build knowledge that will help in
the implementation of new projects.
gnu/linux and bsd unix are all open-source operating systems, but each
has its own goals, utility, licensing, and purpose. sometimes, licenses are not
mutually exclusive and cross-pollination occurs, allowing rapid improvements
in operating-system projects. for example, several major components of
opensolaris have been ported to bsd unix. the advantages of free software
and open sourcing are likely to increase the number and quality of open-source
projects, leading to an increase in the number of individuals and companies
that use these projects.
1.13
summary
an operating system is software that manages the computer hardware, as well
as providing an environment for application programs to run. perhaps the
most visible aspect of an operating system is the interface to the computer
system it provides to the human user.
for a computer to do its job of executing programs, the programs must be in
main memory. main memory is the only large storage area that the processor
can access directly. it is an array of bytes, ranging in size from millions to
billions. each byte in memory has its own address. the main memory is usually
a volatile storage device that loses its contents when power is turned off or
lost. most computer systems provide secondary storage as an extension of
main memory. secondary storage provides a form of nonvolatile storage that
is capable of holding large quantities of data permanently. the most common
secondary-storage device is a magnetic disk, which provides storage of both
programs and data.
the wide variety of storage systems in a computer system can be organized
in a hierarchy according to speed and cost. the higher levels are expensive,
but they are fast. as we move down the hierarchy, the cost per bit generally
decreases, whereas the access time generally increases.
there are several different strategies for designing a computer system.
single-processor systems have only one processor, while multiprocessor
systems contain two or more processors that share physical memory and
peripheral devices. the most common multiprocessor design is symmetric
multiprocessing (or smp), where all processors are considered peers and run
independently of one another. clustered systems are a specialized form of
multiprocessor systems and consist of multiple computer systems connected
by a local-area network.
to best utilize the cpu, modern operating systems employ multiprogram-
ming, whichallowsseveral jobstobe inmemoryatthe same time, thusensuring
that the cpu always has a job to execute. time-sharing systems are an exten-
sion of multiprogramming wherein cpu scheduling algorithms rapidly switch
between jobs, thus providing the illusion that each job is running concurrently.
the operating system must ensure correct operation of the computer
system. to prevent user programs from interfering with the proper operation of
48
chapter 1
introduction
the study of operating systems
there has never been a more interesting time to study operating systems, and
it has never been easier. the open-source movement has overtaken operating
systems, causing many of them to be made available in both source and binary
(executable) format. the list of operating systems available in both formats
includes linux, bsd unix, solaris, and part of mac os x. the availability
of source code allows us to study operating systems from the inside out.
questions that we could once answer only by looking at documentation or
the behavior of an operating system we can now answer by examining the
code itself.
operating systems that are no longer commercially viable have been
open-sourced as well, enabling us to study how systems operated in a
time of fewer cpu, memory, and storage resources. an extensive but
incomplete list of open-source operating-system projects is available from
http://dmoz.org/computers/software/operating systems/open source/.
in addition, the rise of virtualization as a mainstream (and frequently free)
computer function makes it possible to run many operating systems on top of
one core system. for example, vmware ( http://www.vmware.com)provides
a free player for windows on which hundreds of free virtual appliances
can run. virtualbox ( http://www.virtualbox.com) provides a free, open-
source virtual machine manager on many operating systems. using such
tools, students can try out hundreds of operating systems without dedicated
hardware.
in some cases, simulators of specic hardware are also available, allowing
the operating system to run on native hardware, all within the connes
of a modern computer and modern operating system. for example, a
decsystem-20 simulator running on mac os x can boot tops-20, load the
source tapes, and modify and compile a new tops-20 kernel. an interested
student can search the internet to nd the original papers that describe the
operating system, as well as the original manuals.
the advent of open-source operating systems has also made it easier to
make the move from student to operating-system developer. with some
knowledge, some effort, and an internet connection, a student can even create
a new operating-system distribution. just a few years ago, it was difcult or
impossible to get access to source code. now, such access is limited only by
how much interest, time, and disk space a student has.
the system, the hardware has two modes: user mode and kernel mode. various
instructions (such as i/o instructions and halt instructions) are privileged and
can be executed only in kernel mode. the memory in which the operating
system resides must also be protected from modication by the user. a timer
prevents innite loops. these facilities (dual mode, privileged instructions,
memory protection, and timer interrupt) are basic building blocks used by
operating systems to achieve correct operation.
a process (or job) is the fundamental unit of work in an operating system.
process management includes creating and deleting processes and providing
mechanisms for processes to communicate and synchronize with each other.
practice exercises
49
an operating system manages memory by keeping track of what parts of
memory are being used and by whom. the operating system is also responsible
for dynamically allocating and freeing memory space. storage space is also
managed by the operating system; this includes providing le systems for
representing les and directories and managing space on mass-storage devices.
operating systems must also be concerned with protecting and securing the
operating system and users. protection measures control the access of processes
or users to the resources made available by the computer system. security
measures are responsible for defending a computer system from external or
internal attacks.
several datastructuresthatare fundamental tocomputerscience are widely
used in operating systems, including lists, stacks, queues, trees, hash functions,
maps, and bitmaps.
computing takes place in a variety of environments. traditional computing
involves desktop and laptop pcs, usually connected to a computer network.
mobile computing refers to computing on handheld smartphones and tablet
computers, which offer several unique features. distributed systems allow
users to share resources on geographically dispersed hosts connected via
a computer network. services may be provided through either the client
server model or the peer-to-peer model. virtualization involves abstracting
a computers hardware into several different execution environments. cloud
computing uses a distributed system to abstract services into a cloud, where
users may access the services from remote locations. real-time operating
systems are designed for embedded environments, such as consumer devices,
automobiles, and robotics.
the free software movement has created thousands of open-source projects,
including operating systems. because of these projects, students are able to use
source code as a learning tool. they can modify programs and test them,
help nd and x bugs, and otherwise explore mature, full-featured operating
systems, compilers, tools, user interfaces, and other types of programs.
gnu/linux and bsd unix are open-source operating systems. the advan-
tages of free software and open sourcing are likely to increase the number
and quality of open-source projects, leading to an increase in the number of
individuals and companies that use these projects.
practice exercises
1.1
what are the three main purposes of an operating system?
1.2
we have stressed the need for an operating system to make efcient use
of the computing hardware. when is it appropriate for the operating
system to forsake this principle and to waste resources? why is such
a system not really wasteful?
1.3
what is the main difculty that a programmer must overcome in writing
an operating system for a real-time environment?
1.4
keeping in mind the various denitions of operating system, consider
whether the operating system should include applications such as web
browsers and mail programs. argue both that it should and that it should
not, and support your answers.
50
chapter 1
introduction
1.5
how does the distinction between kernel mode and user mode function
as a rudimentary form of protection (security) system?
1.6
which of the following instructions should be privileged?
a.
set value of timer.
b.
read the clock.
c.
clear memory.
d.
issue a trap instruction.
e.
turn off interrupts.
f.
modify entries in device-status table.
g.
switch from user to kernel mode.
h.
access i/o device.
1.7
some early computers protected the operating system by placing it in
a memory partition that could not be modied by either the user job
or the operating system itself. describe two difculties that you think
could arise with such a scheme.
1.8
some cpus provide for more than two modes of operation. what are
two possible uses of these multiple modes?
1.9
timers could be used to compute the current time. provide a short
description of how this could be accomplished.
1.10
give two reasons why caches are useful. what problems do they solve?
what problems do they cause? if a cache can be made as large as the
device for which it is caching (for instance, a cache as large as a disk),
why not make it that large and eliminate the device?
1.11
distinguish between the clientserver and peer-to-peer models of
distributed systems.
exercises
1.12
in a multiprogramming and time-sharing environment, several users
share the system simultaneously. this situation can result in various
security problems.
a.
what are two such problems?
b.
can we ensure the same degree of security in a time-shared
machine as in a dedicated machine? explain your answer.
1.13
the issue of resource utilization shows up in different forms in different
types of operating systems. list what resources must be managed
carefully in the following settings:
a.
mainframe or minicomputer systems
b.
workstations connected to servers
c.
mobile computers
exercises
51
1.14
under what circumstances would a user be better off using a time-
sharing system than a pc or a single-user workstation?
1.15
describe the differences between symmetric and asymmetric multipro-
cessing. what are three advantages and one disadvantage of multipro-
cessor systems?
1.16
how do clustered systems differ from multiprocessor systems? what is
required for two machines belonging to a cluster to cooperate to provide
a highly available service?
1.17
consider a computing cluster consisting of two nodes running a
database. describe two ways in which the cluster software can manage
access to the data on the disk. discuss the benets and disadvantages of
each.
1.18
how are network computers different from traditional personal com-
puters? describe some usage scenarios in which it is advantageous to
use network computers.
1.19
what is the purpose of interrupts? how does an interrupt differ from a
trap? can traps be generated intentionally by a user program? if so, for
what purpose?
1.20
direct memory access is used for high-speed i/o devices in order to
avoid increasing the cpus execution load.
a.
how does the cpu interface with the device to coordinate the
transfer?
b.
how does the cpu know when the memory operations are com-
plete?
c.
the cpu is allowed to execute other programs while the dma
controller is transferring data. does this process interfere with
the execution of the user programs? if so, describe what forms
of interference are caused.
1.21
some computer systems do not provide a privileged mode of operation
in hardware. is it possible to construct a secure operating system for
these computer systems? give arguments both that it is and that it is not
possible.
1.22
many smp systems have different levels of caches; one level is local to
each processing core, and another level is shared among all processing
cores. why are caching systems designed this way?
1.23
consider an smp system similar to the one shown in figure 1.6. illustrate
with an example how data residing in memory could in fact have a
different value in each of the local caches.
1.24
discuss, with examples, how the problem of maintaining coherence of
cached data manifests itself in the following processing environments:
a.
single-processor systems
b.
multiprocessor systems
c.
distributed systems
52
chapter 1
introduction
1.25
describe a mechanism for enforcing memory protection in order to
prevent a program from modifying the memory associated with other
programs.
1.26
which network congurationlan or wanwould best suit the
following environments?
a.
a campus student union
b.
several campus locations across a statewide university system
c.
a neighborhood
1.27
describe some of the challenges of designing operating systems for
mobile devices compared with designing operating systems for tradi-
tional pcs.
1.28
what are some advantages of peer-to-peer systems over client-server
systems?
1.29
describe some distributed applications that would be appropriate for a
peer-to-peer system.
1.30
identify several advantages and several disadvantages of open-source
operating systems. include the types of people who would nd each
aspect to be an advantage or a disadvantage.
bibliographical notes
[brookshear (2012)] provides an overview of computer science in general.
thorough coverage of data structures can be found in [cormen et al. (2009)].
[russinovich and solomon (2009)] give an overview of microsoft windows
and covers considerable technical detail about the system internals and
components. [mcdougall and mauro (2007)] cover the internals of the solaris
operating system. mac os x internals are discussed in [singh (2007)]. [love
(2010)] provides an overview of the linux operating system and great detail
about data structures used in the linux kernel.
many general textbooks cover operating systems, including [stallings
(2011)], [deitel et al. (2004)], and [tanenbaum (2007)]. [kurose and ross (2013)]
provides a general overview of computer networks, including a discussion
of client-server and peer-to-peer systems. [tarkoma and lagerspetz (2011)]
examines several different mobile operating systems, including android and
ios.
[hennessyand patterson(2012)]provide coverage of i/o systems and buses
and of system architecture in general. [bryant and ohallaron (2010)] provide
a thorough overview of a computer system from the perspective of a computer
programmer. details of the intel 64 instruction set and privilege modes can be
found in [intel (2011)].
the history of open sourcing and its benets and challenges appears in
[raymond (1999)]. the free software foundation has published its philosophy
in http://www.gnu.org/philosophy/free-software-for-freedom.html. the open
source of mac os x are available from http://www.apple.com/opensource/.
bibliography
53
wikipedia has an informative entry about the contributions of richard
stallman at http://en.wikipedia.org/wiki/richard stallman.
the source code of multicsisavailable at http://web.mit.edu/multics-history
/source/multics internet server/multics sources.html.
bibliography
[brookshear (2012)]
j. g. brookshear, computer science: an overview, eleventh
edition, addison-wesley (2012).
[bryant and ohallaron (2010)]
r. bryant and d. ohallaron, computer systems:
a programmers perspective, second edition, addison-wesley (2010).
[cormen et al. (2009)]
t. h. cormen, c. e. leiserson, r. l. rivest, and c. stein,
introduction to algorithms, third edition, mit press (2009).
[deitel et al. (2004)]
h. deitel, p. deitel, and d. choffnes, operating systems,
third edition, prentice hall (2004).
[hennessy and patterson (2012)]
j. hennessy and d. patterson, computer archi-
tecture: a quantitative approach, fifth edition, morgan kaufmann (2012).
[intel (2011)]
intel 64 and ia-32 architectures software developers manual, com-
bined volumes: 1, 2a, 2b, 3a and 3b. intel corporation (2011).
[kurose and ross (2013)]
j. kurose and k. ross, computer networkinga top
down approach, sixth edition, addison-wesley (2013).
[love (2010)]
r. love, linux kernel development, third edition, developers
library (2010).
[mcdougall and mauro (2007)]
r. mcdougall and j. mauro, solaris internals,
second edition, prentice hall (2007).
[raymond (1999)]
e. s. raymond, the cathedral and the bazaar, oreilly &
associates (1999).
[russinovich and solomon (2009)]
m. e. russinovich and d. a. solomon, win-
dows internals: including windows server 2008 and windows vista, fifth edition,
microsoft press (2009).
[singh (2007)]
a. singh, mac os x internals: a systems approach, addison-
wesley (2007).
[stallings (2011)]
w. stallings, operating systems, seventh edition, prentice hall
(2011).
[tanenbaum (2007)]
a. s. tanenbaum, modern operating systems, third edition,
prentice hall (2007).
[tarkoma and lagerspetz (2011)]
s. tarkoma and e. lagerspetz, arching over
the mobile computing chasm: platforms and runtimes, ieee computer,
volume 44, (2011), pages 2228.
2
c h a p t e r
operating-
system
structures
an operating system provides the environment within which programs are
executed. internally, operating systems vary greatly in their makeup, since
they are organized along many different lines. the design of a new operating
system is a major task. it is important that the goals of the system be well
dened before the design begins. these goals form the basis for choices among
various algorithms and strategies.
we can view an operating system from several vantage points. one view
focuses on the services that the system provides; another, on the interface that
it makes available to users and programmers; a third, on its components and
their interconnections. in this chapter, we explore all three aspects of operating
systems, showing the viewpoints of users, programmers, and operating system
designers. we consider what services an operating system provides, how they
are provided, how they are debugged, and what the various methodologies
are for designing such systems. finally, we describe how operating systems
are created and how a computer starts its operating system.
chapter objectives
to describe the services an operating system provides to users, processes,
and other systems.
to discuss the various ways of structuring an operating system.
to explain how operating systems are installed and customized and how
they boot.
2.1
operating-system services
an operating system provides an environment for the execution of programs.
it provides certain services to programs and to the users of those programs.
the specic services provided, of course, differ from one operating system to
another, but we can identify common classes. these operating system services
are provided for the convenience of the programmer, to make the programming
55
56
chapter 2
operating-system structures
user and other system programs
services
operating system
hardware
system calls
gui
batch
user interfaces
command line
program
execution
i/o
operations
file
systems
communication
resource
allocation
accounting
protection
and
security
error
detection
figure 2.1
a view of operating system services.
task easier. figure 2.1 shows one view of the various operating-system services
and how they interrelate.
one set of operating system services provides functions that are helpful to
the user.
user interface. almost all operating systems have a user interface (ui).
this interface can take several forms. one is a command-line interface
(cli), which uses text commands and a method for entering them (say,
a keyboard for typing in commands in a specic format with specic
options). another is a batch interface, in which commands and directives
to control those commands are entered into les, and those les are
executed. most commonly, a graphical user interface (gui) is used. here,
the interface is a window system with a pointing device to direct i/o,
choose from menus, and make selections and a keyboard to enter text.
some systems provide two or all three of these variations.
program execution. the system must be able to load a program into
memory and to run that program. the program must be able to end its
execution, either normally or abnormally (indicating error).
i/o operations. a running program may require i/o, which may involve a
le or an i/o device. for specic devices, special functions may be desired
(such as recording to a cd or dvd drive or blanking a display screen). for
efciency and protection, users usually cannot control i/o devices directly.
therefore, the operating system must provide a means to do i/o.
file-system manipulation. the le system is of particular interest. obvi-
ously, programs need to read and write les and directories. they also
need to create and delete them by name, search for a given le, and
list le information. finally, some operating systems include permissions
management to allow or deny access to les or directories based on le
ownership. many operating systems provide a variety of le systems,
sometimes to allow personal choice and sometimes to provide specic
features or performance characteristics.
2.1
operating-system services
57
communications. there are many circumstances in which one process
needs to exchange information with another process. such communication
may occur between processes that are executing on the same computer or
between processes that are executing on different computer systems tied
together by a computer network. communications may be implemented
via shared memory, in which two or more processes read and write to
a shared section of memory, or message passing, in which packets of
information in predened formats are moved between processes by the
operating system.
error detection. the operating system needs to be detecting and correcting
errors constantly. errors may occur in the cpu and memory hardware (such
as a memory error or a power failure), in i/o devices (such as a parity error
on disk, a connection failure on a network, or lack of paper in the printer),
and in the user program (such as an arithmetic overow, an attempt to
access an illegal memory location, or a too-great use of cpu time). for
each type of error, the operating system should take the appropriate action
to ensure correct and consistent computing. sometimes, it has no choice
but to halt the system. at other times, it might terminate an error-causing
process or return an error code to a process for the process to detect and
possibly correct.
another set of operating system functions exists not for helping the user
but rather for ensuring the efcient operation of the system itself. systems with
multiple users can gain efciency by sharing the computer resources among
the users.
resource allocation. when there are multiple users or multiple jobs
running at the same time, resources must be allocated to each of them. the
operating system manages many different types of resources. some (such
as cpu cycles, main memory, and le storage) may have special allocation
code, whereas others (such as i/o devices) may have much more general
request and release code. for instance, in determining how best to use
the cpu, operating systems have cpu-scheduling routines that take into
account the speed of the cpu, the jobs that must be executed, the number of
registers available, and other factors. there may also be routines to allocate
printers, usb storage drives, and other peripheral devices.
accounting. we want to keep track of which users use how much and
what kinds of computer resources. this record keeping may be used for
accounting (so that users can be billed) or simply for accumulating usage
statistics. usage statistics may be a valuable tool for researchers who wish
to recongure the system to improve computing services.
protection and security. the owners of information stored in a multiuser or
networked computer system may want to control use of that information.
when several separate processes execute concurrently, it should not be
possible for one process to interfere with the others or with the operating
system itself. protection involves ensuring that all access to system
resources is controlled. security of the system from outsiders is also
important. such security starts with requiring each user to authenticate
58
chapter 2
operating-system structures
himself or herself to the system, usually by means of a password, to gain
access to system resources. it extends to defending external i/o devices,
including network adapters, from invalid access attempts and to recording
all such connections for detection of break-ins. if a system is to be protected
and secure, precautions must be instituted throughout it. a chain is only
as strong as its weakest link.
2.2
user and operating-system interface
we mentioned earlier that there are several ways for users to interface with
the operating system. here, we discuss two fundamental approaches. one
provides a command-line interface, or command interpreter, that allows users
to directly enter commands to be performed by the operating system. the
other allows users to interface with the operating system via a graphical user
interface, or gui.
2.2.1
command interpreters
some operating systems include the command interpreter in the kernel. others,
such as windows and unix, treat the command interpreter as a special program
that is running when a job is initiated or when a user rst logs on (on interactive
systems). on systems with multiple command interpreters to choose from, the
interpreters are known as shells. for example, on unix and linux systems, a
user may choose among several different shells, including the bourne shell, c
shell, bourne-again shell, korn shell, and others. third-party shells and free
user-written shells are also available. most shells provide similar functionality,
and a users choice of which shell to use is generally based on personal
preference. figure 2.2 shows the bourne shell command interpreter being used
on solaris 10.
the main function of the command interpreter is to get and execute the next
user-specied command. many of the commands given at this level manipulate
les: create, delete, list, print, copy, execute, and so on. the ms-dos and unix
shells operate in this way. these commands can be implemented in two general
ways.
in one approach, the command interpreter itself contains the code to
execute the command. for example, a command to delete a le may cause
the command interpreter to jump to a section of its code that sets up the
parameters and makes the appropriate system call. in this case, the number of
commands that can be given determines the size of the command interpreter,
since each command requires its own implementing code.
an alternative approachused by unix, among other operating systems
implements most commands through system programs. in this case, the
command interpreter does not understand the command in any way; it merely
uses the command to identify a le to be loaded into memory and executed.
thus, the unix command to delete a le
rm file.txt
would search for a le called rm, load the le into memory, and execute it with
the parameter file.txt. the function associated with the rm command would
2.2
user and operating-system interface
59
figure 2.2
the bourne shell command interpreter in solrais 10.
be dened completely by the code in the le rm. in this way, programmers can
add new commands to the system easily by creating new les with the proper
names. the command-interpreter program, which can be small, does not have
to be changed for new commands to be added.
2.2.2
graphical user interfaces
a second strategy for interfacing with the operating system is through a user-
friendly graphical user interface, or gui. here, rather than entering commands
directly via a command-line interface, users employ a mouse-based window-
and-menu system characterized by a desktop metaphor. the user moves the
mouse to position its pointer on images, or icons, on the screen (the desktop)
that represent programs, les, directories, and system functions. depending
on the mouse pointers location, clicking a button on the mouse can invoke a
program, select a le or directoryknown as a folderor pull down a menu
that contains commands.
graphical user interfaces rst appeared due in part to research taking place
in the early 1970s at xerox parc research facility. the rst gui appeared on
the xerox alto computer in 1973. however, graphical interfaces became more
widespread with the advent of apple macintosh computers in the 1980s. the
user interface for the macintosh operating system (mac os) has undergone
various changes over the years, the most signicant being the adoption of
the aqua interface that appeared with mac os x. microsofts rst version of
windowsversion 1.0was based on the addition of a gui interface to the
ms-dos operating system. later versions of windows have made cosmetic
60
chapter 2
operating-system structures
changes in the appearance of the gui along with several enhancements in its
functionality.
because a mouse is impractical for most mobile systems, smartphones and
handheld tablet computers typically use a touchscreen interface. here, users
interact by making gestures on the touchscreenfor example, pressing and
swiping ngers across the screen. figure 2.3 illustrates the touchscreen of the
apple ipad. whereas earlier smartphones included a physical keyboard, most
smartphones now simulate a keyboard on the touchscreen.
traditionally, unix systems have been dominated by command-line inter-
faces. various gui interfaces are available, however. these include the common
desktop environment (cde) and x-windows systems, which are common
on commercial versions of unix, such as solaris and ibms aix system. in
addition, there has been signicant development in gui designs from various
open-source projects, such as k desktop environment (or kde) and the gnome
desktop by the gnu project. both the kde and gnome desktops run on linux
and various unix systems and are available under open-source licenses, which
means their source code is readily available for reading and for modication
under specic license terms.
figure 2.3
the ipad touchscreen.
2.2
user and operating-system interface
61
2.2.3
choice of interface
the choice of whether to use a command-line or gui interface is mostly
one of personal preference. system administrators who manage computers
and power users who have deep knowledge of a system frequently use the
command-line interface. for them, it is more efcient, giving them faster
access to the activities they need to perform. indeed, on some systems, only a
subset of system functions is available via the gui, leaving the less common
tasks to those who are command-line knowledgeable. further, command-
line interfaces usually make repetitive tasks easier, in part because they have
their own programmability. for example, if a frequent task requires a set of
command-line steps, those steps can be recorded into a le, and that le can
be run just like a program. the program is not compiled into executable code
but rather is interpreted by the command-line interface. these shell scripts are
very common on systems that are command-line oriented, such as unix and
linux.
in contrast, most windows users are happy to use the windows gui
environment and almost never use the ms-dos shell interface. the various
changes undergone by the macintosh operating systems provide a nice study
in contrast. historically, mac os has not provided a command-line interface,
always requiring its users to interface with the operating system using its gui.
however, with the release of mac os x (which is in part implemented using a
unix kernel), the operating system now provides both a aqua interface and a
command-line interface. figure 2.4 is a screenshot of the mac os x gui.
figure 2.4
the mac os x gui.
62
chapter 2
operating-system structures
the user interface can vary from system to system and even from user
to user within a system. it typically is substantially removed from the actual
system structure. the design of a useful and friendly user interface is therefore
not a direct function of the operating system. in this book, we concentrate on
the fundamental problems of providing adequate service to user programs.
from the point of view of the operating system, we do not distinguish between
user programs and system programs.
2.3
system calls
system calls provide an interface to the services made available by an operating
system. these calls are generally available as routines written in c and
c++, although certain low-level tasks (for example, tasks where hardware
must be accessed directly) may have to be written using assembly-language
instructions.
before we discuss how an operating system makes system calls available,
lets rst use an example to illustrate how system calls are used: writing a
simple program to read data from one le and copy them to another le. the
rst input that the program will need is the names of the two les: the input le
and the output le. these names can be specied in many ways, depending on
the operating-system design. one approach is for the program to ask the user
for the names. in an interactive system, this approach will require a sequence of
system calls, rst to write a prompting message on the screen and then to read
from the keyboard the characters that dene the two les. on mouse-based and
icon-based systems, a menu of le names is usually displayed in a window.
the user can then use the mouse to select the source name, and a window
can be opened for the destination name to be specied. this sequence requires
many i/o system calls.
once the two le names have been obtained, the program must open the
input le and create the output le. each of these operations requires another
system call. possible error conditions for each operation can require additional
system calls. when the program tries to open the input le, for example, it may
nd that there is no le of that name or that the le is protected against access.
in these cases, the program should print a message on the console (another
sequence of system calls) and then terminate abnormally (another system call).
if the input le exists, then we must create a new output le. we may nd that
there is already an output le with the same name. this situation may cause
the program to abort (a system call), or we may delete the existing le (another
system call) and create a new one (yet another system call). another option,
in an interactive system, is to ask the user (via a sequence of system calls to
output the prompting message and to read the response from the terminal)
whether to replace the existing le or to abort the program.
when both les are set up, we enter a loop that reads from the input le
(a system call) and writes to the output le (another system call). each read
and write must return status information regarding various possible error
conditions. on input, the program may nd that the end of the le has been
reached or that there was a hardware failure in the read (such as a parity error).
the write operation may encounter various errors, depending on the output
device (for example, no more disk space).
2.3
system calls
63
finally, after the entire le is copied, the program may close both les
(another system call), write a message to the console or window (more system
calls), and nally terminate normally (the nal system call). this system-call
sequence is shown in figure 2.5.
as you can see, even simple programs may make heavy use of the
operating system. frequently, systems execute thousands of system calls
per second. most programmers never see this level of detail, however.
typically, application developers design programs according to an application
programming interface (api). the api species a set of functions that are
available to an application programmer, including the parameters that are
passed to each function and the return values the programmer can expect.
three of the most common apis available to application programmers are
the windows api for windows systems, the posix api for posix-based systems
(which include virtually all versions of unix, linux, and mac os x), and the java
api for programs that run on the java virtual machine. a programmer accesses
an api via a library of code provided by the operating system. in the case of
unix and linux for programs written in the c language, the library is called
libc. note thatunless speciedthe system-call names used throughout
this text are generic examples. each operating system has its own name for
each system call.
behind the scenes, the functions that make up an api typically invoke the
actual system calls on behalf of the application programmer. for example, the
windows function createprocess() (which unsurprisingly is used to create
a new process) actually invokes the ntcreateprocess() system call in the
windows kernel.
why would an application programmer prefer programming according to
an api rather than invoking actual system calls? there are several reasons for
doing so. one benet concerns program portability. an application program-
source file
destination file
example system call sequence
acquire input file name
write prompt to screen
accept input
acquire output file name
write prompt to screen
accept input
open the input file
if file doesn't exist, abort
create output file
if file exists, abort
loop
read from input file
write to output file
until read fails
close output file
write completion message to screen
terminate normally
figure 2.5
example of how system calls are used.
64
chapter 2
operating-system structures
example of standard api
as an example of a standard api, consider the read() function that is
available in unix and linux systems. the api for this function is obtained
from the man page by invoking the command
man read
on the command line. a description of this api appears below:
#include <unistd.h>
ssize_t
read(int fd, void *buf, size_t count)
return
value
function
name
parameters
a program that uses the read() function must include the unistd.h header
le, as this le denes the ssize t and size t data types (among other
things). the parameters passed to read() are as follows:
int fdthe le descriptor to be read
void *bufa buffer where the data will be read into
size t countthe maximum number of bytes to be read into the
buffer
on a successful read, the number of bytes read is returned. a return value of
0 indicates end of le. if an error occurs, read() returns 1.
mer designing a program using an api can expect her program to compile and
run on any system that supports the same api (although, in reality, architectural
differences often make this more difcult than it may appear). furthermore,
actual system calls can often be more detailed and difcult to work with than
the api available to an application programmer. nevertheless, there often exists
a strong correlation between a function in the api and its associated system call
within the kernel. in fact, many of the posix and windows apis are similar to
the native system calls provided by the unix, linux, and windows operating
systems.
for most programming languages, the run-time support system (a set of
functions built into libraries included with a compiler) provides a system-
call interface that serves as the link to system calls made available by the
operating system. the system-call interface intercepts function calls in the api
and invokes the necessary system calls within the operating system. typically,
a number is associated with each system call, and the system-call interface
maintains a table indexed according to these numbers. the system call interface
2.3
system calls
65
implementation
of open ( )
system call
open ( )
user
mode
return
user application
system call interface
kernel
mode
i
open ( )
figure 2.6
the handling of a user application invoking the open() system call.
then invokes the intended system call in the operating-system kernel and
returns the status of the system call and any return values.
the caller need know nothing about how the system call is implemented
or what it does during execution. rather, the caller need only obey the api and
understand what the operating system will do as a result of the execution of
that system call. thus, most of the details of the operating-system interface
are hidden from the programmer by the api and are managed by the run-time
support library. the relationship between an api, the system-call interface,
and the operating system is shown in figure 2.6, which illustrates how the
operating system handles a user application invoking the open() system call.
system calls occur in different ways, depending on the computer in use.
often, more information is required than simply the identity of the desired
system call. the exact type and amount of information vary according to the
particular operating system and call. for example, to get input, we may need
to specify the le or device to use as the source, as well as the address and
length of the memory buffer into which the input should be read. of course,
the device or le and length may be implicit in the call.
three general methods are used to pass parameters to the operating system.
the simplest approach is to pass the parameters in registers. in some cases,
however, there may be more parameters than registers. in these cases, the
parameters are generally stored in a block, or table, in memory, and the
address of the block is passed as a parameter in a register (figure 2.7). this
is the approach taken by linux and solaris. parameters also can be placed,
or pushed, onto the stack by the program and popped off the stack by the
operating system. some operating systems prefer the block or stack method
because those approaches do not limit the number or length of parameters
being passed.
66
chapter 2
operating-system structures
code for
system
call 13
operating system
user program
use parameters
from table x
register
x
x: parameters
for call
load address x
system call 13
figure 2.7
passing of parameters as a table.
2.4
types of system calls
system calls can be grouped roughly into six major categories: process
control, le manipulation, device manipulation, information maintenance,
communications, and protection. in sections 2.4.1 through 2.4.6, we briey
discuss the types of system calls that may be provided by an operating system.
most of these system calls support, or are supported by, concepts and functions
that are discussed in later chapters. figure 2.8 summarizes the types of system
calls normally provided by an operating system. as mentioned, in this text,
we normally refer to the system calls by generic names. throughout the text,
however, we provide examples of the actual counterparts to the system calls
for windows, unix, and linux systems.
2.4.1
process control
a running program needs to be able to halt its execution either normally
(end()) or abnormally (abort()). if a system call is made to terminate the
currently running program abnormally, or if the program runs into a problem
and causes an error trap, a dump of memory is sometimes taken and an error
message generated. the dump is written to disk and may be examined by
a debuggera system program designed to aid the programmer in nding
and correcting errors, or bugsto determine the cause of the problem. under
either normal or abnormal circumstances, the operating system must transfer
control to the invoking command interpreter. the command interpreter then
reads the next command. in an interactive system, the command interpreter
simply continues with the next command; it is assumed that the user will
issue an appropriate command to respond to any error. in a gui system, a
pop-up window might alert the user to the error and ask for guidance. in a
batch system, the command interpreter usually terminates the entire job and
continues with the next job. some systems may allow for special recovery
actions in case an error occurs. if the program discovers an error in its input
and wants to terminate abnormally, it may also want to dene an error level.
more severe errors can be indicated by a higher-level error parameter. it is then
2.4
types of system calls
67
process control
end, abort
load, execute
create process, terminate process
get process attributes, set process attributes
wait for time
wait event, signal event
allocate and free memory
file management
create le, delete le
open, close
read, write, reposition
get le attributes, set le attributes
device management
request device, release device
read, write, reposition
get device attributes, set device attributes
logically attach or detach devices
information maintenance
get time or date, set time or date
get system data, set system data
get process, le, or device attributes
set process, le, or device attributes
communications
create, delete communication connection
send, receive messages
transfer status information
attach or detach remote devices
figure 2.8
types of system calls.
possible to combine normal and abnormal termination by dening a normal
termination as an error at level 0. the command interpreter or a following
program can use this error level to determine the next action automatically.
a process or job executing one program may want to load() and
execute() another program. this feature allows the command interpreter to
execute a program as directed by, for example, a user command, the click of a
68
chapter 2
operating-system structures
examples of windows and unix system calls
windows
unix
process
createprocess()
fork()
control
exitprocess()
exit()
waitforsingleobject()
wait()
file
createfile()
open()
manipulation
readfile()
read()
writefile()
write()
closehandle()
close()
device
setconsolemode()
ioctl()
manipulation
readconsole()
read()
writeconsole()
write()
information
getcurrentprocessid()
getpid()
maintenance
settimer()
alarm()
sleep()
sleep()
communication
createpipe()
pipe()
createfilemapping()
shm open()
mapviewoffile()
mmap()
protection
setfilesecurity()
chmod()
initlializesecuritydescriptor()
umask()
setsecuritydescriptorgroup()
chown()
mouse, or a batch command. an interesting question is where to return control
when the loaded program terminates. this question is related to whether the
existing program is lost, saved, or allowed to continue execution concurrently
with the new program.
if control returns to the existing program when the new program termi-
nates, we must save the memory image of the existing program; thus, we have
effectively created a mechanism for one program to call another program. if
both programs continue concurrently, we have created a new job or process to
be multiprogrammed. often, there is a system call specically for this purpose
(create process() or submit job()).
if we create a new job or process, or perhaps even a set of jobs or
processes, we should be able to control its execution. this control requires
the ability to determine and reset the attributes of a job or process, includ-
ing the jobs priority, its maximum allowable execution time, and so on
(get process attributes() and set process attributes()). we may also
want to terminate a job or process that we created (terminate process()) if
we nd that it is incorrect or is no longer needed.
2.4
types of system calls
69
example of standard c library
the standard c library provides a portion of the system-call interface for
many versions of unix and linux. as an example, lets assume a c program
invokes the printf() statement. the c library intercepts this call and
invokes the necessary system call (or calls) in the operating systemin this
instance, the write() system call. the c library takes the value returned by
write() and passes it back to the user program. this is shown below:
write ( )
system call
user
mode
kernel
mode
#include <stdio.h>
int main ( )
{
printf ("greetings");
return 0;
}
standard c library
write ( )
having created new jobs or processes, we may need to wait for them to
nish their execution. we may want to wait for a certain amount of time to
pass (wait time()). more probably, we will want to wait for a specic event
to occur (wait event()). the jobs or processes should then signal when that
event has occurred (signal event()).
quite often, two or more processes may share data. to ensure the integrity
of the data being shared, operating systems often provide system calls allowing
a process to lock shared data. then, no other process can access the data until
the lock is released. typically, such system calls include acquire lock() and
release lock(). system calls of these types, dealing with the coordination of
concurrent processes, are discussed in great detail in chapter 5.
there are so many facets of and variations in process and job control that
we next use two examplesone involving a single-tasking system and the
other a multitasking systemto clarify these concepts. the ms-dos operating
system is an example of a single-tasking system. it has a command interpreter
that is invoked when the computer is started (figure 2.9(a)). because ms-dos
is single-tasking, it uses a simple method to run a program and does not create
a new process. it loads the program into memory, writing over most of itself to
70
chapter 2
operating-system structures
(a)
(b)
free memory
command
interpreter
kernel
process
free memory
command
interpreter
kernel
figure 2.9
ms-dos execution. (a) at system startup. (b) running a program.
give the program as much memory as possible (figure 2.9(b)). next, it sets the
instruction pointer to the rst instruction of the program. the program then
runs, and either an error causes a trap, or the program executes a system call
to terminate. in either case, the error code is saved in the system memory for
later use. following this action, the small portion of the command interpreter
that was not overwritten resumes execution. its rst task is to reload the rest
of the command interpreter from disk. then the command interpreter makes
the previous error code available to the user or to the next program.
freebsd (derived from berkeley unix) is an example of a multitasking
system. when a user logs on to the system, the shell of the users choice
is run. this shell is similar to the ms-dos shell in that it accepts commands
and executes programs that the user requests. however, since freebsd is a
multitasking system, the command interpreter may continue running while
another program is executed (figure 2.10). to start a new process, the shell
free memory
interpreter
kernel
process d
process c
process b
figure 2.10
freebsd running multiple programs.
2.4
types of system calls
71
executes a fork() system call. then, the selected program is loaded into
memory via an exec() system call, and the program is executed. depending
on the way the command was issued, the shell then either waits for the process
to nish or runs the process in the background. in the latter case, the shell
immediately requests another command. when a process is running in the
background, it cannot receive input directly from the keyboard, because the
shell is using this resource. i/o is therefore done through les or through a gui
interface. meanwhile, the user is free to ask the shell to run other programs, to
monitor the progress of the running process, to change that programs priority,
and so on. when the process is done, it executes an exit() system call to
terminate, returning to the invoking process a status code of 0 or a nonzero
error code. this status or error code is then available to the shell or other
programs. processes are discussed in chapter 3 with a program example using
the fork() and exec() system calls.
2.4.2
file management
the le system is discussed in more detail in chapters 11 and 12. we can,
however, identify several common system calls dealing with les.
we rst need to be able to create() and delete() les. either system call
requires the name of the le and perhaps some of the les attributes. once
the le is created, we need to open() it and to use it. we may also read(),
write(), or reposition() (rewind or skip to the end of the le, for example).
finally, we need to close() the le, indicating that we are no longer using it.
we may need these same sets of operations for directories if we have a
directory structure for organizing les in the le system. in addition, for either
les or directories, we need to be able to determine the values of various
attributes and perhaps to reset them if necessary. file attributes include the le
name, le type, protection codes, accounting information, and so on. at least
two system calls, get file attributes() and set file attributes(), are
required for this function. some operating systems provide many more calls,
such as calls for le move() and copy(). others might provide an api that
performs those operations using code and other system calls, and others might
provide system programs to perform those tasks. if the system programs are
callable by other programs, then each can be considered an api by other system
programs.
2.4.3
device management
a process may need several resources to executemain memory, disk drives,
access to les, and so on. if the resources are available, they can be granted,
and control can be returned to the user process. otherwise, the process will
have to wait until sufcient resources are available.
the various resources controlled by the operating system can be thought
of as devices. some of these devices are physical devices (for example, disk
drives), while others can be thought of as abstract or virtual devices (for
example, les). a system with multiple users may require us to rst request()
a device, to ensure exclusive use of it. after we are nished with the device, we
release() it. these functions are similar to the open() and close() system
calls for les. other operating systems allow unmanaged access to devices.
72
chapter 2
operating-system structures
the hazard then is the potential for device contention and perhaps deadlock,
which are described in chapter 7.
once the device has been requested (and allocated to us), we can read(),
write(), and (possibly) reposition() the device, just as we can with les. in
fact, the similarity between i/o devices and les is so great that many operating
systems, including unix, merge the two into a combined ledevice structure.
in this case, a set of system calls is used on both les and devices. sometimes,
i/o devices are identied by special le names, directory placement, or le
attributes.
the user interface can also make les and devices appear to be similar, even
though the underlying system calls are dissimilar. this is another example of
the many design decisions that go into building an operating system and user
interface.
2.4.4
information maintenance
many system calls exist simply for the purpose of transferring information
between the user program and the operating system. for example, most
systems have a system call to return the current time() and date(). other
system calls may return information about the system, such as the number of
current users, the version number of the operating system, the amount of free
memory or disk space, and so on.
another set of system calls is helpful in debugging a program. many
systems provide system calls to dump() memory. this provision is useful for
debugging. a program trace lists each system call as it is executed. even
microprocessors provide a cpu mode known as single step, in which a trap
is executed by the cpu after every instruction. the trap is usually caught by a
debugger.
many operating systems provide a time prole of a program to indicate
the amount of time that the program executes at a particular location or set
of locations. a time prole requires either a tracing facility or regular timer
interrupts. at every occurrence of the timer interrupt, the value of the program
counter is recorded. with sufciently frequent timer interrupts, a statistical
picture of the time spent on various parts of the program can be obtained.
in addition, the operating system keeps information about all its processes,
and system calls are used to access this information. generally, calls are
also used to reset the process information (get process attributes() and
set process attributes()). in section 3.1.3, we discuss what information is
normally kept.
2.4.5
communication
there are two common models of interprocess communication: the message-
passing model and the shared-memory model. in the message-passing model,
the communicating processes exchange messages with one another to transfer
information. messages can be exchanged between the processes either directly
or indirectly through a common mailbox. before communication can take
place, a connection must be opened. the name of the other communicator
must be known, be it another process on the same system or a process on
another computer connected by a communications network. each computer in
a network has a host name by which it is commonly known. a host also has a
2.4
types of system calls
73
network identier, such as an ip address. similarly, each process has a process
name, and this name is translated into an identier by which the operating
system can refer to the process. the get hostid() and get processid()
system calls do this translation. the identiers are then passed to the general-
purpose open() and close() calls provided by the le system or to specic
open connection()andclose connection()systemcalls, dependingonthe
systems model of communication. the recipient process usually must give its
permission for communication to take place with an accept connection()
call. most processes that will be receiving connections are special-purpose
daemons, which are system programs provided for that purpose. they execute
a wait for connection() call and are awakened when a connection is made.
the source of the communication, known as the client, and the receiving
daemon, known as a server, then exchange messages by using read message()
and write message() system calls. the close connection() call terminates
the communication.
in the shared-memory model, processes use shared memory create()
and shared memory attach() system calls to create and gain access to regions
of memory owned by other processes. recall that, normally, the operating
system tries to prevent one process from accessing another processs memory.
shared memory requires that two or more processes agree to remove this
restriction. they can then exchange information by reading and writing data
in the shared areas. the form of the data is determined by the processes and is
not under the operating systems control. the processes are also responsible for
ensuring that they are not writing to the same location simultaneously. such
mechanisms are discussed in chapter 5. in chapter 4, we look at a variation of
the process schemethreadsin which memory is shared by default.
both of the models just discussed are common in operating systems,
and most systems implement both. message passing is useful for exchanging
smaller amounts of data, because no conicts need be avoided. it is also easier to
implement than is shared memory for intercomputer communication. shared
memory allows maximum speed and convenience of communication, since it
can be done at memory transfer speeds when it takes place within a computer.
problems exist, however, in the areas of protection and synchronization
between the processes sharing memory.
2.4.6
protection
protection provides a mechanism for controlling access to the resources
provided by a computer system. historically, protection was a concern only on
multiprogrammed computer systems with several users. however, with the
advent of networking and the internet, all computer systems, from servers to
mobile handheld devices, must be concerned with protection.
typically, system calls providing protection include set permission()
and get permission(), which manipulate the permission settings of
resources such as les and disks. the allow user() and deny user() system
calls specify whether particular users canor cannotbe allowed access to
certain resources.
we cover protection in chapter 14 and the much larger issue of security in
chapter 15.
74
chapter 2
operating-system structures
2.5
system programs
another aspect of a modern system is its collection of system programs. recall
figure 1.1, which depicted the logical computer hierarchy. at the lowest level is
hardware. next is the operating system, then the system programs, and nally
the application programs. system programs, also known as system utilities,
provide a convenient environment for program development and execution.
some of them are simply user interfaces to system calls. others are considerably
more complex. they can be divided into these categories:
file management. these programs create, delete, copy, rename, print,
dump, list, and generally manipulate les and directories.
status information. some programs simply ask the system for the date,
time, amount of available memory or disk space, number of users, or
similar status information. others are more complex, providing detailed
performance, logging, and debugging information. typically, these pro-
grams format and print the output to the terminal or other output devices
or les or display it in a window of the gui. some systems also support a
registry, which is used to store and retrieve conguration information.
file modication. several text editors may be available to create and
modify the content of les stored on disk or other storage devices. there
may also be special commands to search contents of les or perform
transformations of the text.
programming-language support. compilers, assemblers, debuggers, and
interpreters for common programming languages (such as c, c++, java,
and perl) are often provided with the operating system or available as a
separate download.
program loading and execution. once a program is assembled or com-
piled, it must be loaded into memory to be executed. the system may
provide absolute loaders, relocatable loaders, linkage editors, and overlay
loaders. debugging systems for either higher-level languages or machine
language are needed as well.
communications. these programs provide the mechanism for creating
virtual connections among processes, users, and computer systems. they
allow users to send messages to one anothers screens, to browse web
pages, to send e-mail messages, to log in remotely, or to transfer les from
one machine to another.
background services. all general-purpose systems have methods for
launching certain system-program processes at boot time. some of these
processes terminate after completing their tasks, while others continue
to run until the system is halted. constantly running system-program
processes are known as services, subsystems, or daemons. one example is
the network daemon discussed in section 2.4.5. in that example, a system
needed a service to listen for network connections in order to connect
those requests to the correct processes. other examples include process
schedulers that start processes according to a specied schedule, system
error monitoring services, and print servers. typical systems have dozens
2.6
operating-system design and implementation
75
of daemons. in addition, operating systems that run important activities
in user context rather than in kernel context may use daemons to run these
activities.
along with system programs, most operating systems are supplied with
programs that are useful in solving common problems or performing common
operations. such application programs include web browsers, word proces-
sors and text formatters, spreadsheets, database systems, compilers, plotting
and statistical-analysis packages, and games.
the view of the operating system seen by most users is dened by the
application and system programs, rather than by the actual system calls.
consider a users pc. when a users computer is running the mac os x
operating system, the user might see the gui, featuring a mouse-and-windows
interface. alternatively, or even in one of the windows, the user might have a
command-line unix shell. both use the same set of system calls, but the system
calls look different and act in different ways. further confusing the user view,
consider the user dual-booting from mac os x into windows. now the same
user on the same hardware has two entirely different interfaces and two sets of
applications using the same physical resources. on the same hardware, then,
a user can be exposed to multiple user interfaces sequentially or concurrently.
2.6
operating-system design and implementation
in this section, we discuss problems we face in designing and implementing an
operating system. there are, of course, no complete solutions to such problems,
but there are approaches that have proved successful.
2.6.1
design goals
the rst problem in designing a system is to dene goals and specications.
at the highest level, the design of the system will be affected by the choice of
hardware and the type of system: batch, time sharing, single user, multiuser,
distributed, real time, or general purpose.
beyond this highest design level, the requirements may be much harder
to specify. the requirements can, however, be divided into two basic groups:
user goals and system goals.
users want certain obvious properties in a system. the system should be
convenient to use, easy to learn and to use, reliable, safe, and fast. of course,
these specications are not particularly useful in the system design, since there
is no general agreement on how to achieve them.
a similar set of requirements can be dened by those people who must
design, create, maintain, and operate the system. the system should be easy to
design, implement, and maintain; and it should be exible, reliable, error free,
and efcient. again, these requirements are vague and may be interpreted in
various ways.
there is, in short, no unique solution to the problem of dening the
requirements for an operating system. the wide range of systems in existence
shows that different requirements can result in a large variety of solutions for
different environments. for example, the requirements for vxworks, a real-
76
chapter 2
operating-system structures
time operating system for embedded systems, must have been substantially
different from those for mvs, a large multiuser, multiaccess operating system
for ibm mainframes.
specifying and designing an operating system is a highly creative task.
although no textbook can tell you how to do it, general principles have
been developed in the eld of software engineering, and we turn now to
a discussion of some of these principles.
2.6.2
mechanisms and policies
one important principle is the separation of policy from mechanism. mecha-
nisms determine how to do something; policies determine what will be done.
for example, the timer construct (see section 1.5.2) is a mechanism for ensuring
cpu protection, but deciding how long the timer is to be set for a particular
user is a policy decision.
the separation of policy and mechanism is important for exibility. policies
are likely to change across places or over time. in the worst case, each change
in policy would require a change in the underlying mechanism. a general
mechanism insensitive to changes in policy would be more desirable. a change
in policy would then require redenition of only certain parameters of the
system. for instance, consider a mechanism for giving priority to certain types
of programs over others. if the mechanism is properly separated from policy,
it can be used either to support a policy decision that i/o-intensive programs
should have priority over cpu-intensive ones or to support the opposite policy.
microkernel-based operating systems (section 2.7.3) take the separation of
mechanism and policy to one extreme by implementing a basic set of primitive
building blocks. these blocks are almost policy free, allowing more advanced
mechanisms and policies to be added via user-created kernel modules or user
programs themselves. as an example, consider the history of unix. at rst,
it had a time-sharing scheduler. in the latest version of solaris, scheduling
is controlled by loadable tables. depending on the table currently loaded,
the system can be time sharing, batch processing, real time, fair share, or
any combination. making the scheduling mechanism general purpose allows
vast policy changes to be made with a single load-new-table command. at
the other extreme is a system such as windows, in which both mechanism
and policy are encoded in the system to enforce a global look and feel. all
applications have similar interfaces, because the interface itself is built into
the kernel and system libraries. the mac os x operating system has similar
functionality.
policy decisions are important for all resource allocation. whenever it is
necessary to decide whether or not to allocate a resource, a policy decision must
be made. whenever the question is how rather than what, it is a mechanism
that must be determined.
2.6.3
implementation
once an operating system is designed, it must be implemented. because
operating systems are collections of many programs, written by many people
over a long period of time, it is difcult to make general statements about how
they are implemented.
2.6
operating-system design and implementation
77
early operating systems were written in assembly language. now, although
some operating systems are still written in assembly language, most are written
in a higher-level language such as c or an even higher-level language such as
c++. actually, an operating system can be written in more than one language.
the lowest levels of the kernel might be assembly language. higher-level
routines might be in c, and system programs might be in c or c++, in
interpreted scripting languages like perl or python, or in shell scripts. in
fact, a given linux distribution probably includes programs written in all of
those languages.
the rst system that was not written in assembly language was probably
the master control program (mcp) for burroughs computers. mcp was written
in a variant of algol. multics, developed at mit, was written mainly in
the system programming language pl/1. the linux and windows operating
system kernels are written mostly in c, although there are some small sections
of assembly code for device drivers and for saving and restoring the state of
registers.
the advantages of using a higher-level language, or at least a systems-
implementation language, for implementing operating systems are the same
as those gained when the language is used for application programs: the code
can be written faster, is more compact, and is easier to understand and debug.
in addition, improvements in compiler technology will improve the generated
code for the entire operating system by simple recompilation. finally, an
operating system is far easier to portto move to some other hardware
if it is written in a higher-level language. for example, ms-dos was written in
intel 8088 assembly language. consequently, it runs natively only on the intel
x86 family of cpus. (note that although ms-dos runs natively only on intel
x86, emulators of the x86 instruction set allow the operating system to run on
other cpusbut more slowly, and with higher resource use. as we mentioned
in chapter 1, emulators are programs that duplicate the functionality of one
system on another system.) the linux operating system, in contrast, is written
mostly in c and is available natively on a number of different cpus, including
intel x86, oracle sparc, and ibmpowerpc.
the only possible disadvantages of implementing an operating system in a
higher-level language are reduced speed and increased storage requirements.
this, however, is no longer a major issue in todays systems. although an
expert assembly-language programmer can produce efcient small routines,
for large programs a modern compiler can perform complex analysis and apply
sophisticated optimizations that produce excellent code. modern processors
have deep pipelining and multiple functional units that can handle the details
of complex dependencies much more easily than can the human mind.
as is true in other systems, major performance improvements in oper-
ating systems are more likely to be the result of better data structures and
algorithms than of excellent assembly-language code. in addition, although
operating systems are large, only a small amount of the code is critical to high
performance; the interrupt handler, i/o manager, memory manager, and cpu
scheduler are probably the most critical routines. after the system is written
and is working correctly, bottleneck routines can be identied and can be
replaced with assembly-language equivalents.
78
chapter 2
operating-system structures
2.7
operating-system structure
a system as large and complex as a modern operating system must be
engineered carefully if it is to function properly and be modied easily. a
common approach is to partition the task into small components, or modules,
rather than have one monolithic system. each of these modules should be
a well-dened portion of the system, with carefully dened inputs, outputs,
and functions. we have already discussed briey in chapter 1 the common
components of operating systems. in this section, we discuss how these
components are interconnected and melded into a kernel.
2.7.1
simple structure
many operating systems do not have well-dened structures. frequently, such
systems started as small, simple, and limited systems and then grew beyond
their original scope. ms-dos is an example of such a system. it was originally
designed and implemented by a few people who had no idea that it would
become so popular. it was written to provide the most functionality in the
least space, so it was not carefully divided into modules. figure 2.11 shows its
structure.
in ms-dos, the interfaces and levels of functionality are not well separated.
for instance, application programs are able to access the basic i/o routines
to write directly to the display and disk drives. such freedom leaves ms-dos
vulnerable to errant (or malicious) programs, causing entire system crashes
when user programs fail. of course, ms-dos was also limited by the hardware
of its era. because the intel 8088 for which it was written provides no dual
mode and no hardware protection, the designers of ms-dos had no choice but
to leave the base hardware accessible.
another example of limited structuring is the original unix operating
system. like ms-dos, unix initially was limited by hardware functionality. it
consists of two separable parts: the kernel and the system programs. the kernel
rom bios device drivers
application program
ms-dos device drivers
resident system program
figure 2.11
ms-dos layer structure.
2.7
operating-system structure
79
kernel
(the users)
shells and commands
compilers and interpreters
system libraries
system-call interface to the kernel
signals terminal
handling
character i/o system
terminal drivers
file system
swapping block i/o
system
disk and tape drivers
cpu scheduling
page replacement
demand paging
virtual memory
kernel interface to the hardware
terminal controllers
terminals
device controllers
disks and tapes
memory controllers
physical memory
figure 2.12
traditional unix system structure.
is further separated into a series of interfaces and device drivers, which have
been added and expanded over the years as unix has evolved. we can view the
traditional unix operating system as being layered to some extent, as shown in
figure 2.12. everything below the system-call interface and above the physical
hardware is the kernel. the kernel provides the le system, cpu scheduling,
memory management, and other operating-system functions through system
calls. taken in sum, that is an enormous amount of functionality to be combined
into one level. this monolithic structure was difcult to implement and
maintain. it had a distinct performance advantage, however: there is very little
overhead in the system call interface or in communication within the kernel.
we still see evidence of this simple, monolithic structure in the unix, linux,
and windows operating systems.
2.7.2
layered approach
with proper hardware support, operating systems can be broken into pieces
that are smaller and more appropriate than those allowed by the original
ms-dos and unix systems. the operating system can then retain much greater
control over the computer and over the applications that make use of that
computer. implementers have more freedom in changing the inner workings
of the system and in creating modular operating systems. under a top-
down approach, the overall functionality and features are determined and
are separated into components. information hiding is also important, because
it leaves programmers free to implement the low-level routines as they see t,
provided that the external interface of the routine stays unchanged and that
the routine itself performs the advertised task.
a system can be made modular in many ways. one method is the layered
approach, in which the operating system is broken into a number of layers
(levels). the bottom layer (layer 0) is the hardware; the highest (layer n) is the
user interface. this layering structure is depicted in figure 2.13.
80
chapter 2
operating-system structures
layer n
user interface
layer 1
layer 0
hardware
figure 2.13
a layered operating system.
an operating-system layer is an implementation of an abstract object made
up of data and the operations that can manipulate those data. a typical
operating-system layersay, layer mconsists of data structures and a set
of routines that can be invoked by higher-level layers. layer m, in turn, can
invoke operations on lower-level layers.
the main advantage of the layered approach is simplicity of construction
and debugging. the layers are selected so that each uses functions (operations)
and services of only lower-level layers. this approach simplies debugging
and system verication. the rst layer can be debugged without any concern
for the rest of the system, because, by denition, it uses only the basic hardware
(which is assumed correct) to implement its functions. once the rst layer is
debugged, its correct functioning can be assumed while the second layer is
debugged, and so on. if an error is found during the debugging of a particular
layer, the error must be on that layer, because the layers below it are already
debugged. thus, the design and implementation of the system are simplied.
each layer is implemented only with operations provided by lower-level
layers. a layer does not need to know how these operations are implemented;
it needs to know only what these operations do. hence, each layer hides the
existence of certain data structures, operations, and hardware from higher-level
layers.
the major difculty with the layered approach involves appropriately
dening the various layers. because a layer can use only lower-level layers,
careful planning is necessary. for example, the device driver for the backing
store (disk space used by virtual-memory algorithms) must be at a lower
level than the memory-management routines, because memory management
requires the ability to use the backing store.
other requirements may not be so obvious. the backing-store driver would
normally be above the cpu scheduler, because the driver may need to wait for
i/o and the cpu can be rescheduled during this time. however, on a large
2.7
operating-system structure
81
system, the cpu scheduler may have more information about all the active
processes than can t in memory. therefore, this information may need to be
swapped in and out of memory, requiring the backing-store driver routine to
be below the cpu scheduler.
a nal problem with layered implementations is that they tend to be less
efcient than other types. for instance, when a user program executes an i/o
operation, it executes a system call that is trapped to the i/o layer, which calls
the memory-management layer, which in turn calls the cpu-scheduling layer,
which is then passed to the hardware. at each layer, the parameters may be
modied, data may need to be passed, and so on. each layer adds overhead to
the system call. the net result is a system call that takes longer than does one
on a nonlayered system.
these limitations have caused a small backlash against layering in recent
years. fewer layers with more functionality are being designed, providing
most of the advantages of modularized code while avoiding the problems of
layer denition and interaction.
2.7.3
microkernels
we have already seen that as unix expanded, the kernel became large
and difcult to manage. in the mid-1980s, researchers at carnegie mellon
university developed an operating system called mach that modularized
the kernel using the microkernel approach. this method structures the
operating system by removing all nonessential components from the kernel and
implementing them as system and user-level programs. the result is a smaller
kernel. there is little consensus regarding which services should remain in the
kernel and which should be implemented in user space. typically, however,
microkernels provide minimal process and memory management, in addition
to a communication facility. figure 2.14 illustrates the architecture of a typical
microkernel.
the main function of the microkernel is to provide communication between
the client program and the various services that are also running in user space.
communication is provided through message passing, which was described
in section 2.4.5. for example, if the client program wishes to access a le, it
application
program
file
system
device
driver
interprocess
communication
memory
managment
cpu
scheduling
messages
messages
microkernel
hardware
user
mode
kernel
mode
figure 2.14
architecture of a typical microkernel.
82
chapter 2
operating-system structures
must interact with the le server. the client program and service never interact
directly. rather, they communicate indirectly by exchanging messages with the
microkernel.
one benet of the microkernel approach is that it makes extending
the operating system easier. all new services are added to user space and
consequently do not require modication of the kernel. when the kernel does
have to be modied, the changes tend to be fewer, because the microkernel is
a smaller kernel. the resulting operating system is easier to port from one
hardware design to another. the microkernel also provides more security
and reliability, since most services are running as userrather than kernel
processes. if a service fails, the rest of the operating system remains untouched.
some contemporary operating systems have used the microkernel
approach. tru64 unix (formerly digital unix) provides a unix interface to the
user, but it is implemented with a mach kernel. the mach kernel maps unix
system calls into messages to the appropriate user-level services. the mac os x
kernel (also known as darwin) is also partly based on the mach microkernel.
another example is qnx, a real-time operating system for embedded
systems. the qnx neutrino microkernel provides services for message passing
and process scheduling. it also handles low-level network communication
and hardware interrupts. all other services in qnx are provided by standard
processes that run outside the kernel in user mode.
unfortunately, the performance of microkernels can suffer due to increased
system-function overhead. consider the history of windows nt. the rst
release had a layered microkernel organization. this versions performance
was low compared with that of windows 95. windows nt 4.0 partially
corrected the performance problem by moving layers from user space to
kernel space and integrating them more closely. by the time windows xp
was designed, windows architecture had become more monolithic than
microkernel.
2.7.4
modules
perhaps the best current methodology for operating-system design involves
using loadable kernel modules. here, the kernel has a set of core components
and links in additional services via modules, either at boot time or during run
time. this type of design is common in modern implementations of unix, such
as solaris, linux, and mac os x, as well as windows.
the idea of the design is for the kernel to provide core services while
other services are implemented dynamically, as the kernel is running. linking
services dynamically is preferable to adding new features directly to the kernel,
which would require recompiling the kernel every time a change was made.
thus, for example, we might build cpu scheduling and memory management
algorithms directly into the kernel and then add support for different le
systems by way of loadable modules.
the overall result resembles a layered system in that each kernel section
has dened, protected interfaces; but it is more exible than a layered system,
because any module can call any other module. the approach is also similar to
the microkernel approach in that the primary module has only core functions
and knowledge of how to load and communicate with other modules; but it
2.7
operating-system structure
83
core solaris
kernel
file systems
loadable
system calls
executable
formats
streams
modules
miscellaneous
modules
device and
bus drivers
scheduling
classes
figure 2.15
solaris loadable modules.
is more efcient, because modules do not need to invoke message passing in
order to communicate.
the solaris operating system structure, shown in figure 2.15, is organized
around a core kernel with seven types of loadable kernel modules:
1. scheduling classes
2. file systems
3. loadable system calls
4. executable formats
5.
streams modules
6. miscellaneous
7. device and bus drivers
linux also uses loadable kernel modules, primarily for supporting device
drivers and le systems. we cover creating loadable kernel modules in linux
as a programming exercise at the end of this chapter.
2.7.5
hybrid systems
in practice, very few operating systems adopt a single, strictly dened
structure. instead, they combine different structures, resulting in hybrid
systems that address performance, security, and usability issues. for example,
both linux and solaris are monolithic, because having the operating system
in a single address space provides very efcient performance. however,
they are also modular, so that new functionality can be dynamically added
to the kernel. windows is largely monolithic as well (again primarily for
performance reasons), but it retains some behavior typical of microkernel
systems, including providing support for separate subsystems (known as
operating-system personalities) that run as user-mode processes. windows
systems also provide support for dynamically loadable kernel modules. we
provide case studies of linux and windows 7 in in chapters 18 and 19,
respectively. in the remainder of this section, we explore the structure of
84
chapter 2
operating-system structures
three hybrid systems: the apple mac os x operating system and the two most
prominent mobile operating systemsios and android.
2.7.5.1
mac os x
the apple mac os x operating system uses a hybrid structure. as shown in
figure 2.16, it is a layered system. the top layers include the aqua user interface
(figure 2.4) and a set of application environments and services. notably,
the cocoa environment species an api for the objective-c programming
language, which is used for writing mac os x applications. below these
layers is the kernel environment, which consists primarily of the mach
microkernel and the bsd unix kernel. mach provides memory management;
support for remote procedure calls (rpcs) and interprocess communication
(ipc) facilities, including message passing; and thread scheduling. the bsd
component provides a bsd command-line interface, support for networking
and le systems, and an implementation of posix apis, including pthreads.
in addition to mach and bsd, the kernel environment provides an i/o kit
for development of device drivers and dynamically loadable modules (which
mac os x refers to as kernel extensions). as shown in figure 2.16, the bsd
application environment can make use of bsd facilities directly.
2.7.5.2
ios
ios is a mobile operating system designed by apple to run its smartphone, the
iphone, as well as its tablet computer, the ipad. ios is structured on the mac
os x operating system, with added functionality pertinent to mobile devices,
but does not directly run mac os x applications. the structure of ios appears
in figure 2.17.
cocoa touch is an api for objective-c that provides several frameworks for
developing applications that run on ios devices. the fundamental difference
between cocoa, mentioned earlier, and cocoa touch is that the latter provides
support for hardware features unique to mobile devices, such as touch screens.
the media services layer provides services for graphics, audio, and video.
graphical user interface
aqua
application environments and services
kernel environment
java
cocoa
quicktime
bsd
mach
i/o kit
kernel extensions
bsd
figure 2.16
the mac os x structure.
2.7
operating-system structure
85
cocoa touch
media services
core services
core os
figure 2.17
architecture of apples ios.
the core services layer provides a variety of features, including support for
cloud computing and databases. the bottom layer represents the core operating
system, which is based on the kernel environment shown in figure 2.16.
2.7.5.3
android
the android operating system was designed by the open handset alliance
(led primarily by google) and was developed for android smartphones and
tablet computers. whereas ios is designed to run on apple mobile devices
and is close-sourced, android runs on a variety of mobile platforms and is
open-sourced, partly explaining its rapid rise in popularity. the structure of
android appears in figure 2.18.
android is similar to ios in that it is a layered stack of software that
provides a rich set of frameworks for developing mobile applications. at the
bottom of this software stack is the linux kernel, although it has been modied
by google and is currently outside the normal distribution of linux releases.
applications
application framework
android runtime
core libraries
dalvik
virtual machine
libraries
linux kernel
sqlite
opengl
surface
manager
webkit
libc
media
framework
figure 2.18
architecture of googles android.
86
chapter 2
operating-system structures
linux is used primarily for process, memory, and device-driver support for
hardware and has been expanded to include power management. the android
runtime environment includes a core set of libraries as well as the dalvik virtual
machine. software designers for android devices develop applications in the
java language. however, rather than using the standard java api, google has
designed a separate android api for java development. the java class les are
rst compiled to java bytecode and then translated into an executable le that
runs on the dalvik virtual machine. the dalvik virtual machine was designed
for android and is optimized for mobile devices with limited memory and
cpu processing capabilities.
the set of libraries available for android applications includes frameworks
for developing web browsers (webkit), database support (sqlite), and multi-
media. the libc library is similar to the standard c library but is much smaller
and has been designed for the slower cpus that characterize mobile devices.
2.8
operating-system debugging
we have mentioned debugging frequently in this chapter. here, we take a closer
look. broadly, debugging is the activity of nding and xing errors in a system,
both in hardware and in software. performance problems are considered bugs,
so debugging can also include performance tuning, which seeks to improve
performance by removing processing bottlenecks. in this section, we explore
debugging process and kernel errors and performance problems. hardware
debugging is outside the scope of this text.
2.8.1
failure analysis
if a process fails, most operating systems write the error information to a log
le to alert system operators or users that the problem occurred. the operating
system can also take a core dumpa capture of the memory of the process
and store it in a le for later analysis. (memory was referred to as the core
in the early days of computing.) running programs and core dumps can be
probed by a debugger, which allows a programmer to explore the code and
memory of a process.
debugging user-level process code is a challenge. operating-system kernel
debugging is even more complex because of the size and complexity of the
kernel, its control of the hardware, and the lack of user-level debugging tools.
a failure in the kernel is called a crash. when a crash occurs, error information
is saved to a log le, and the memory state is saved to a crash dump.
operating-system debugging and process debugging frequently use dif-
ferent tools and techniques due to the very different nature of these two tasks.
consider that a kernel failure in the le-system code would make it risky for
the kernel to try to save its state to a le on the le system before rebooting.
a common technique is to save the kernels memory state to a section of disk
set aside for this purpose that contains no le system. if the kernel detects
an unrecoverable error, it writes the entire contents of memory, or at least the
kernel-owned parts of the system memory, to the disk area. when the system
reboots, a process runs to gather the data from that area and write it to a crash
2.8
operating-system debugging
87
kernighans law
debugging is twice as hard as writing the code in the rst place. therefore,
if you write the code as cleverly as possible, you are, by denition, not smart
enough to debug it.
dump le within a le system for analysis. obviously, such strategies would
be unnecessary for debugging ordinary user-level processes.
2.8.2
performance tuning
we mentioned earlier that performance tuning seeks to improve performance
by removing processing bottlenecks. to identify bottlenecks, we must be able
to monitor system performance. thus, the operating system must have some
means of computing and displaying measures of system behavior. in a number
of systems, the operating system does this by producing trace listings of system
behavior. all interesting events are logged with their time and important
parameters and are written to a le. later, an analysis program can process
the log le to determine system performance and to identify bottlenecks and
inefciencies. these same traces can be run as input for a simulation of a
suggested improved system. traces also can help people to nd errors in
operating-system behavior.
another approach to performance tuning uses single-purpose, interactive
tools that allow users and administrators to question the state of various system
components to look for bottlenecks. one such tool employs the unix command
top to display the resources used on the system, as well as a sorted list of
the top resource-using processes. other tools display the state of disk i/o,
memory allocation, and network trafc.
the windows task manager is a similar tool for windows systems. the
taskmanagerincludesinformationforcurrentapplicationsaswell asprocesses,
cpu and memory usage, and networking statistics. a screen shot of the task
manager appears in figure 2.19.
making operating systems easier to understand, debug, and tune as they
run is an active area of research and implementation. a new generation of
kernel-enabled performance analysis tools has made signicant improvements
in how this goal can be achieved. next, we discuss a leading example of such
a tool: the solaris 10 dtrace dynamic tracing facility.
2.8.3
dtrace
dtrace is a facility that dynamically adds probes to a running system, both
in user processes and in the kernel. these probes can be queried via the d
programming language to determine an astonishing amount about the kernel,
the system state, and process activities. for example, figure 2.20 follows an
application as it executes a system call (ioctl()) and shows the functional
calls within the kernel as they execute to perform the system call. lines ending
with u are executed in user mode, and lines ending in k in kernel mode.
88
chapter 2
operating-system structures
figure 2.19
the windows task manager.
debugging the interactions between user-level and kernel code is nearly
impossible without a toolset that understands both sets of code and can
instrument the interactions. for that toolset to be truly useful, it must be able
to debug any area of a system, including areas that were not written with
debugging in mind, and do so without affecting system reliability. this tool
must also have a minimum performance impactideally it should have no
impact when not in use and a proportional impact during use. the dtrace tool
meets these requirements and provides a dynamic, safe, low-impact debugging
environment.
until the dtrace framework and tools became available with solaris 10,
kernel debugging was usually shrouded in mystery and accomplished via
happenstance and archaic code and tools. for example, cpus have a breakpoint
feature that will halt execution and allow a debugger to examine the state of the
system. then execution can continue until the next breakpoint or termination.
this method cannot be used in a multiuser operating-system kernel without
negatively affecting all of the users on the system. proling, which periodically
samples the instruction pointer to determine which code is being executed, can
show statistical trends but not individual activities. code can be included in
the kernel to emit specic data under specic circumstances, but that code
slows down the kernel and tends not to be included in the part of the kernel
where the specic problem being debugged is occurring.
2.8
operating-system debugging
89
# ./all.d pgrep xclock xeventsqueued
dtrace: script ./all.d matched 52377 probes
cpu function
0 > xeventsqueued
u
0 > _xeventsqueued
u
0	 > _x11transbytesreadable
u
0	 < _x11transbytesreadable
u
0	 > _x11transsocketbytesreadable u
0	 < _x11transsocketbytesreadable u
0	 > ioctl
u
0	 > ioctl
k
0		 > getf
k
0		 > set_active_fd
k
0		 < set_active_fd
k
0		 < getf
k
0		 > get_udatamodel
k
0		 < get_udatamodel
k
...
0		 > releasef
k
0		 > clear_active_fd
k
0		 < clear_active_fd
k
0		 > cv_broadcast
k
0		 < cv_broadcast
k
0		 < releasef
k
0	 < ioctl
k
0	 < ioctl
u
0 < _xeventsqueued
u
0 < xeventsqueued
u
figure 2.20
solaris 10 dtrace follows a system call within the kernel.
in contrast, dtrace runs on production systemssystems that are running
important or critical applicationsand causes no harm to the system. it
slows activities while enabled, but after execution it resets the system to its
pre-debugging state. it is also a broad and deep tool. it can broadly debug
everything happening in the system (both at the user and kernel levels and
between the user and kernel layers). it can also delve deep into code, showing
individual cpu instructions or kernel subroutine activities.
dtrace is composed of a compiler, a framework, providers of probes
written within that framework, and consumers of those probes. dtrace
providers create probes. kernel structures exist to keep track of all probes that
the providers have created. the probes are stored in a hash-table data structure
that is hashed by name and indexed according to unique probe identiers.
when a probe is enabled, a bit of code in the area to be probed is rewritten
to call dtrace probe(probe identifier) and then continue with the codes
original operation. different providers create different kinds of probes. for
example, a kernel system-call probe works differently from a user-process
probe, and that is different from an i/o probe.
dtrace features a compiler that generates a byte code that is run in the
kernel. this code is assured to be safe by the compiler. for example, no loops
are allowed, and only specic kernel state modications are allowed when
specically requested. only users with dtrace privileges (or root users)
90
chapter 2
operating-system structures
are allowed to use dtrace, as it can retrieve private kernel data (and modify
data if requested). the generated code runs in the kernel and enables probes.
it also enables consumers in user mode and enables communications between
the two.
a dtrace consumer is code that is interested in a probe and its results.
a consumer requests that the provider create one or more probes. when a
probe res, it emits data that are managed by the kernel. within the kernel,
actions called enabling control blocks, or ecbs, are performed when probes
re. one probe can cause multiple ecbs to execute if more than one consumer
is interested in that probe. each ecb contains a predicate (if statement) that
can lter out that ecb. otherwise, the list of actions in the ecb is executed. the
most common action is to capture some bit of data, such as a variables value at
that point of the probe execution. by gathering such data, a complete picture of
a user or kernel action can be built. further, probes ring from both user space
and the kernel can show how a user-level action caused kernel-level reactions.
such data are invaluable for performance monitoring and code optimization.
once the probe consumer terminates, its ecbs are removed. if there are no
ecbs consuming a probe, the probe is removed. that involves rewriting the
code to remove the dtrace probe() call and put back the original code. thus,
before a probe is created and after it is destroyed, the system is exactly the
same, as if no probing occurred.
dtrace takes care to assure that probes do not use too much memory or
cpu capacity, which could harm the running system. the buffers used to hold
the probe results are monitored for exceeding default and maximum limits.
cpu time for probe execution is monitored as well. if limits are exceeded, the
consumer is terminated, along with the offending probes. buffers are allocated
per cpu to avoid contention and data loss.
an example of d code and its output shows some of its utility. the following
program shows the dtrace code to enable scheduler probes and record the
amount of cpu time of each process running with user id 101 while those
probes are enabled (that is, while the program runs):
sched:::on-cpu
uid == 101
{
self->ts = timestamp;
}
sched:::off-cpu
self->ts
{
@time[execname] = sum(timestamp - self->ts);
self->ts = 0;
}
the output of the program, showing the processes and how much time (in
nanoseconds) they spend running on the cpus, is shown in figure 2.21.
because dtrace is part of the open-source opensolaris version of the solaris
10 operating system, it has been added to other operating systems when those
2.9
operating-system generation
91
# dtrace -s sched.d
dtrace: script sched.d matched 6 probes
c
gnome-settings-d
142354
gnome-vfs-daemon
158243
dsdm
189804
wnck-applet
200030
gnome-panel
277864
clock-applet
374916
mapping-daemon
385475
xscreensaver
514177
metacity
539281
xorg
2579646
gnome-terminal
5007269
mixer applet2
7388447
java
10769137
figure 2.21
output of the d code.
systems do not have conicting license agreements. for example, dtrace has
been added to mac os x and freebsd and will likely spread further due to its
unique capabilities. other operating systems, especially the linux derivatives,
are adding kernel-tracing functionality as well. still other operating systems
are beginning to include performance and tracing tools fostered by research at
various institutions, including the paradyn project.
2.9
operating-system generation
it is possible to design, code, and implement an operating system specically
for one machine at one site. more commonly, however, operating systems
are designed to run on any of a class of machines at a variety of sites with
a variety of peripheral congurations. the system must then be congured
or generated for each specic computer site, a process sometimes known as
system generation sysgen.
the operating system is normally distributed on disk, on cd-rom or
dvd-rom, or as an iso image, which is a le in the format of a cd-rom
or dvd-rom. to generate a system, we use a special program. this sysgen
program reads from a given le, or asks the operator of the system for
information concerning the specic conguration of the hardware system, or
probes the hardware directly to determine what components are there. the
following kinds of information must be determined.
what cpu is to be used? what options (extended instruction sets, oating-
point arithmetic, and so on) are installed? for multiple cpu systems, each
cpu may be described.
how will the boot disk be formatted? how many sections, or partitions,
will it be separated into, and what will go into each partition?
92
chapter 2
operating-system structures
how much memory is available? some systems will determine this value
themselves by referencing memory location after memory location until an
illegal address fault is generated. this procedure denes the nal legal
address and hence the amount of available memory.
what devices are available? the system will need to know how to address
each device (the device number), the device interrupt number, the devices
type and model, and any special device characteristics.
what operating-system options are desired, or what parameter values are
to be used? these options or values might include how many buffers of
which sizes should be used, what type of cpu-scheduling algorithm is
desired, what the maximum number of processes to be supported is, and
so on.
once this information is determined, it can be used in several ways. at one
extreme, a system administrator can use it to modify a copy of the source code of
the operating system. the operating system then is completely compiled. data
declarations, initializations, and constants, along with conditional compilation,
produce an output-object version of the operating system that is tailored to the
system described.
at a slightly less tailored level, the system description can lead to the
creation of tables and the selection of modules from a precompiled library.
these modules are linked together to form the generated operating system.
selection allows the library to contain the device drivers for all supported i/o
devices, but only those needed are linked into the operating system. because
the system is not recompiled, system generation is faster, but the resulting
system may be overly general.
at the other extreme, it is possible to construct a system that is completely
table driven. all the code is always part of the system, and selection occurs at
execution time, rather than at compile or link time. system generation involves
simply creating the appropriate tables to describe the system.
the major differences among these approaches are the size and generality
of the generated system and the ease of modifying it as the hardware
conguration changes. consider the cost of modifying the system to support a
newly acquired graphics terminal or another disk drive. balanced against that
cost, of course, is the frequency (or infrequency) of such changes.
2.10 system boot
after an operating system is generated, it must be made available for use by
the hardware. but how does the hardware know where the kernel is or how to
load that kernel? the procedure of starting a computer by loading the kernel
is known as booting the system. on most computer systems, a small piece of
code known as the bootstrap program or bootstrap loader locates the kernel,
loads it into main memory, and starts its execution. some computer systems,
such as pcs, use a two-step process in which a simple bootstrap loader fetches
a more complex boot program from disk, which in turn loads the kernel.
when a cpu receives a reset eventfor instance, when it is powered up
or rebootedthe instruction register is loaded with a predened memory
2.11
summary
93
location, and execution starts there. at that location is the initial bootstrap
program. this program is in the form of read-only memory (rom), because
the ram is in an unknown state at system startup. rom is convenient because
it needs no initialization and cannot easily be infected by a computer virus.
the bootstrap program can perform a variety of tasks. usually, one task
is to run diagnostics to determine the state of the machine. if the diagnostics
pass, the program can continue with the booting steps. it can also initialize all
aspects of the system, from cpu registers to device controllers and the contents
of main memory. sooner or later, it starts the operating system.
some systemssuch as cellular phones, tablets, and game consolesstore
the entire operating system in rom. storing the operating system in rom is
suitable for small operating systems, simple supporting hardware, and rugged
operation. a problem with this approach is that changing the bootstrap code
requires changing the rom hardware chips. some systems resolve this problem
by using erasable programmable read-only memory (eprom), which is read-
only except when explicitly given a command to become writable. all forms
of rom are also known as rmware, since their characteristics fall somewhere
between those of hardware and those of software. a problem with rmware
in general is that executing code there is slower than executing code in ram.
some systems store the operating system in rmware and copy it to ram for
fast execution. a nal issue with rmware is that it is relatively expensive, so
usually only small amounts are available.
for large operating systems (including most general-purpose operating
systems like windows, mac os x, and unix) or for systems that change
frequently, the bootstrap loader is stored in rmware, and the operating system
is on disk. in this case, the bootstrap runs diagnostics and has a bit of code
that can read a single block at a xed location (say block zero) from disk into
memory and execute the code from that boot block. the program stored in the
boot block may be sophisticated enough to load the entire operating system
into memory and begin its execution. more typically, it is simple code (as it ts
in a single disk block) and knows only the address on disk and length of the
remainder of the bootstrap program. grub is an example of an open-source
bootstrap program for linux systems. all of the disk-bound bootstrap, and the
operating system itself, can be easily changed by writing new versions to disk.
a disk that has a boot partition (more on that in section 10.5.1) is called a boot
disk or system disk.
now that the full bootstrap program has been loaded, it can traverse the
le system to nd the operating system kernel, load it into memory, and start
its execution. it is only at this point that the system is said to be running.
2.11
summary
operating systems provide a number of services. at the lowest level, system
calls allow a running program to make requests from the operating system
directly. at a higher level, the command interpreter or shell provides a
mechanism for a user to issue a request without writing a program. commands
may come from les during batch-mode execution or directly from a terminal
or desktop gui when in an interactive or time-shared mode. system programs
are provided to satisfy many common user requests.
94
chapter 2
operating-system structures
the types of requests vary according to level. the system-call level must
provide the basic functions, such as process control and le and device
manipulation. higher-level requests, satised by the command interpreter or
system programs, are translated into a sequence of system calls. system services
can be classied into several categories: program control, status requests, and
i/o requests. program errors can be considered implicit requests for service.
the design of a new operating system is a major task. it is important that
the goals of the system be well dened before the design begins. the type of
system desired is the foundation for choices among various algorithms and
strategies that will be needed.
throughout the entire design cycle, we must be careful to separate policy
decisions from implementation details (mechanisms). this separation allows
maximum exibility if policy decisions are to be changed later.
once an operating system is designed, it must be implemented. oper-
ating systems today are almost always written in a systems-implementation
language or in a higher-level language. this feature improves their implemen-
tation, maintenance, and portability.
a system as large and complex as a modern operating system must
be engineered carefully. modularity is important. designing a system as a
sequence of layers or using a microkernel is considered a good technique. many
operating systems now support dynamically loaded modules, which allow
adding functionality to an operating system while it is executing. generally,
operating systems adopt a hybrid approach that combines several different
types of structures.
debugging process and kernel failures can be accomplished through the
use of debuggers and other tools that analyze core dumps. tools such as dtrace
analyze production systems to nd bottlenecks and understand other system
behavior.
to create an operating system for a particular machine conguration, we
must perform system generation. for the computer system to begin running,
the cpu must initialize and start executing the bootstrap program in rmware.
the bootstrap can execute the operating system directly if the operating system
is also in the rmware, or it can complete a sequence in which it loads
progressively smarter programs from rmware and disk until the operating
system itself is loaded into memory and executed.
practice exercises
2.1
what is the purpose of system calls?
2.2
what are the ve major activities of an operating system with regard to
process management?
2.3
what are the three major activities of an operating system with regard
to memory management?
2.4
what are the three major activities of an operating system with regard
to secondary-storage management?
2.5
what is the purpose of the command interpreter? why is it usually
separate from the kernel?
exercises
95
2.6
what system calls have to be executed by a command interpreter or shell
in order to start a new process?
2.7
what is the purpose of system programs?
2.8
what is the main advantage of the layered approach to system design?
what are the disadvantages of the layered approach?
2.9
list ve services provided by an operating system, and explain how each
creates convenience for users. in which cases would it be impossible for
user-level programs to provide these services? explain your answer.
2.10
why do some systems store the operating system in rmware, while
others store it on disk?
2.11
how could a system be designed to allow a choice of operating systems
from which to boot? what would the bootstrap program need to do?
exercises
2.12
the services and functions provided by an operating system can be
divided into two main categories. briey describe the two categories,
and discuss how they differ.
2.13
describe three general methods for passing parameters to the operating
system.
2.14
describe how you could obtain a statistical prole of the amount of time
spent by a program executing different sections of its code. discuss the
importance of obtaining such a statistical prole.
2.15
what are the ve major activities of an operating system with regard to
le management?
2.16
what are the advantages and disadvantages of using the same system-
call interface for manipulating both les and devices?
2.17
would it be possible for the user to develop a new command interpreter
using the system-call interface provided by the operating system?
2.18
what are the two models of interprocess communication? what are the
strengths and weaknesses of the two approaches?
2.19
why is the separation of mechanism and policy desirable?
2.20
it is sometimes difcult to achieve a layered approach if two components
of the operating system are dependent on each other. identify a scenario
in which it is unclear how to layer two system components that require
tight coupling of their functionalities.
2.21
what is the main advantage of the microkernel approach to system
design? how do user programs and system services interact in a
microkernel architecture? what are the disadvantages of using the
microkernel approach?
2.22
what are the advantages of using loadable kernel modules?
96
chapter 2
operating-system structures
2.23
how are ios and android similar? how are they different?
2.24
explain why java programs running on android systems do not use the
standard java api and virtual machine.
2.25
the experimental synthesis operating system has an assembler incor-
porated in the kernel. to optimize system-call performance, the kernel
assembles routines within kernel space to minimize the path that the
system call must take through the kernel. this approach is the antithesis
of the layered approach, in which the path through the kernel is extended
to make building the operating system easier. discuss the pros and cons
of the synthesis approach to kernel design and system-performance
optimization.
programming problems
2.26
in section 2.3, we described a program that copies the contents of one le
to a destination le. this program works by rst prompting the user for
the name of the source and destination les. write this program using
either the windows or posix api. be sure to include all necessary error
checking, including ensuring that the source le exists.
once you have correctly designed and tested the program, if you
used a system that supports it, run the program using a utility that traces
system calls. linux systems provide the strace utility, and solaris and
mac os x systems use the dtrace command. as windows systems do
not provide such features, you will have to trace through the windows
version of this program using a debugger.
programming projects
linux kernel modules
in this project, you will learn how to create a kernel module and load it into the
linux kernel. the project can be completed using the linux virtual machine
that is available with this text. although you may use an editor to write these
c programs, you will have to use the terminal application to compile the
programs, and you will have to enter commands on the command line to
manage the modules in the kernel.
as youll discover, the advantage of developing kernel modules is that it
is a relatively easy method of interacting with the kernel, thus allowing you to
write programs that directly invoke kernel functions. it is important for you
to keep in mind that you are indeed writing kernel code that directly interacts
with the kernel. that normally means that any errors in the code could crash
the system! however, since you will be using a virtual machine, any failures
will at worst only require rebooting the system.
part icreating kernel modules
the rst part of this project involves following a series of steps for creating and
inserting a module into the linux kernel.
programming projects
97
you can list all kernel modules that are currently loaded by entering the
command
lsmod
this command will list the current kernel modules in three columns: name,
size, and where the module is being used.
the following program (named simple.c and available with the source
code for this text) illustrates a very basic kernel module that prints appropriate
messages when the kernel module is loaded and unloaded.
#include <linux/init.h>
#include <linux/kernel.h>
#include <linux/module.h>
/* this function is called when the module is loaded. */
int simple init(void)
{
printk(kern info "loading module\n");
return 0;
}
/* this function is called when the module is removed. */
void simple exit(void)
{
printk(kern info "removing module\n");
}
/* macros for registering module entry and exit points. */
module init(simple init);
module exit(simple exit);
module license("gpl");
module description("simple module");
module author("sgg");
the function simple init() is the module entry point, which represents
the function that is invoked when the module is loaded into the kernel.
similarly, the simple exit() function is the module exit pointthe function
that is called when the module is removed from the kernel.
the module entry point function must return an integer value, with 0
representing success and any other value representing failure. the module exit
point function returns void. neither the module entry point nor the module
exit point is passed any parameters. the two following macros are used for
registering the module entry and exit points with the kernel:
module init()
module exit()
98
chapter 2
operating-system structures
notice how both the module entry and exit point functions make calls
to the printk() function. printk() is the kernel equivalent of printf(),
yet its output is sent to a kernel log buffer whose contents can be read by
the dmesg command. one difference between printf() and printk() is that
printk() allows us to specify a priority ag whose values are given in the
<linux/printk.h> include le. in this instance, the priority is kern info,
which is dened as an informational message.
the nal linesmodule license(), module description(), and mod-
ule author()represent details regarding the software license, description
of the module, and author. for our purposes, we do not depend on this
information, but we include it because it is standard practice in developing
kernel modules.
this kernel module simple.c is compiled using the makefile accom-
panying the source code with this project. to compile the module, enter the
following on the command line:
make
the compilation produces several les. the le simple.ko represents the
compiled kernel module. the following step illustrates inserting this module
into the linux kernel.
loading and removing kernel modules
kernel modulesare loaded usingtheinsmodcommand, whichisrunasfollows:
sudo insmod simple.ko
to check whether the module has loaded, enter the lsmod command and search
for the module simple. recall that the module entry point is invoked when
the module is inserted into the kernel. to check the contents of this message in
the kernel log buffer, enter the command
dmesg
you should see the message "loading module."
removing the kernel module involves invoking the rmmod command
(notice that the .ko sufx is unnecessary):
sudo rmmod simple
be sure to check with the dmesg command to ensure the module has been
removed.
because the kernel log buffer can ll up quickly, it often makes sense to
clear the buffer periodically. this can be accomplished as follows:
sudo dmesg -c
programming projects
99
part i assignment
proceed through the steps described above to create the kernel module and to
load and unload the module. be sure to check the contents of the kernel log
buffer using dmesg to ensure you have properly followed the steps.
part iikernel data structures
the second part of this project involves modifying the kernel module so that
it uses the kernel linked-list data structure.
in section 1.10, we covered various data structures that are common in
operating systems. the linux kernel provides several of these structures. here,
we explore using the circular, doubly linked list that is available to kernel
developers. much of what we discuss is available in the linux source code
in this instance, the include le <linux/list.h>and we recommend that
you examine this le as you proceed through the following steps.
initially, you must dene a struct containing the elements that are to be
inserted in the linked list. the following c struct denes birthdays:
struct birthday {
int day;
int month;
int year;
struct list head list;
}
notice the member struct list head list. the list head structure is
dened in the include le <linux/types.h>. its intention is to embed the
linked list within the nodes that comprise the list. this list head structure is
quite simpleit merely holds two members, next and prev, that point to the
next and previous entries in the list. by embedding the linked list within the
structure, linux makes it possible to manage the data structure with a series of
macro functions.
inserting elements into the linked list
we can declare a list head object, which we use as a reference to the head of
the list by using the list head() macro
static list head(birthday list);
this macro denes and initializes the variable birthday list, which is of type
struct list head.
100
chapter 2
operating-system structures
we create and initialize instances of struct birthday as follows:
struct birthday *person;
person = kmalloc(sizeof(*person), gfp kernel);
person->day = 2;
person->month= 8;
person->year = 1995;
init list head(&person->list);
the kmalloc() function is the kernel equivalent of the user-level malloc()
function for allocating memory, except that kernel memory is being allocated.
(the gfp kernel ag indicates routine kernel memory allocation.) the macro
init list head() initializes the list member in struct birthday. we can
then add this instance to the end of the linked list using the list add tail()
macro:
list add tail(&person->list, &birthday list);
traversing the linked list
traversing the list involves using the list for each entry() macro, which
accepts three parameters:
a pointer to the structure being iterated over
a pointer to the head of the list being iterated over
the name of the variable containing the list head structure
the following code illustrates this macro:
struct birthday *ptr;
list for each entry(ptr, &birthday list, list) {
/* on each iteration ptr points */
/* to the next birthday struct */
}
removing elements from the linked list
removing elements from the list involves using the list del() macro, which
is passed a pointer to struct list head
list del(struct list head *element)
this removes element from the list while maintaining the structure of the
remainder of the list.
perhaps the simplest approach for removing all elements from a
linked list is to remove each element as you traverse the list. the macro
list for each entry safe() behaves much like list for each entry()
bibliographical notes
101
except that it is passed an additional argument that maintains the value of the
next pointer of the item being deleted. (this is necessary for preserving the
structure of the list.) the following code example illustrates this macro:
struct birthday *ptr, *next
list for each entry safe(ptr,next,&birthday list,list) {
/* on each iteration ptr points */
/* to the next birthday struct */
list del(&ptr->list);
kfree(ptr);
}
notice that after deleting each element, we return memory that was previously
allocated with kmalloc() back to the kernel with the call to kfree(). careful
memory managementwhich includes releasing memory to prevent memory
leaksis crucial when developing kernel-level code.
part ii assignment
in the module entry point, create a linked list containing ve struct birthday
elements. traverse the linked list and output its contents to the kernel log buffer.
invoke the dmesg command to ensure the list is properly constructed once the
kernel module has been loaded.
in the module exit point, delete the elements from the linked list and return
the free memory back to the kernel. again, invoke the dmesg command to check
that the list has been removed once the kernel module has been unloaded.
bibliographical notes
[dijkstra (1968)] advocated the layered approach to operating-system design.
[brinch-hansen (1970)] was an early proponent of constructing an operating
system as a kernel (or nucleus) on which more complete systems could be
built. [tarkoma and lagerspetz (2011)] provide an overview of various mobile
operating systems, including android and ios.
ms-dos, version 3.1, is described in [microsoft (1986)]. windows nt
and windows 2000 are described by [solomon (1998)] and [solomon and
russinovich (2000)]. windows xp internals are described in [russinovich
and solomon (2009)]. [hart (2005)] covers windows systems programming
in detail. bsd unix is described in [mckusick et al. (1996)]. [love (2010)] and
[mauerer (2008)] thoroughly discuss the linux kernel. in particular, [love
(2010)] covers linux kernel modules as well as kernel data structures. several
unix systemsincluding machare treated in detail in [vahalia (1996)]. mac
os x is presented at http://www.apple.com/macosx and in [singh (2007)].
solaris is fully described in [mcdougall and mauro (2007)].
dtrace is discussed in [gregg and mauro (2011)]. the dtrace source code
is available at http://src.opensolaris.org/source/.
102
chapter 2
operating-system structures
bibliography
[brinch-hansen (1970)]
p. brinch-hansen, the nucleus of a multiprogram-
ming system, communications of the acm, volume 13, number 4 (1970), pages
238241 and 250.
[dijkstra (1968)]
e. w. dijkstra, the structure of the the multiprogramming
system, communications of the acm, volume 11, number 5 (1968), pages
341346.
[gregg and mauro (2011)]
b. gregg and j. mauro, dtracedynamic tracing in
oracle solaris, mac os x, and freebsd, prentice hall (2011).
[hart (2005)]
j. m. hart, windows system programming, third edition, addison-
wesley (2005).
[love (2010)]
r. love, linux kernel development, third edition, developers
library (2010).
[mauerer (2008)]
w. mauerer, professional linux kernel architecture, john wiley
and sons (2008).
[mcdougall and mauro (2007)]
r. mcdougall and j. mauro, solaris internals,
second edition, prentice hall (2007).
[mckusick et al. (1996)]
m. k. mckusick, k. bostic, and m. j. karels, the design
and implementation of the 4.4 bsd unix operating system, john wiley and sons
(1996).
[microsoft (1986)]
microsoft ms-dos users reference and microsoft ms-dos
programmers reference. microsoft press (1986).
[russinovich and solomon (2009)]
m. e. russinovich and d. a. solomon, win-
dows internals: including windows server 2008 and windows vista, fifth edition,
microsoft press (2009).
[singh (2007)]
a. singh, mac os x internals: a systems approach, addison-
wesley (2007).
[solomon (1998)]
d. a. solomon, inside windows nt, second edition, microsoft
press (1998).
[solomon and russinovich (2000)]
d. a. solomon and m. e. russinovich, inside
microsoft windows 2000, third edition, microsoft press (2000).
[tarkoma and lagerspetz (2011)]
s. tarkoma and e. lagerspetz, arching over
the mobile computing chasm: platforms and runtimes, ieee computer,
volume 44, (2011), pages 2228.
[vahalia (1996)]
u. vahalia, unix internals: the new frontiers, prentice hall
(1996).
part two
process
management
a process can be thought of as a program in execution. a process will
need certain resourcessuch as cpu time, memory, les, and i/o devices
to accomplish its task. these resources are allocated to the process
either when it is created or while it is executing.
a process is the unit of work in most systems. systems consist of
a collection of processes: operating-system processes execute system
code, and user processes execute user code. all these processes may
execute concurrently.
although traditionally a process contained only a single thread of
control as it ran, most modern operating systems now support processes
that have multiple threads.
the operating system is responsible for several important aspects of
process and thread management: the creation and deletion of both user
and system processes; the scheduling of processes; and the provision of
mechanisms for synchronization, communication, and deadlock handling
for processes.
3
c h a p t e r
processes
early computers allowed only one program to be executed at a time. this
program had complete control of the system and had access to all the systems
resources. in contrast, contemporary computer systems allow multiple pro-
grams to be loaded into memory and executed concurrently. this evolution
required rmer control and more compartmentalization of the various pro-
grams; and these needs resulted in the notion of a process, which is a program
in execution. a process is the unit of work in a modern time-sharing system.
the more complex the operating system is, the more it is expected to do on
behalf of its users. although its main concern is the execution of user programs,
it also needs to take care of various system tasks that are better left outside the
kernel itself. a system therefore consists of a collection of processes: operating-
system processes executing system code and user processes executing user
code. potentially, all these processes can execute concurrently, with the cpu (or
cpus) multiplexed among them. by switching the cpu between processes, the
operating system can make the computer more productive. in this chapter, you
will read about what processes are and how they work.
chapter objectives
to introduce the notion of a processa program in execution, which forms
the basis of all computation.
to describe the various features of processes, including scheduling,
creation, and termination.
to explore interprocess communication using shared memory and mes-
sage passing.
to describe communication in clientserver systems.
3.1
process concept
a question that arises in discussing operating systems involves what to call
all the cpu activities. a batch system executes jobs, whereas a time-shared
105
106
chapter 3
processes
system has user programs, or tasks. even on a single-user system, a user may
be able to run several programs at one time: a word processor, a web browser,
and an e-mail package. and even if a user can execute only one program at a
time, such as on an embedded device that does not support multitasking, the
operating system may need to support its own internal programmed activities,
such as memory management. in many respects, all these activities are similar,
so we call all of them processes.
the terms job and process are used almost interchangeably in this text.
although we personally prefer the term process, much of operating-system
theory and terminology was developed during a time when the major activity
of operating systems was job processing. it would be misleading to avoid
the use of commonly accepted terms that include the word job (such as job
scheduling) simply because process has superseded job.
3.1.1
the process
informally, as mentioned earlier, a process is a program in execution. a process
is more than the program code, which is sometimes known as the text section.
it also includes the current activity, as represented by the value of the program
counter and the contents of the processors registers. a process generally also
includes the process stack, which contains temporary data (such as function
parameters, return addresses, and local variables), and a data section, which
contains global variables. a process may also include a heap, which is memory
that is dynamically allocated during process run time. the structure of a process
in memory is shown in figure 3.1.
we emphasize that a program by itself is not a process. a program is a
passive entity, such as a le containing a list of instructions stored on disk
(often called an executable le). in contrast, a process is an active entity,
with a program counter specifying the next instruction to execute and a set
of associated resources. a program becomes a process when an executable le
is loaded into memory. two common techniques for loading executable les
text
0
max
data
heap
stack
figure 3.1
process in memory.
3.1
process concept
107
are double-clicking an icon representing the executable le and entering the
name of the executable le on the command line (as in prog.exe or a.out).
although two processes may be associated with the same program, they
are nevertheless considered two separate execution sequences. for instance,
several users may be running different copies of the mail program, or the same
user may invoke many copies of the web browser program. each of these is a
separate process; and although the text sections are equivalent, the data, heap,
and stack sections vary. it is also common to have a process that spawns many
processes as it runs. we discuss such matters in section 3.4.
note that a process itself can be an execution environment for other
code. the java programming environment provides a good example. in most
circumstances, an executable java program is executed within the java virtual
machine (jvm). the jvm executes as a process that interprets the loaded java
code and takes actions (via native machine instructions) on behalf of that code.
for example, to run the compiled java program program.class, we would
enter
java program
the command java runs the jvm as an ordinary process, which in turns
executes the java program program in the virtual machine. the concept is the
same as simulation, except that the code, instead of being written for a different
instruction set, is written in the java language.
3.1.2
process state
as a process executes, it changes state. the state of a process is dened in part
by the current activity of that process. a process may be in one of the following
states:
new. the process is being created.
running. instructions are being executed.
waiting. the process is waiting for some event to occur (such as an i/o
completion or reception of a signal).
ready. the process is waiting to be assigned to a processor.
terminated. the process has nished execution.
these names are arbitrary, and they vary across operating systems. the states
that they represent are found on all systems, however. certain operating
systems also more nely delineate process states. it is important to realize
that only one process can be running on any processor at any instant. many
processes may be ready and waiting, however. the state diagram corresponding
to these states is presented in figure 3.2.
3.1.3
process control block
each process is represented in the operating system by a process control block
(pcb)also called a task control block. a pcbis shown in figure 3.3. it contains
many pieces of information associated with a specic process, including these:
108
chapter 3
processes
new
terminated
running
ready
admitted
interrupt
scheduler dispatch
i/o or event completion
i/o or event wait
exit
waiting
figure 3.2
diagram of process state.
process state. the state may be new, ready, running, waiting, halted, and
so on.
program counter. the counter indicates the address of the next instruction
to be executed for this process.
cpu registers. the registers vary in number and type, depending on
the computer architecture. they include accumulators, index registers,
stack pointers, and general-purpose registers, plus any condition-code
information. along with the program counter, this state information must
be saved when an interrupt occurs, to allow the process to be continued
correctly afterward (figure 3.4).
cpu-scheduling information. this information includes a process priority,
pointers to scheduling queues, and any other scheduling parameters.
(chapter 6 describes process scheduling.)
memory-management information. this information may include such
items as the value of the base and limit registers and the page tables, or the
segment tables, depending on the memory system used by the operating
system (chapter 8).
process state
process number
program counter
memory limits
list of open files
registers
figure 3.3
process control block (pcb).
3.1
process concept
109
process p0
process p1
save state into pcb0
save state into pcb1
reload state from pcb1
reload state from pcb0
operating system
idle
idle
executing
idle
executing
executing
interrupt or system call
interrupt or system call
figure 3.4
diagram showing cpu switch from process to process.
accounting information. this information includes the amount of cpu
and real time used, time limits, account numbers, job or process numbers,
and so on.
i/o status information. this information includes the list of i/o devices
allocated to the process, a list of open les, and so on.
in brief, the pcb simply serves as the repository for any information that may
vary from process to process.
3.1.4
threads
the process model discussed so far has implied that a process is a program that
performs a single thread of execution. for example, when a process is running
a word-processor program, a single thread of instructions is being executed.
this single thread of control allows the process to perform only one task at
a time. the user cannot simultaneously type in characters and run the spell
checker within the same process, for example. most modern operating systems
have extended the process concept to allow a process to have multiple threads
of execution and thus to perform more than one task at a time. this feature
is especially benecial on multicore systems, where multiple threads can run
in parallel. on a system that supports threads, the pcb is expanded to include
information for each thread. other changes throughout the system are also
needed to support threads. chapter 4 explores threads in detail.
110
chapter 3
processes
process representation in linux
the process control block in the linux operating system is represented by
the c structure task struct, which is found in the <linux/sched.h>
include le in the kernel source-code directory. this structure contains all the
necessary information for representing a process, including the state of the
process, scheduling and memory-management information, list of open les,
and pointers to the processs parent and a list of its children and siblings. (a
processs parent is the process that created it; its children are any processes
that it creates. its siblings are children with the same parent process.) some
of these elds include:
long state; /* state of the process */
struct sched entity se; /* scheduling information */
struct task struct *parent; /* this processs parent */
struct list head children; /* this processs children */
struct files struct *files; /* list of open files */
struct mm struct *mm; /* address space of this process */
for example, the state of a process is represented by the eld long state
in this structure. within the linux kernel, all active processes are represented
using a doubly linked list of task struct. the kernel maintains a pointer
currentto the process currently executing on the system, as shown below:
struct task_struct
process information
struct task_struct
process information
current
(currently executing proccess)
struct task_struct
process information
as an illustration of how the kernel might manipulate one of the elds in
the task struct for a specied process, lets assume the system would like
to change the state of the process currently running to the value new state.
if current is a pointer to the process currently executing, its state is changed
with the following:
current->state = new state;
3.2
process scheduling
the objective of multiprogramming is to have some process running at all
times, to maximize cpu utilization. the objective of time sharing is to switch the
cpu among processes so frequently that users can interact with each program
3.2
process scheduling
111
queue header
pcb7
pcb3
pcb5
pcb14
pcb6
pcb2
head
head
head
head
head
ready
queue
disk
unit 0
terminal
unit 0
mag
tape
unit 0
mag
tape
unit 1
tail
registers
registers
tail
tail
tail
tail
figure 3.5
the ready queue and various i/o device queues.
while it is running. to meet these objectives, the process scheduler selects
an available process (possibly from a set of several available processes) for
program execution on the cpu. for a single-processor system, there will never
be more than one running process. if there are more processes, the rest will
have to wait until the cpu is free and can be rescheduled.
3.2.1
scheduling queues
as processes enter the system, they are put into a job queue, which consists
of all processes in the system. the processes that are residing in main memory
and are ready and waiting to execute are kept on a list called the ready queue.
this queue is generally stored as a linked list. a ready-queue header contains
pointers to the rst and nal pcbs in the list. each pcb includes a pointer eld
that points to the next pcb in the ready queue.
the system also includes other queues. when a process is allocated the
cpu, it executes for a while and eventually quits, is interrupted, or waits for
the occurrence of a particular event, such as the completion of an i/o request.
suppose the process makes an i/o request to a shared device, such as a disk.
since there are many processes in the system, the disk may be busy with the
i/o request of some other process. the process therefore may have to wait for
the disk. the list of processes waiting for a particular i/o device is called a
device queue. each device has its own device queue (figure 3.5).
112
chapter 3
processes
ready queue
cpu
i/o
i/o queue
i/o request
time slice
expired
fork a
child
wait for an
interrupt
interrupt
occurs
child
executes
figure 3.6
queueing-diagram representation of process scheduling.
a common representation of process scheduling is a queueing diagram,
such as that in figure 3.6. each rectangular box represents a queue. two types
of queues are present: the ready queue and a set of device queues. the circles
represent the resources that serve the queues, and the arrows indicate the ow
of processes in the system.
a new process is initially put in the ready queue. it waits there until it is
selected for execution, or dispatched. once the process is allocated the cpu
and is executing, one of several events could occur:
the process could issue an i/o request and then be placed in an i/o queue.
the process could create a new child process and wait for the childs
termination.
the process could be removed forcibly from the cpu, as a result of an
interrupt, and be put back in the ready queue.
in the rst two cases, the process eventually switches from the waiting state
to the ready state and is then put back in the ready queue. a process continues
this cycle until it terminates, at which time it is removed from all queues and
has its pcb and resources deallocated.
3.2.2
schedulers
a process migrates among the various scheduling queues throughout its
lifetime. the operating system must select, for scheduling purposes, processes
from these queues in some fashion. the selection process is carried out by the
appropriate scheduler.
often, in a batch system, more processes are submitted than can be executed
immediately. these processes are spooled to a mass-storage device (typically a
disk), where they are kept for later execution. the long-term scheduler, or job
scheduler, selects processes from this pool and loads them into memory for
3.2
process scheduling
113
execution. the short-term scheduler, or cpu scheduler, selects from among
the processes that are ready to execute and allocates the cpu to one of them.
the primary distinction between these two schedulers lies in frequency
of execution. the short-term scheduler must select a new process for the cpu
frequently. a process may execute for only a few milliseconds before waiting
for an i/o request. often, the short-term scheduler executes at least once every
100 milliseconds. because of the short time between executions, the short-term
scheduler must be fast. if it takes 10 milliseconds to decide to execute a process
for 100 milliseconds, then 10/(100 + 10) = 9 percent of the cpu is being used
(wasted) simply for scheduling the work.
the long-term scheduler executes much less frequently; minutes may sep-
arate the creation of one new process and the next. the long-term scheduler
controls the degree of multiprogramming (the number of processes in mem-
ory). if the degree of multiprogramming is stable, then the average rate of
process creation must be equal to the average departure rate of processes
leaving the system. thus, the long-term scheduler may need to be invoked
only when a process leaves the system. because of the longer interval between
executions, the long-term scheduler can afford to take more time to decide
which process should be selected for execution.
it is important that the long-term scheduler make a careful selection. in
general, most processes can be described as either i/o bound or cpu bound.
an i/o-bound process is one that spends more of its time doing i/o than
it spends doing computations. a cpu-bound process, in contrast, generates
i/o requests infrequently, using more of its time doing computations. it is
important that the long-term scheduler select a good process mix of i/o-bound
and cpu-bound processes. if all processes are i/o bound, the ready queue will
almost always be empty, and the short-term scheduler will have little to do.
if all processes are cpu bound, the i/o waiting queue will almost always be
empty, devices will go unused, and again the system will be unbalanced. the
system with the best performance will thus have a combination of cpu-bound
and i/o-bound processes.
on some systems, the long-term scheduler may be absent or minimal.
for example, time-sharing systems such as unix and microsoft windows
systems often have no long-term scheduler but simply put every new process in
memory for the short-term scheduler. the stability of these systems depends
either on a physical limitation (such as the number of available terminals)
or on the self-adjusting nature of human users. if performance declines to
unacceptable levels on a multiuser system, some users will simply quit.
some operating systems, such as time-sharing systems, may introduce an
additional, intermediate level of scheduling. this medium-term scheduler is
diagrammed in figure 3.7. the key idea behind a medium-term scheduler is
that sometimes it can be advantageous to remove a process from memory
(and from active contention for the cpu) and thus reduce the degree of
multiprogramming. later, the process can be reintroduced into memory, and its
execution can be continued where it left off. this scheme is called swapping.
the process is swapped out, and is later swapped in, by the medium-term
scheduler. swapping may be necessary to improve the process mix or because
a change in memory requirements has overcommitted available memory,
requiring memory to be freed up. swapping is discussed in chapter 8.
114
chapter 3
processes
swap in
swap out
end
cpu
i/o
i/o waiting
queues
ready queue
partially executed
swapped-out processes
figure 3.7
addition of medium-term scheduling to the queueing diagram.
3.2.3
context switch
as mentioned in section 1.2.1, interrupts cause the operating system to change
a cpu from its current task and to run a kernel routine. such operations happen
frequently on general-purpose systems. when an interrupt occurs, the system
needs to save the current context of the process running on the cpu so that
it can restore that context when its processing is done, essentially suspending
the process and then resuming it. the context is represented in the pcb of the
process. it includes the value of the cpu registers, the process state (see figure
3.2), and memory-management information. generically, we perform a state
save of the current state of the cpu, be it in kernel or user mode, and then a
state restore to resume operations.
switching the cpu to another process requires performing a state save of
the current process and a state restore of a different process. this task is known
as a context switch. when a context switch occurs, the kernel saves the context
of the old process in its pcb and loads the saved context of the new process
scheduled to run. context-switch time is pure overhead, because the system
does no useful work while switching. switching speed varies from machine to
machine, depending on the memory speed, the number of registers that must
be copied, and the existence of special instructions (such as a single instruction
to load or store all registers). a typical speed is a few milliseconds.
context-switch times are highly dependent on hardware support. for
instance, some processors (such as the sun ultrasparc) provide multiple sets
of registers. a context switch here simply requires changing the pointer to the
current register set. of course, if there are more active processes than there are
register sets, the system resorts to copying register data to and from memory,
as before. also, the more complex the operating system, the greater the amount
of work that must be done during a context switch. as we will see in chapter
8, advanced memory-management techniques may require that extra data be
switched with each context. for instance, the address space of the current
process must be preserved as the space of the next task is prepared for use.
how the address space is preserved, and what amount of work is needed
to preserve it, depend on the memory-management method of the operating
system.
3.3
operations on processes
115
multitasking in mobile systems
because of the constraints imposed on mobile devices, early versions of ios
did not provide user-application multitasking; only one application runs in
the foreground and all other user applications are suspended. operating-
system tasks were multitasked because they were written by apple and well
behaved. however, beginning with ios 4, apple now provides a limited
form of multitasking for user applications, thus allowing a single foreground
application to run concurrently with multiple background applications. (on
a mobile device, the foreground application is the application currently
open and appearing on the display. the background application remains
in memory, but does not occupy the display screen.) the ios 4 programming
api provides support for multitasking, thus allowing a process to run in
the background without being suspended. however, it is limited and only
available for a limited number of application types, including applications
running a single, nite-length task (such as completing a download of
content from a network);
receiving notications of an event occurring (such as a new email
message);
with long-running background tasks (such as an audio player.)
apple probably limits multitasking due to battery life and memory use
concerns. the cpu certainly has the features to support multitasking, but
apple chooses to not take advantage of some of them in order to better
manage resource use.
android does not place such constraints on the types of applications that
can run in the background. if an application requires processing while in
the background, the application must use a service, a separate application
component that runs on behalf of the background process. consider a
streaming audio application: if the application moves to the background, the
service continues to send audio les to the audio device driver on behalf of
the background application. in fact, the service will continue to run even if the
background application is suspended. services do not have a user interface
and have a small memory footprint, thus providing an efcient technique for
multitasking in a mobile environment.
3.3
operations on processes
the processes in most systems can execute concurrently, and they may
be created and deleted dynamically. thus, these systems must provide a
mechanism for process creation and termination. in this section, we explore
the mechanisms involved in creating processes and illustrate process creation
on unix and windows systems.
116
chapter 3
processes
3.3.1
process creation
during the course of execution, a process may create several new processes. as
mentioned earlier, the creating process is called a parent process, and the new
processes are called the children of that process. each of these new processes
may in turn create other processes, forming a tree of processes.
most operating systems (including unix, linux, and windows) identify
processes according to a unique process identier (or pid), which is typically
an integer number. the pid provides a unique value for each process in the
system, and it can be used as an index to access various attributes of a process
within the kernel.
figure 3.8 illustrates a typical process tree for the linux operating system,
showing the name of each process and its pid. (we use the term process rather
loosely, as linux prefers the term task instead.) the init process (which always
has a pid of 1) serves as the root parent process for all user processes. once the
system has booted, the init process can also create various user processes, such
as a web or print server, an ssh server, and the like. in figure 3.8, we see two
children of initkthreadd and sshd. the kthreadd process is responsible
for creating additional processes that perform tasks on behalf of the kernel
(in this situation, khelper and pdflush). the sshd process is responsible for
managing clients that connect to the system by using ssh (which is short for
secure shell). the login process is responsible for managing clients that directly
log onto the system. in this example, a client has logged on and is using the
bash shell, which has been assigned pid 8416. using the bash command-line
interface, this user has created the process ps as well as the emacs editor.
on unix and linux systems, we can obtain a listing of processes by using
the ps command. for example, the command
ps -el
will list complete information for all processes currently active in the system.
it is easy to construct a process tree similar to the one shown in figure 3.8 by
recursively tracing parent processes all the way to the init process.
init
pid = 1
sshd
pid = 3028
login
pid = 8415
kthreadd
pid = 2
sshd
pid = 3610
pdflush
pid = 200
khelper
pid = 6
tcsch
pid = 4005
emacs
pid = 9204
bash
pid = 8416
ps
pid = 9298
figure 3.8
a tree of processes on a typical linux system.
3.3
operations on processes
117
in general, when a process creates a child process, that child process will
need certain resources (cpu time, memory, les, i/o devices) to accomplish
its task. a child process may be able to obtain its resources directly from
the operating system, or it may be constrained to a subset of the resources
of the parent process. the parent may have to partition its resources among
its children, or it may be able to share some resources (such as memory or
les) among several of its children. restricting a child process to a subset of
the parents resources prevents any process from overloading the system by
creating too many child processes.
in addition to supplying various physical and logical resources, the parent
process may pass along initialization data (input) to the child process. for
example, consider a process whose function is to display the contents of a le
say, image.jpgon the screen of a terminal. when the process is created,
it will get, as an input from its parent process, the name of the le image.jpg.
using that le name, it will open the le and write the contents out. it may
also get the name of the output device. alternatively, some operating systems
pass resources to child processes. on such a system, the new process may get
two open les, image.jpg and the terminal device, and may simply transfer
the datum between the two.
when a process creates a new process, two possibilities for execution exist:
1. the parent continues to execute concurrently with its children.
2. the parent waits until some or all of its children have terminated.
there are also two address-space possibilities for the new process:
1. the child process is a duplicate of the parent process (it has the same
program and data as the parent).
2. the child process has a new program loaded into it.
to illustrate these differences, lets rst consider the unix operating system.
in unix, as weve seen, each process is identied by its process identier,
which is a unique integer. a new process is created by the fork() system
call. the new process consists of a copy of the address space of the original
process. this mechanism allows the parent process to communicate easily with
its child process. both processes (the parent and the child) continue execution
at the instruction after the fork(), with one difference: the return code for
the fork() is zero for the new (child) process, whereas the (nonzero) process
identier of the child is returned to the parent.
after a fork() system call, one of the two processes typically uses the
exec() system call to replace the processs memory space with a new program.
the exec() system call loads a binary le into memory (destroying the
memory image of the program containing the exec() system call) and starts
its execution. in this manner, the two processes are able to communicate and
then go their separate ways. the parent can then create more children; or, if it
has nothing else to do while the child runs, it can issue a wait() system call to
move itself off the ready queue until the termination of the child. because the
118
chapter 3
processes
#include <sys/types.h>
#include <stdio.h>
#include <unistd.h>
int main()
{
pid t pid;
/* fork a child process */
pid = fork();
if (pid < 0) { /* error occurred */
fprintf(stderr, "fork failed");
return 1;
}
else if (pid == 0) { /* child process */
execlp("/bin/ls","ls",null);
}
else { /* parent process */
/* parent will wait for the child to complete */
wait(null);
printf("child complete");
}
return 0;
}
figure 3.9
creating a separate process using the unix fork() system call.
call to exec() overlays the processs address space with a new program, the
call to exec() does not return control unless an error occurs.
the c program shown in figure 3.9 illustrates the unix system calls
previously described. we now have two different processes running copies
of the same program. the only difference is that the value of pid (the process
identier) for the child process is zero, while that for the parent is an integer
value greater than zero (in fact, it is the actual pid of the child process). the
child process inherits privileges and scheduling attributes from the parent,
as well certain resources, such as open les. the child process then overlays
its address space with the unix command /bin/ls (used to get a directory
listing) using the execlp() system call (execlp() is a version of the exec()
system call). the parent waits for the child process to complete with the wait()
system call. when the child process completes (by either implicitly or explicitly
invoking exit()), the parent process resumes from the call to wait(), where it
completes using the exit() system call. this is also illustrated in figure 3.10.
of course, there is nothing to prevent the child from not invoking exec()
and instead continuing to execute as a copy of the parent process. in this
scenario, the parent and child are concurrent processes running the same code
3.3
operations on processes
119
pid = fork()
exec()
parent
parent (pid > 0)
child (pid = 0)
wait()
exit()
parent resumes
figure 3.10
process creation using the fork() system call.
instructions. because the child is a copy of the parent, each process has its own
copy of any data.
as an alternative example, we next consider process creation in windows.
processes are created in the windows api using the createprocess() func-
tion, which is similar to fork() in that a parent creates a new child process.
however, whereas fork() has the child process inheriting the address space
of its parent, createprocess() requires loading a specied program into the
address space of the child process at process creation. furthermore, whereas
fork() is passed no parameters, createprocess() expects no fewer than ten
parameters.
the c program shown in figure 3.11 illustrates the createprocess()
function, which creates a child process that loads the application mspaint.exe.
we opt for many of the default values of the ten parameters passed to
createprocess(). readers interested in pursuing the details of process
creation and management in the windows api are encouraged to consult the
bibliographical notes at the end of this chapter.
the two parameters passed to the createprocess() function are instances
of the
startupinfo and process information structures. startupinfo
species many properties of the new process, such as window size and
appearance and handles to standard input and output les. the pro-
cess information structure contains a handle and the identiers to the
newly created process and its thread. we invoke the zeromemory() func-
tion to allocate memory for each of these structures before proceeding with
createprocess().
the rst two parameters passed to createprocess() are the application
name and command-line parameters. if the application name is null (as it is
in this case), the command-line parameter species the application to load. in
this instance, we are loading the microsoft windows mspaint.exe application.
beyond these two initial parameters, we use the default parameters for
inheriting process and thread handles as well as specifying that there will be no
creation ags. we also use the parents existing environment block and starting
directory. last, we provide two pointers to the startupinfo and process -
information structures created at the beginning of the program. in figure
3.9, the parent process waits for the child to complete by invoking the wait()
system call. the equivalent of this in windows is waitforsingleobject(),
which is passed a handle of the child processpi.hprocessand waits for
this process to complete. once the child process exits, control returns from the
waitforsingleobject() function in the parent process.
120
chapter 3
processes
#include <stdio.h>
#include <windows.h>
int main(void)
{
startupinfo si;
process information pi;
/* allocate memory */
zeromemory(&si, sizeof(si));
si.cb = sizeof(si);
zeromemory(&pi, sizeof(pi));
/* create child process */
if (!createprocess(null, /* use command line */
"c:\\windows\\system32\\mspaint.exe", /* command */
null, /* dont inherit process handle */
null, /* dont inherit thread handle */
false, /* disable handle inheritance */
0, /* no creation flags */
null, /* use parents environment block */
null, /* use parents existing directory */
&si,
&pi))
{
fprintf(stderr, "create process failed");
return -1;
}
/* parent will wait for the child to complete */
waitforsingleobject(pi.hprocess, infinite);
printf("child complete");
/* close handles */
closehandle(pi.hprocess);
closehandle(pi.hthread);
}
figure 3.11
creating a separate process using the windows api.
3.3.2
process termination
a process terminates when it nishes executing its nal statement and asks the
operating system to delete it by using the exit() system call. at that point, the
process may return a status value (typically an integer) to its parent process
(via the wait() system call). all the resources of the processincluding
physical and virtual memory, open les, and i/o buffersare deallocated
by the operating system.
termination can occur in other circumstances as well. a process can cause
the termination of another process via an appropriate system call (for example,
terminateprocess() in windows). usually, such a system call can be invoked
3.3
operations on processes
121
only by the parent of the process that is to be terminated. otherwise, users could
arbitrarily kill each others jobs. note that a parent needs to know the identities
of its children if it is to terminate them. thus, when one process creates a new
process, the identity of the newly created process is passed to the parent.
a parent may terminate the execution of one of its children for a variety of
reasons, such as these:
the child has exceeded its usage of some of the resources that it has been
allocated. (to determine whether this has occurred, the parent must have
a mechanism to inspect the state of its children.)
the task assigned to the child is no longer required.
the parent is exiting, and the operating system does not allow a child to
continue if its parent terminates.
some systems do not allow a child to exist if its parent has terminated. in
such systems, if a process terminates (either normally or abnormally), then
all its children must also be terminated. this phenomenon, referred to as
cascading termination, is normally initiated by the operating system.
to illustrate process execution and termination, consider that, in linux
and unix systems, we can terminate a process by using the exit() system
call, providing an exit status as a parameter:
/* exit with status 1 */
exit(1);
in fact, under normal termination, exit() may be called either directly (as
shown above) or indirectly (by a return statement in main()).
a parent process may wait for the termination of a child process by using
the wait() system call. the wait() system call is passed a parameter that
allows the parent to obtain the exit status of the child. this system call also
returns the process identier of the terminated child so that the parent can tell
which of its children has terminated:
pid t pid;
int status;
pid = wait(&status);
when a process terminates, its resources are deallocated by the operating
system. however, its entry in the process table must remain there until the
parent calls wait(), because the process table contains the processs exit status.
a process that has terminated, but whose parent has not yet called wait(), is
known as a zombie process. all processes transition to this state when they
terminate, but generally they exist as zombies only briey. once the parent
calls wait(), the process identier of the zombie process and its entry in the
process table are released.
now consider what would happen if a parent did not invoke wait() and
instead terminated, thereby leaving its child processes as orphans. linux and
unix address this scenario by assigning the init process as the new parent to
122
chapter 3
processes
orphan processes. (recall from figure 3.8 that the init process is the root of the
process hierarchy in unix and linux systems.) the init process periodically
invokes wait(), thereby allowing the exit status of any orphaned process to be
collected and releasing the orphans process identier and process-table entry.
3.4
interprocess communication
processes executing concurrently in the operating system may be either
independent processes or cooperating processes. a process is
independent
if it cannot affect or be affected by the other processes executing in the system.
any process that does not share data with any other process is independent. a
process is cooperating if it can affect or be affected by the other processes
executing in the system. clearly, any process that shares data with other
processes is a cooperating process.
there are several reasons for providing an environment that allows process
cooperation:
information sharing. since several users may be interested in the same
piece of information (for instance, a shared le), we must provide an
environment to allow concurrent access to such information.
computation speedup. if we want a particular task to run faster, we must
break it into subtasks, each of which will be executing in parallel with the
others. notice that such a speedup can be achieved only if the computer
has multiple processing cores.
modularity. we may want to construct the system in a modular fashion,
dividing the system functions into separate processes or threads, as we
discussed in chapter 2.
convenience. even an individual user may work on many tasks at the
same time. for instance, a user may be editing, listening to music, and
compiling in parallel.
cooperating processes require an interprocess communication (ipc) mech-
anism that will allow them to exchange data and information. there are two
fundamental models of interprocess communication: shared memory and mes-
sage passing. in the shared-memory model, a region of memory that is shared
by cooperating processes is established. processes can then exchange informa-
tion by reading and writing data to the shared region. in the message-passing
model, communication takes place by means of messages exchanged between
the cooperating processes. the two communications models are contrasted in
figure 3.12.
both of the models just mentioned are common in operating systems,
and many systems implement both. message passing is useful for exchanging
smaller amounts of data, because no conicts need be avoided. message
passing is also easier to implement in a distributed system than shared memory.
(although there are systems that provide distributed shared memory, we do not
consider them in this text.) shared memory can be faster than message passing,
since message-passing systems are typically implemented using system calls
3.4
interprocess communication
123
multiprocess architecturechrome browser
many websites contain active content such as javascript, flash, and html5 to
provide a rich and dynamic web-browsing experience. unfortunately, these
web applications may also contain software bugs, which can result in sluggish
response times and can even cause the web browser to crash. this isnt a big
problem in a web browser that displays content from only one website. but
most contemporary web browsers provide tabbed browsing, which allows a
single instance of a web browser application to open several websites at the
same time, with each site in a separate tab. to switch between the different
sites , a user need only click on the appropriate tab. this arrangement is
illustrated below:
a problem with this approach is that if a web application in any tab crashes,
the entire processincluding all other tabs displaying additional websites
crashes as well.
googles chrome web browser was designed to address this issue by
using a multiprocess architecture. chrome identies three different types of
processes: browser, renderers, and plug-ins.
the browser process is responsible for managing the user interface as
well as disk and network i/o. a new browser process is created when
chrome is started. only one browser process is created.
renderer processes contain logic for rendering web pages. thus, they
contain the logic for handling html, javascript, images, and so forth. as
a general rule, a new renderer process is created for each website opened
in a new tab, and so several renderer processes may be active at the same
time.
a plug-in process is created for each type of plug-in (such as flash or
quicktime) in use. plug-in processes contain the code for the plug-in as
well as additional code that enables the plug-in to communicate with
associated renderer processes and the browser process.
the advantage of the multiprocess approach is that websites run in
isolation from one another. if one website crashes, only its renderer process
is affected; all other processes remain unharmed. furthermore, renderer
processes run in a sandbox, which means that access to disk and network
i/o is restricted, minimizing the effects of any security exploits.
and thus require the more time-consuming task of kernel intervention. in
shared-memory systems, system calls are required only to establish shared-
124
chapter 3
processes
process a
message queue
kernel
(a)
(b)
process a
shared memory
kernel
process b
m0 m1 m2
...
m3
mn
process b
figure 3.12
communications models. (a) message passing. (b) shared memory.
memory regions. once shared memory is established, all accesses are treated
as routine memory accesses, and no assistance from the kernel is required.
recent research on systems with several processing cores indicates that
message passing provides better performance than shared memory on such
systems. shared memory suffers from cache coherency issues, which arise
because shared data migrate among the several caches. as the number of
processing cores on systems increases, it is possible that we will see message
passing as the preferred mechanism for ipc.
in the remainder of this section, we explore shared-memory and message-
passing systems in more detail.
3.4.1
shared-memory systems
interprocess communication using shared memory requires communicating
processes to establish a region of shared memory. typically, a shared-memory
region resides in the address space of the process creating the shared-memory
segment. other processes that wish to communicate using this shared-memory
segment must attach it to their address space. recall that, normally, the
operating system tries to prevent one process from accessing another processs
memory. shared memory requires that two or more processes agree to remove
this restriction. they can then exchange information by reading and writing
data in the shared areas. the form of the data and the location are determined by
these processes and are not under the operating systems control. the processes
are also responsible for ensuring that they are not writing to the same location
simultaneously.
to illustrate the concept of cooperating processes, lets consider the
producerconsumer problem, which is a common paradigm for cooperating
processes. a producer process produces information that is consumed by a
consumer process. for example, a compiler may produce assembly code that
is consumed by an assembler. the assembler, in turn, may produce object
modules that are consumed by the loader. the producerconsumer problem
3.4
interprocess communication
125
item next produced;
while (true) {
/* produce an item in next produced */
while (((in + 1) % buffer size) == out)
; /* do nothing */
buffer[in] = next produced;
in = (in + 1) % buffer size;
}
figure 3.13
the producer process using shared memory.
also provides a useful metaphor for the clientserver paradigm. we generally
think of a server as a producer and a client as a consumer. for example, a web
server produces (that is, provides) html les and images, which are consumed
(that is, read) by the client web browser requesting the resource.
one solution to the producerconsumer problem uses shared memory. to
allow producer and consumer processes to run concurrently, we must have
available a buffer of items that can be lled by the producer and emptied by
the consumer. this buffer will reside in a region of memory that is shared by
the producer and consumer processes. a producer can produce one item while
the consumer is consuming another item. the producer and consumer must
be synchronized, so that the consumer does not try to consume an item that
has not yet been produced.
two types of buffers can be used. the unbounded buffer places no practical
limit on the size of the buffer. the consumer may have to wait for new items,
but the producer can always produce new items. the bounded buffer assumes
a xed buffer size. in this case, the consumer must wait if the buffer is empty,
and the producer must wait if the buffer is full.
lets look more closely at how the bounded buffer illustrates interprocess
communication using shared memory. the following variables reside in a
region of memory shared by the producer and consumer processes:
#define buffer size 10
typedef struct {
. . .
}item;
item buffer[buffer size];
int in = 0;
int out = 0;
the shared buffer is implemented as a circular array with two logical pointers:
in and out. the variable in points to the next free position in the buffer; out
points to the rst full position in the buffer. the buffer is empty when in ==
out; the buffer is full when ((in + 1) % buffer size) == out.
the code for the producer process is shown in figure 3.13, and the code
for the consumer process is shown in figure 3.14. the producer process has a
126
chapter 3
processes
item next consumed;
while (true) {
while (in == out)
; /* do nothing */
next consumed = buffer[out];
out = (out + 1) % buffer size;
/* consume the item in next consumed */
}
figure 3.14
the consumer process using shared memory.
local variable next produced in which the new item to be produced is stored.
the consumer process has a local variable next consumed in which the item
to be consumed is stored.
this scheme allows at most buffer size 1 items in the buffer at the
same time. we leave it as an exercise for you to provide a solution in which
buffer size items can be in the buffer at the same time. in section 3.5.1, we
illustrate the posix api for shared memory.
one issue this illustration does not address concerns the situation in which
both the producer process and the consumer process attempt to access the
shared buffer concurrently. in chapter 5, we discuss how synchronization
among cooperating processes can be implemented effectively in a shared-
memory environment.
3.4.2
message-passing systems
in section 3.4.1, we showed how cooperating processes can communicate in a
shared-memory environment. the scheme requires that these processes share a
region of memory and that the code for accessing and manipulating the shared
memory be written explicitly by the application programmer. another way to
achieve the same effect is for the operating system to provide the means for
cooperating processes to communicate with each other via a message-passing
facility.
message passing provides a mechanism to allow processes to communicate
and to synchronize their actions without sharing the same address space. it is
particularly useful in a distributed environment, where the communicating
processes may reside on different computers connected by a network. for
example, an internet chat program could be designed so that chat participants
communicate with one another by exchanging messages.
a message-passing facility provides at least two operations:
send(message)
receive(message)
messages sent by a process can be either xed or variable in size. if only
xed-sized messages can be sent, the system-level implementation is straight-
forward. this restriction, however, makes the task of programming more
difcult. conversely, variable-sized messages require a more complex system-
3.4
interprocess communication
127
level implementation, but the programming task becomes simpler. this is a
common kind of tradeoff seen throughout operating-system design.
if processes p and q want to communicate, they must send messages to and
receive messages from each other: a communication link must exist between
them. this link can be implemented in a variety of ways. we are concerned here
not with the links physical implementation (such as shared memory, hardware
bus, or network, which are covered in chapter 17) but rather with its logical
implementation. here are several methods for logically implementing a link
and the send()/receive() operations:
direct or indirect communication
synchronous or asynchronous communication
automatic or explicit buffering
we look at issues related to each of these features next.
3.4.2.1
naming
processes that want to communicate must have a way to refer to each other.
they can use either direct or indirect communication.
under direct communication, each process that wants to communicate
must explicitly name the recipient or sender of the communication. in this
scheme, the send() and receive() primitives are dened as:
send(p, message)send a message to process p.
receive(q, message)receive a message from process q.
a communication link in this scheme has the following properties:
a link is established automatically between every pair of processes that
want to communicate. the processes need to know only each others
identity to communicate.
a link is associated with exactly two processes.
between each pair of processes, there exists exactly one link.
this scheme exhibits symmetry in addressing; that is, both the sender
process and the receiver process must name the other to communicate. a
variant of this scheme employs asymmetry in addressing. here, only the sender
names the recipient; the recipient is not required to name the sender. in this
scheme, the send() and receive() primitives are dened as follows:
send(p, message)send a message to process p.
receive(id, message)receive a message from any process. the
variable id is set to the name of the process with which communication
has taken place.
128
chapter 3
processes
the disadvantage in both of these schemes (symmetric and asymmetric)
is the limited modularity of the resulting process denitions. changing the
identier of a process may necessitate examining all other process denitions.
all references to the old identier must be found, so that they can be modied
to the new identier. in general, any such hard-coding techniques, where
identiersmustbe explicitlystated, are lessdesirable thantechniquesinvolving
indirection, as described next.
with indirect communication, the messages are sent to and received from
mailboxes, or ports. a mailbox can be viewed abstractly as an object into which
messages can be placed by processes and from which messages can be removed.
each mailbox has a unique identication. for example, posix message queues
use an integer value to identify a mailbox. a process can communicate with
another process via a number of different mailboxes, but two processes can
communicate only if they have a shared mailbox. the send() and receive()
primitives are dened as follows:
send(a, message)send a message to mailbox a.
receive(a, message)receive a message from mailbox a.
in this scheme, a communication link has the following properties:
a link is established between a pair of processes only if both members of
the pair have a shared mailbox.
a link may be associated with more than two processes.
between each pair of communicating processes, a number of different links
may exist, with each link corresponding to one mailbox.
now suppose that processes p1, p2, and p3 all share mailbox a. process
p1 sends a message to a, while both p2 and p3 execute a receive() from a.
which process will receive the message sent by p1? the answer depends on
which of the following methods we choose:
allow a link to be associated with two processes at most.
allow at most one process at a time to execute a receive() operation.
allow the system to select arbitrarily which process will receive the
message (that is, either p2 or p3, but not both, will receive the message). the
system may dene an algorithm for selecting which process will receive the
message (for example, round robin, where processes take turns receiving
messages). the system may identify the receiver to the sender.
a mailbox may be owned either by a process or by the operating system.
if the mailbox is owned by a process (that is, the mailbox is part of the address
space of the process), then we distinguish between the owner (which can
only receive messages through this mailbox) and the user (which can only
send messages to the mailbox). since each mailbox has a unique owner, there
can be no confusion about which process should receive a message sent to
this mailbox. when a process that owns a mailbox terminates, the mailbox
3.4
interprocess communication
129
disappears. any process that subsequently sends a message to this mailbox
must be notied that the mailbox no longer exists.
in contrast, a mailbox that is owned by the operating system has an
existence of its own. it is independent and is not attached to any particular
process. the operating system then must provide a mechanism that allows a
process to do the following:
create a new mailbox.
send and receive messages through the mailbox.
delete a mailbox.
the process that creates a new mailbox is that mailboxs owner by default.
initially, the owner is the only process that can receive messages through this
mailbox. however, the ownership and receiving privilege may be passed to
other processes through appropriate system calls. of course, this provision
could result in multiple receivers for each mailbox.
3.4.2.2
synchronization
communication between processes takes place through calls to send() and
receive() primitives. there are different design options for implementing
each primitive. message passing may be either blocking or nonblocking
also known as synchronous and asynchronous. (throughout this text, you
will encounter the concepts of synchronous and asynchronous behavior in
relation to various operating-system algorithms.)
blocking send. the sending process is blocked until the message is
received by the receiving process or by the mailbox.
nonblocking send. the sending process sends the message and resumes
operation.
blocking receive. the receiver blocks until a message is available.
nonblocking receive. the receiver retrieves either a valid message or a
null.
different combinations of send() and receive() are possible. when both
send() and receive() are blocking, we have a rendezvous between the
sender and the receiver. the solution to the producerconsumer problem
becomes trivial when we use blocking send() and receive() statements.
the producer merely invokes the blocking send() call and waits until the
message is delivered to either the receiver or the mailbox. likewise, when the
consumer invokes receive(), it blocks until a message is available. this is
illustrated in figures 3.15 and 3.16.
3.4.2.3
buffering
whether communication is direct or indirect, messages exchanged by commu-
nicating processes reside in a temporary queue. basically, such queues can be
implemented in three ways:
130
chapter 3
processes
message next produced;
while (true) {
/* produce an item in next produced */
send(next produced);
}
figure 3.15
the producer process using message passing.
zero capacity. the queue has a maximum length of zero; thus, the link
cannot have any messages waiting in it. in this case, the sender must block
until the recipient receives the message.
bounded capacity. the queue has nite length n; thus, at most n messages
can reside in it. if the queue is not full when a new message is sent, the
message is placed in the queue (either the message is copied or a pointer
to the message is kept), and the sender can continue execution without
waiting. the links capacity is nite, however. if the link is full, the sender
must block until space is available in the queue.
unbounded capacity. the queues length is potentially innite; thus, any
number of messages can wait in it. the sender never blocks.
the zero-capacity case is sometimes referred to as a message system with no
buffering. the other cases are referred to as systems with automatic buffering.
3.5
examples of ipc systems
in this section, we explore three different ipc systems. we rst cover the posix
api forshared memoryand thendiscussmessage passinginthe machoperating
system. we conclude with windows, which interestingly uses shared memory
as a mechanism for providing certain types of message passing.
3.5.1
an example: posix shared memory
several ipc mechanisms are available for posix systems, including shared
memory and message passing. here, we explore the posix api for shared
memory.
posix shared memory is organized using memory-mapped les, which
associate the region of shared memory with a le. a process must rst create
message next consumed;
while (true) {
receive(next consumed);
/* consume the item in next consumed */
}
figure 3.16
the consumer process using message passing.
3.5
examples of ipc systems
131
a shared-memory object using the shm open() system call, as follows:
shm fd = shm open(name, o creat | o rdrw, 0666);
the rst parameter species the name of the shared-memory object. processes
that wish to access this shared memory must refer to the object by this name.
the subsequent parameters specify that the shared-memory object is to be
created if it does not yet exist (o creat) and that the object is open for reading
and writing (o rdrw). the last parameter establishes the directory permissions
of the shared-memory object. a successful call to shm open() returns an integer
le descriptor for the shared-memory object.
once the object is established, the ftruncate() function is used to
congure the size of the object in bytes. the call
ftruncate(shm fd, 4096);
sets the size of the object to 4,096 bytes.
finally, the mmap() function establishes a memory-mapped le containing
the shared-memory object. it also returns a pointer to the memory-mapped le
that is used for accessing the shared-memory object.
the programs shown in figure 3.17 and 3.18 use the producerconsumer
model in implementing shared memory. the producer establishes a shared-
memory object and writes to shared memory, and the consumer reads from
shared memory.
the producer, shown in figure 3.17, creates a shared-memory object named
os and writes the infamous string "hello world!" to shared memory. the
program memory-maps a shared-memory object of the specied size and
allows writing to the object. (obviously, only writing is necessary for the
producer.) the ag map shared species that changes to the shared-memory
object will be visible to all processes sharing the object. notice that we write to
the shared-memory object by calling the sprintf() function and writing the
formatted string to the pointer ptr. after each write, we must increment the
pointer by the number of bytes written.
the consumer process, shown in figure 3.18, reads and outputs the contents
of the shared memory. the consumer also invokes the shm unlink() function,
which removes the shared-memory segment after the consumer has accessed
it. we provide further exercises using the posix shared-memory api in the
programming exercises at the end of this chapter. additionally, we provide
more detailed coverage of memory mapping in section 9.7.
3.5.2
an example: mach
as an example of message passing, we next consider the mach operating
system. you may recall that we introduced mach in chapter 2 as part of the mac
os x operating system. the mach kernel supports the creation and destruction
of multiple tasks, which are similar to processes but have multiple threads
of control and fewer associated resources. most communication in mach
including all intertask informationis carried out by messages. messages are
sent to and received from mailboxes, called ports in mach.
132
chapter 3
processes
#include <stdio.h>
#include <stlib.h>
#include <string.h>
#include <fcntl.h>
#include <sys/shm.h>
#include <sys/stat.h>
int main()
{
/* the size (in bytes) of shared memory object */
const int size 4096;
/* name of the shared memory object */
const char *name = "os";
/* strings written to shared memory */
const char *message 0 = "hello";
const char *message 1 = "world!";
/* shared memory file descriptor */
int shm fd;
/* pointer to shared memory obect */
void *ptr;
/* create the shared memory object */
shm fd = shm open(name, o creat | o rdrw, 0666);
/* configure the size of the shared memory object */
ftruncate(shm fd, size);
/* memory map the shared memory object */
ptr = mmap(0, size, prot write, map shared, shm fd, 0);
/* write to the shared memory object */
sprintf(ptr,"%s",message 0);
ptr += strlen(message 0);
sprintf(ptr,"%s",message 1);
ptr += strlen(message 1);
return 0;
}
figure 3.17
producer process illustrating posix shared-memory api.
even system calls are made by messages. when a task is created, two
special mailboxesthe kernel mailbox and the notify mailboxare also
created. the kernel uses the kernel mailbox to communicate with the task and
sends notication of event occurrences to the notify port. only three system
calls are needed for message transfer. the msg send() call sends a message
to a mailbox. a message is received via msg receive(). remote procedure
calls (rpcs) are executed via msg rpc(), which sends a message and waits for
exactly one return message from the sender. in this way, the rpc models a
3.5
examples of ipc systems
133
#include <stdio.h>
#include <stlib.h>
#include <fcntl.h>
#include <sys/shm.h>
#include <sys/stat.h>
int main()
{
/* the size (in bytes) of shared memory object */
const int size 4096;
/* name of the shared memory object */
const char *name = "os";
/* shared memory file descriptor */
int shm fd;
/* pointer to shared memory obect */
void *ptr;
/* open the shared memory object */
shm fd = shm open(name, o rdonly, 0666);
/* memory map the shared memory object */
ptr = mmap(0, size, prot read, map shared, shm fd, 0);
/* read from the shared memory object */
printf("%s",(char *)ptr);
/* remove the shared memory object */
shm unlink(name);
return 0;
}
figure 3.18
consumer process illustrating posix shared-memory api.
typical subroutine procedure call but can work between systemshence the
term remote. remote procedure calls are covered in detail in section 3.6.2.
the port allocate() system call creates a new mailbox and allocates
space for its queue of messages. the maximum size of the message queue
defaults to eight messages. the task that creates the mailbox is that mailboxs
owner. the owner is also allowed to receive from the mailbox. only one task
at a time can either own or receive from a mailbox, but these rights can be sent
to other tasks.
the mailboxs message queue is initially empty. as messages are sent to
the mailbox, the messages are copied into the mailbox. all messages have the
same priority. mach guarantees that multiple messages from the same sender
are queued in rst-in, rst-out (fifo) order but does not guarantee an absolute
ordering. for instance, messages from two senders may be queued in any order.
the messages themselves consist of a xed-length header followed by a
variable-length data portion. the header indicates the length of the message
and includes two mailbox names. one mailbox name species the mailbox
134
chapter 3
processes
to which the message is being sent. commonly, the sending thread expects a
reply, so the mailbox name of the sender is passed on to the receiving task,
which can use it as a return address.
the variable part of a message is a list of typed data items. each entry
in the list has a type, size, and value. the type of the objects specied in the
message is important, since objects dened by the operating systemsuch as
ownership or receive access rights, task states, and memory segmentsmay
be sent in messages.
the send and receive operations themselves are exible. for instance, when
a message is sent to a mailbox, the mailbox may be full. if the mailbox is not
full, the message is copied to the mailbox, and the sending thread continues. if
the mailbox is full, the sending thread has four options:
1. wait indenitely until there is room in the mailbox.
2. wait at most n milliseconds.
3. do not wait at all but rather return immediately.
4. temporarily cache a message. here, a message is given to the operating
system to keep, even though the mailbox to which that message is being
sent is full. when the message can be put in the mailbox, a message is sent
back to the sender. only one message to a full mailbox can be pending at
any time for a given sending thread.
the nal option is meant for server tasks, such as a line-printer driver. after
nishing a request, such tasks may need to send a one-time reply to the task
that requested service, but they must also continue with other service requests,
even if the reply mailbox for a client is full.
the receive operation must specify the mailbox or mailbox set from which a
message is to be received. a mailbox set is a collection of mailboxes, as declared
by the task, which can be grouped together and treated as one mailbox for the
purposes of the task. threads in a task can receive only from a mailbox or
mailbox set for which the task has receive access. a port status() system
call returns the number of messages in a given mailbox. the receive operation
attempts to receive from (1) any mailbox in a mailbox set or (2) a specic
(named) mailbox. if no message is waiting to be received, the receiving thread
can either wait at most n milliseconds or not wait at all.
the mach system was especially designed for distributed systems, which
we discuss in chapter 17, but mach was shown to be suitable for systems
with fewer processing cores, as evidenced by its inclusion in the mac os x
system. the major problem with message systems has generally been poor
performance caused by double copying of messages: the message is copied
rst from the sender to the mailbox and then from the mailbox to the receiver.
the mach message system attempts to avoid double-copy operations by using
virtual-memory-management techniques (chapter 9). essentially, mach maps
the address space containing the senders message into the receivers address
space. the message itself is never actually copied. this message-management
technique provides a large performance boost but works for only intrasystem
messages. the mach operating system is discussed in more detail in the online
appendix b.
3.5
examples of ipc systems
135
3.5.3
an example: windows
the windows operating system is an example of modern design that employs
modularity to increase functionality and decrease the time needed to imple-
ment new features. windows provides support for multiple operating envi-
ronments, or subsystems. application programs communicate with these
subsystems via a message-passing mechanism. thus, application programs
can be considered clients of a subsystem server.
the message-passing facility in windows is called the advanced local
procedure call (alpc) facility. it is used for communication between two
processes on the same machine. it is similar to the standard remote procedure
call (rpc) mechanism that is widely used, but it is optimized for and specic
to windows. (remote procedure calls are covered in detail in section 3.6.2.)
like mach, windows uses a port object to establish and maintain a connection
between two processes. windows uses two types of ports: connection ports
and communication ports.
server processes publish connection-port objects that are visible to all
processes. when a client wants services from a subsystem, it opens a handle to
the servers connection-port object and sends a connection request to that port.
the server then creates a channel and returns a handle to the client. the channel
consists of a pair of private communication ports: one for clientserver
messages, the other for serverclient messages. additionally, communication
channels support a callback mechanism that allows the client and server to
accept requests when they would normally be expecting a reply.
when an alpc channel is created, one of three message-passing techniques
is chosen:
1. for small messages (up to 256 bytes), the ports message queue is used
as intermediate storage, and the messages are copied from one process to
the other.
2. larger messages must be passed through a section object, which is a
region of shared memory associated with the channel.
3. when the amount of data is too large to t into a section object, an api is
available that allows server processes to read and write directly into the
address space of a client.
the client has to decide when it sets up the channel whether it will need
to send a large message. if the client determines that it does want to send
large messages, it asks for a section object to be created. similarly, if the server
decides that replies will be large, it creates a section object. so that the section
object can be used, a small message is sent that contains a pointer and size
information about the section object. this method is more complicated than
the rst method listed above, but it avoids data copying. the structure of
advanced local procedure calls in windows is shown in figure 3.19.
it is important to note that the alpc facility in windows is not part of the
windows api and hence is not visible to the application programmer. rather,
applications using the windows api invoke standard remote procedure calls.
when the rpc is being invoked on a process on the same system, the rpc is
handled indirectly through an alpc. procedure call. additionally, many kernel
services use alpc to communicate with client processes.
136
chapter 3
processes
connection
port
connection
request
handle
handle
handle
client
communication port
server
communication port
shared
section object
(> 256 bytes)
server
client
figure 3.19
advanced local procedure calls in windows.
3.6
communication in clientserver systems
in section 3.4, we described how processes can communicate using shared
memory and message passing. these techniques can be used for communica-
tion in clientserver systems (section 1.11.4) as well. in this section, we explore
three other strategies for communication in clientserver systems: sockets,
remote procedure calls (rpcs), and pipes.
3.6.1
sockets
a socket is dened as an endpoint for communication. a pair of processes
communicating over a network employs a pair of socketsone for each
process. a socket is identied by an ip address concatenated with a port
number. in general, sockets use a clientserver architecture. the server waits
for incoming client requests by listening to a specied port. once a request
is received, the server accepts a connection from the client socket to complete
the connection. servers implementing specic services (such as telnet, ftp, and
http) listen to well-known ports (a telnet server listens to port 23; an ftp
server listens to port 21; and a web, or http, server listens to port 80). all
ports below 1024 are considered well known; we can use them to implement
standard services.
when a client process initiates a request for a connection, it is assigned a
port by its host computer. this port has some arbitrary number greater than
1024. for example, if a client on host x with ip address 146.86.5.20 wishes to
establish a connection with a web server (which is listening on port 80) at
address 161.25.19.8, host x may be assigned port 1625. the connection will
consist of a pair of sockets: (146.86.5.20:1625) on host x and (161.25.19.8:80)
on the web server. this situation is illustrated in figure 3.20. the packets
traveling between the hosts are delivered to the appropriate process based on
the destination port number.
all connections must be unique. therefore, if another process also on host
x wished to establish another connection with the same web server, it would be
assigned a port number greater than 1024 and not equal to 1625. this ensures
that all connections consist of a unique pair of sockets.
3.6
communication in clientserver systems
137
socket
(146.86.5.20:1625)
host x
(146.86.5.20)
socket
(161.25.19.8:80)
web server
(161.25.19.8)
figure 3.20
communication using sockets.
although most program examples in this text use c, we will illustrate
sockets using java, as it provides a much easier interface to sockets and has a
rich library for networking utilities. those interested in socket programming
in c or c++ should consult the bibliographical notes at the end of the chapter.
java provides three different types of sockets. connection-oriented (tcp)
sockets are implemented with the socket class. connectionless (udp) sockets
use the datagramsocket class. finally, the multicastsocket class is a subclass
of the datagramsocket class. a multicast socket allows data to be sent to
multiple recipients.
our example describes a date server that uses connection-oriented tcp
sockets. the operation allows clients to request the current date and time from
the server. the server listens to port 6013, although the port could have any
arbitrary number greater than 1024. when a connection is received, the server
returns the date and time to the client.
the date server is shown in figure 3.21. the server creates a serversocket
that species that it will listen to port 6013. the server then begins listening
to the port with the accept() method. the server blocks on the accept()
method waiting for a client to request a connection. when a connection request
is received, accept() returns a socket that the server can use to communicate
with the client.
the details of how the server communicates with the socket are as follows.
the server rst establishesaprintwriterobjectthatitwill use tocommunicate
with the client. a printwriter object allows the server to write to the socket
using the routine print() and println() methods for output. the server
process sends the date to the client, calling the method println(). once it
has written the date to the socket, the server closes the socket to the client and
resumes listening for more requests.
a client communicates with the server by creating a socket and connecting
to the port on which the server is listening. we implement such a client in the
java program shown in figure 3.22. the client creates a socket and requests
a connection with the server at ip address 127.0.0.1 on port 6013. once the
connection is made, the client can read from the socket using normal stream
i/o statements. after it has received the date from the server, the client closes
138
chapter 3
processes
import java.net.*;
import java.io.*;
public class dateserver
{
public static void main(string[] args) {
try {
serversocket sock = new serversocket(6013);
/* now listen for connections */
while (true) {
socket client = sock.accept();
printwriter pout = new
printwriter(client.getoutputstream(), true);
/* write the date to the socket */
pout.println(new java.util.date().tostring());
/* close the socket and resume */
/* listening for connections */
client.close();
}
}
catch (ioexception ioe) {
system.err.println(ioe);
}
}
}
figure 3.21
date server.
the socket and exits. the ip address 127.0.0.1 is a special ip address known as the
loopback. when a computer refers to ip address 127.0.0.1, it is referring to itself.
this mechanism allows a client and server on the same host to communicate
using the tcp/ip protocol. the ip address 127.0.0.1 could be replaced with the
ip address of another host running the date server. in addition to an ip address,
an actual host name, such as www.westminstercollege.edu, can be used as
well.
communication using socketsalthough common and efcientis con-
sidered a low-level form of communication between distributed processes.
one reason is that sockets allow only an unstructured stream of bytes to be
exchanged between the communicating threads. it is the responsibility of the
client or server application to impose a structure on the data. in the next two
subsections, we look at two higher-level methods of communication: remote
procedure calls (rpcs) and pipes.
3.6.2
remote procedure calls
one of the most common forms of remote service is the rpc paradigm, which
we discussed briey in section 3.5.2. the rpc was designed as a way to
3.6
communication in clientserver systems
139
import java.net.*;
import java.io.*;
public class dateclient
{
public static void main(string[] args) {
try {
/* make connection to server socket */
socket sock = new socket("127.0.0.1",6013);
inputstream in = sock.getinputstream();
bufferedreader bin = new
bufferedreader(new inputstreamreader(in));
/* read the date from the socket */
string line;
while ( (line = bin.readline()) != null)
system.out.println(line);
/* close the socket connection*/
sock.close();
}
catch (ioexception ioe) {
system.err.println(ioe);
}
}
}
figure 3.22
date client.
abstract the procedure-call mechanism for use between systems with network
connections. it is similar in many respects to the ipc mechanism described in
section 3.4, and it is usually built on top of such a system. here, however,
because we are dealing with an environment in which the processes are
executing on separate systems, we must use a message-based communication
scheme to provide remote service.
in contrast to ipc messages, the messages exchanged in rpccommunication
are well structured and are thus no longer just packets of data. each message is
addressed to an rpc daemon listening to a port on the remote system, and each
contains an identier specifying the function to execute and the parameters
to pass to that function. the function is then executed as requested, and any
output is sent back to the requester in a separate message.
a port is simply a number included at the start of a message packet.
whereas a system normally has one network address, it can have many ports
within that address to differentiate the many network services it supports. if a
remote process needs a service, it addresses a message to the proper port. for
instance, if a system wished to allow other systems to be able to list its current
users, it would have a daemon supporting such an rpc attached to a port
say, port 3027. any remote system could obtain the needed information (that
140
chapter 3
processes
is, the list of current users) by sending an rpc message to port 3027 on the
server. the data would be received in a reply message.
the semantics of rpcs allows a client to invoke a procedure on a remote
host as it would invoke a procedure locally. the rpc system hides the details
that allow communication to take place by providing a stub on the client side.
typically, a separate stub exists for each separate remote procedure. when the
client invokes a remote procedure, the rpc system calls the appropriate stub,
passing it the parameters provided to the remote procedure. this stub locates
the port on the server and marshals the parameters. parameter marshalling
involves packaging the parameters into a form that can be transmitted over
a network. the stub then transmits a message to the server using message
passing. a similar stub on the server side receives this message and invokes
the procedure on the server. if necessary, return values are passed back to the
client using the same technique. on windows systems, stub code is compiled
from a specication written in the microsoft interface denition language
(midl), which is used for dening the interfaces between client and server
programs.
one issue that must be dealt with concerns differences in data representa-
tion on the client and server machines. consider the representation of 32-bit
integers. some systems (known as big-endian) store the most signicant byte
rst, while other systems (known as little-endian) store the least signicant
byte rst. neither order is better per se; rather, the choice is arbitrary within
a computer architecture. to resolve differences like this, many rpc systems
dene a machine-independent representation of data. one such representation
is known as external data representation (xdr). on the client side, parameter
marshalling involves converting the machine-dependent data into xdr before
they are sent to the server. on the server side, the xdr data are unmarshalled
and converted to the machine-dependent representation for the server.
another important issue involves the semantics of a call. whereas local
procedure calls fail only under extreme circumstances, rpcs can fail, or be
duplicated and executed more than once, as a result of common network
errors. one way to address this problem is for the operating system to ensure
that messages are acted on exactly once, rather than at most once. most local
procedure calls have the exactly once functionality, but it is more difcult to
implement.
first, consider at most once. this semantic can be implemented by
attaching a timestamp to each message. the server must keep a history of
all the timestamps of messages it has already processed or a history large
enough to ensure that repeated messages are detected. incoming messages
that have a timestamp already in the history are ignored. the client can then
send a message one or more times and be assured that it only executes once.
for exactly once, we need to remove the risk that the server will never
receive the request. to accomplish this, the server must implement the at
most once protocol described above but must also acknowledge to the client
that the rpc call was received and executed. these ack messages are common
throughout networking. the client must resend each rpc call periodically until
it receives the ack for that call.
yet another important issue concerns the communication between a server
and a client. with standard procedure calls, some form of binding takes place
during link, load, or execution time (chapter 8) so that a procedure calls name
3.6
communication in clientserver systems
141
client
user calls kernel
to send rpc
message to
procedure x
matchmaker
receives
message, looks
up answer
matchmaker
replies to client
with port p
daemon
listening to
port p receives
message
daemon
processes
request and
processes send
output
kernel sends
message to
matchmaker to
find port number
from: client
to: server
port: matchmaker
re: address
for rpc x
from: client
to: server
port: port p
<contents>
from: rpc
port: p
to: client
port: kernel
<output>
from: server
to: client
port: kernel
re: rpc x
port: p
kernel places
port p in user
rpc message
kernel sends
rpc
kernel receives
reply, passes
it to user
messages
server
figure 3.23
execution of a remote procedure call (rpc).
is replaced by the memory address of the procedure call. the rpc scheme
requires a similar binding of the client and the server port, but how does a client
know the port numbers on the server? neither system has full information
about the other, because they do not share memory.
two approaches are common. first, the binding information may be
predetermined, in the form of xed port addresses. at compile time, an rpc
call has a xed port number associated with it. once a program is compiled,
the server cannot change the port number of the requested service. second,
binding can be done dynamically by a rendezvous mechanism. typically, an
operating system provides a rendezvous (also called a matchmaker) daemon
on a xed rpc port. a client then sends a message containing the name of
the rpc to the rendezvous daemon requesting the port address of the rpc it
needs to execute. the port number is returned, and the rpc calls can be sent
to that port until the process terminates (or the server crashes). this method
requires the extra overhead of the initial request but is more exible than the
rst approach. figure 3.23 shows a sample interaction.
the rpc scheme is useful in implementing a distributed le system
(chapter 17). such a system can be implemented as a set of rpc daemons
142
chapter 3
processes
and clients. the messages are addressed to the distributed le system port on a
server on which a le operation is to take place. the message contains the disk
operation to be performed. the disk operation might be read, write, rename,
delete, or status, corresponding to the usual le-related system calls. the
return message contains any data resulting from that call, which is executed by
the dfs daemon on behalf of the client. for instance, a message might contain
a request to transfer a whole le to a client or be limited to a simple block
request. in the latter case, several requests may be needed if a whole le is to
be transferred.
3.6.3
pipes
a pipe acts as a conduit allowing two processes to communicate. pipes were
one of the rst ipc mechanisms in early unix systems. they typically provide
one of the simpler ways for processes to communicate with one another,
although they also have some limitations. in implementing a pipe, four issues
must be considered:
1. does the pipe allow bidirectional communication, or is communication
unidirectional?
2. if two-way communication is allowed, is it half duplex (data can travel
only one way at a time) or full duplex (data can travel in both directions
at the same time)?
3. must a relationship (such as parentchild) exist between the communi-
cating processes?
4. can the pipes communicate over a network, or must the communicating
processes reside on the same machine?
in the following sections, we explore two common types of pipes used on both
unix and windows systems: ordinary pipes and named pipes.
3.6.3.1
ordinary pipes
ordinary pipes allow two processes to communicate in standard producer
consumer fashion: the producer writes to one end of the pipe (the write-end)
and the consumer reads from the other end (the read-end). as a result, ordinary
pipes are unidirectional, allowing only one-way communication. if two-way
communication is required, two pipes must be used, with each pipe sending
data in a different direction. we next illustrate constructing ordinary pipes
on both unix and windows systems. in both program examples, one process
writes the message greetings to the pipe, while the other process reads this
message from the pipe.
on unix systems, ordinary pipes are constructed using the function
pipe(int fd[])
this function creates a pipe that is accessed through the int fd[] le
descriptors: fd[0] is the read-end of the pipe, and fd[1] is the write-end.
3.6
communication in clientserver systems
143
parent
fd(0)
fd(1)
child
fd(0)
fd(1)
pipe
figure 3.24
file descriptors for an ordinary pipe.
unix treats a pipe as a special type of le. thus, pipes can be accessed using
ordinary read() and write() system calls.
an ordinary pipe cannot be accessed from outside the process that created
it. typically, a parent process creates a pipe and uses it to communicate with
a child process that it creates via fork(). recall from section 3.3.1 that a child
process inherits open les from its parent. since a pipe is a special type of le,
the child inherits the pipe from its parent process. figure 3.24 illustrates the
relationship of the le descriptor fd to the parent and child processes.
in the unix program shown in figure 3.25, the parent process creates a
pipe and then sends a fork() call creating the child process. what occurs after
the fork() call depends on how the data are to ow through the pipe. in
this instance, the parent writes to the pipe, and the child reads from it. it is
important to notice that both the parent process and the child process initially
close their unused ends of the pipe. although the program shown in figure
3.25 does not require this action, it is an important step to ensure that a process
reading from the pipe can detect end-of-le (read() returns 0) when the writer
has closed its end of the pipe.
ordinary pipes on windows systems are termed anonymous pipes, and
they behave similarly to their unix counterparts: they are unidirectional and
#include <sys/types.h>
#include <stdio.h>
#include <string.h>
#include <unistd.h>
#define buffer size 25
#define read end 0
#define write end 1
int main(void)
{
char write msg[buffer size] = "greetings";
char read msg[buffer size];
int fd[2];
pid t pid;
/* program continues in figure 3.26 */
figure 3.25
ordinary pipe in unix.
144
chapter 3
processes
/* create the pipe */
if (pipe(fd) == -1) {
fprintf(stderr,"pipe failed");
return 1;
}
/* fork a child process */
pid = fork();
if (pid < 0) { /* error occurred */
fprintf(stderr, "fork failed");
return 1;
}
if (pid > 0) { /* parent process */
/* close the unused end of the pipe */
close(fd[read end]);
/* write to the pipe */
write(fd[write end], write msg, strlen(write msg)+1);
/* close the write end of the pipe */
close(fd[write end]);
}
else { /* child process */
/* close the unused end of the pipe */
close(fd[write end]);
/* read from the pipe */
read(fd[read end], read msg, buffer size);
printf("read %s",read msg);
/* close the write end of the pipe */
close(fd[read end]);
}
return 0;
}
figure 3.26
figure 3.25, continued.
employ parentchild relationships between the communicating processes.
in addition, reading and writing to the pipe can be accomplished with the
ordinary readfile() and writefile() functions. the windows api for
creating pipes is the createpipe() function, which is passed four parameters.
the parameters provide separate handles for (1) reading and (2) writing to the
pipe, as well as (3) an instance of the startupinfo structure, which is used to
specify that the child process is to inherit the handles of the pipe. furthermore,
(4) the size of the pipe (in bytes) may be specied.
figure 3.27 illustrates a parent process creating an anonymous pipe for
communicating with its child. unlike unix systems, in which a child process
3.6
communication in clientserver systems
145
#include <stdio.h>
#include <stdlib.h>
#include <windows.h>
#define buffer size 25
int main(void)
{
handle readhandle, writehandle;
startupinfo si;
process information pi;
char message[buffer size] = "greetings";
dword written;
/* program continues in figure 3.28 */
figure 3.27
windows anonymous pipeparent process.
automatically inherits a pipe created by its parent, windows requires the
programmer to specify which attributes the child process will inherit. this is
accomplished by rst initializing the security attributes structure to allow
handles to be inherited and then redirecting the child processs handles for
standard input or standard output to the read or write handle of the pipe.
since the child will be reading from the pipe, the parent must redirect the
childs standard input to the read handle of the pipe. furthermore, as the
pipes are half duplex, it is necessary to prohibit the child from inheriting the
write-end of the pipe. the program to create the child process is similar to the
program in figure 3.11, except that the fth parameter is set to true, indicating
that the child process is to inherit designated handles from its parent. before
writing to the pipe, the parent rst closes its unused read end of the pipe. the
child process that reads from the pipe is shown in figure 3.29. before reading
from the pipe, this program obtains the read handle to the pipe by invoking
getstdhandle().
note that ordinary pipes require a parentchild relationship between the
communicating processes on both unix and windows systems. this means
that these pipes can be used only for communication between processes on the
same machine.
3.6.3.2
named pipes
ordinary pipes provide a simple mechanism for allowing a pair of processes
to communicate. however, ordinary pipes exist only while the processes are
communicating with one another. on both unix and windows systems, once
the processes have nished communicating and have terminated, the ordinary
pipe ceases to exist.
named pipes provide a much more powerful communication tool. com-
munication can be bidirectional, and no parentchild relationship is required.
once a named pipe is established, several processes can use it for communi-
cation. in fact, in a typical scenario, a named pipe has several writers. addi-
tionally, named pipes continue to exist after communicating processes have
146
chapter 3
processes
/* set up security attributes allowing pipes to be inherited */
security attributes sa = {sizeof(security attributes),null,true};
/* allocate memory */
zeromemory(&pi, sizeof(pi));
/* create the pipe */
if (!createpipe(&readhandle, &writehandle, &sa, 0)) {
fprintf(stderr, "create pipe failed");
return 1;
}
/* establish the start info structure for the child process */
getstartupinfo(&si);
si.hstdoutput = getstdhandle(std output handle);
/* redirect standard input to the read end of the pipe */
si.hstdinput = readhandle;
si.dwflags = startf usestdhandles;
/* dont allow the child to inherit the write end of pipe */
sethandleinformation(writehandle, handle flag inherit, 0);
/* create the child process */
createprocess(null, "child.exe", null, null,
true, /* inherit handles */
0, null, null, &si, &pi);
/* close the unused end of the pipe */
closehandle(readhandle);
/* the parent writes to the pipe */
if (!writefile(writehandle, message,buffer size,&written,null))
fprintf(stderr, "error writing to pipe.");
/* close the write end of the pipe */
closehandle(writehandle);
/* wait for the child to exit */
waitforsingleobject(pi.hprocess, infinite);
closehandle(pi.hprocess);
closehandle(pi.hthread);
return 0;
}
figure 3.28
figure 3.27, continued.
nished. both unix and windows systems support named pipes, although the
details of implementation differ greatly. next, we explore named pipes in each
of these systems.
3.7
summary
147
#include <stdio.h>
#include <windows.h>
#define buffer size 25
int main(void)
{
handle readhandle;
char buffer[buffer size];
dword read;
/* get the read handle of the pipe */
readhandle = getstdhandle(std input handle);
/* the child reads from the pipe */
if (readfile(readhandle, buffer, buffer size, &read, null))
printf("child read %s",buffer);
else
fprintf(stderr, "error reading from pipe");
return 0;
}
figure 3.29
windows anonymous pipeschild process.
named pipes are referred to as fifos in unix systems. once created, they
appear as typical les in the le system. a fifo is created with the mkfifo()
system call and manipulated with the ordinary open(), read(), write(),
and close() system calls. it will continue to exist until it is explicitly deleted
from the le system. although fifos allow bidirectional communication, only
half-duplex transmission is permitted. if data must travel in both directions,
two fifos are typically used. additionally, the communicating processes must
reside on the same machine. if intermachine communication is required,
sockets (section 3.6.1) must be used.
named pipes on windows systems provide a richer communication mech-
anism than their unix counterparts. full-duplex communication is allowed,
and the communicating processes may reside on either the same or different
machines. additionally, only byte-oriented data may be transmitted across a
unix fifo, whereas windows systems allow either byte- or message-oriented
data. named pipes are created with the createnamedpipe() function, and a
client can connect to a named pipe using connectnamedpipe(). communi-
cation over the named pipe can be accomplished using the readfile() and
writefile() functions.
3.7
summary
a process is a program in execution. as a process executes, it changes state. the
state of a process is dened by that processs current activity. each process may
be in one of the following states: new, ready, running, waiting, or terminated.
148
chapter 3
processes
pipes in practice
pipes are used quite often in the unix command-line environment for
situations in which the output of one command serves as input to another. for
example, the unix ls command produces a directory listing. for especially
long directory listings, the output may scroll through several screens. the
command more manages output by displaying only one screen of output at
a time; the user must press the space bar to move from one screen to the next.
setting up a pipe between the ls and more commands (which are running as
individual processes) allows the output of ls to be delivered as the input to
more, enabling the user to display a large directory listing a screen at a time.
a pipe can be constructed on the command line using the | character. the
complete command is
ls | more
in this scenario, the ls command serves as the producer, and its output is
consumed by the more command.
windows systems provide a more command for the dos shell with
functionality similar to that of its unix counterpart. the dos shell also uses
the | character for establishing a pipe. the only difference is that to get
a directory listing, dos uses the dir command rather than ls, as shown
below:
dir | more
each process is represented in the operating system by its own process control
block (pcb).
a process, when it is not executing, is placed in some waiting queue. there
are two major classes of queues in an operating system: i/o request queues
and the ready queue. the ready queue contains all the processes that are ready
to execute and are waiting for the cpu. each process is represented by a pcb.
the operating system must select processes from various scheduling
queues. long-term (job) scheduling is the selection of processes that will be
allowed to contend for the cpu. normally, long-term scheduling is heavily
inuenced by resource-allocation considerations, especially memory manage-
ment. short-term (cpu) scheduling is the selection of one process from the
ready queue.
operating systems must provide a mechanism for parent processes to
create new child processes. the parent may wait for its children to terminate
before proceeding, or the parent and children may execute concurrently. there
are several reasons for allowing concurrent execution: information sharing,
computation speedup, modularity, and convenience.
the processes executing in the operating system may be either independent
processes or cooperating processes. cooperating processes require an interpro-
cess communication mechanism to communicate with each other. principally,
communication is achieved through two schemes: shared memory and mes-
sage passing. the shared-memory method requires communicating processes
practice exercises
149
#include <sys/types.h>
#include <stdio.h>
#include <unistd.h>
int value = 5;
int main()
{
pid t pid;
pid = fork();
if (pid == 0) { /* child process */
value += 15;
return 0;
}
else if (pid > 0) { /* parent process */
wait(null);
printf("parent: value = %d",value); /* line a */
return 0;
}
}
figure 3.30
what output will be at line a?
to share some variables. the processes are expected to exchange information
through the use of these shared variables. in a shared-memory system, the
responsibility for providing communication rests with the application pro-
grammers; the operating system needs to provide only the shared memory.
the message-passing method allows the processes to exchange messages.
the responsibility for providing communication may rest with the operating
system itself. these two schemes are not mutually exclusive and can be used
simultaneously within a single operating system.
communication in clientserver systems may use (1) sockets, (2) remote
procedure calls (rpcs), or (3) pipes. a socket is dened as an endpoint for
communication. a connection between a pair of applications consists of a pair
of sockets, one at each end of the communication channel. rpcs are another
form of distributed communication. an rpc occurs when a process (or thread)
calls a procedure on a remote application. pipes provide a relatively simple
ways for processes to communicate with one another. ordinary pipes allow
communication between parent and child processes, while named pipes permit
unrelated processes to communicate.
practice exercises
3.1
using the program shown in figure 3.30, explain what the output will
be at line a.
3.2
including the initial parent process, how many processes are created by
the program shown in figure 3.31?
150
chapter 3
processes
#include <stdio.h>
#include <unistd.h>
int main()
{
/* fork a child process */
fork();
/* fork another child process */
fork();
/* and fork another */
fork();
return 0;
}
figure 3.31
how many processes are created?
3.3
original versions of apples mobile ios operating system provided no
means of concurrent processing. discuss three major complications that
concurrent processing adds to an operating system.
3.4
the sun ultrasparc processor has multiple register sets. describe what
happens when a context switch occurs if the new context is already
loaded into one of the register sets. what happens if the new context is
in memory rather than in a register set and all the register sets are in
use?
3.5
when a process creates a new process using the fork() operation, which
of the following states is shared between the parent process and the child
process?
a.
stack
b.
heap
c.
shared memory segments
3.6
consider the exactly oncesemantic with respect to the rpc mechanism.
does the algorithm for implementing this semantic execute correctly
even if the ack message sent back to the client is lost due to a network
problem? describe the sequence of messages, and discuss whether
exactly once is still preserved.
3.7
assume that a distributed system is susceptible to server failure. what
mechanisms would be required to guarantee the exactly once semantic
for execution of rpcs?
exercises
3.8
describe the differences among short-term, medium-term, and long-
term scheduling.
exercises
151
#include <stdio.h>
#include <unistd.h>
int main()
{
int i;
for (i = 0; i < 4; i++)
fork();
return 0;
}
figure 3.32
how many processes are created?
3.9
describe the actions taken by a kernel to context-switch between
processes.
3.10
construct a process tree similar to figure 3.8. to obtain process infor-
mation for the unix or linux system, use the command ps -ael.
#include <sys/types.h>
#include <stdio.h>
#include <unistd.h>
int main()
{
pid t pid;
/* fork a child process */
pid = fork();
if (pid < 0) { /* error occurred */
fprintf(stderr, "fork failed");
return 1;
}
else if (pid == 0) { /* child process */
execlp("/bin/ls","ls",null);
printf("line j");
}
else { /* parent process */
/* parent will wait for the child to complete */
wait(null);
printf("child complete");
}
return 0;
}
figure 3.33
when will line j be reached?
152
chapter 3
processes
use the command man ps to get more information about the ps com-
mand. the task manager on windows systems does not provide the
parent process id, but the process monitor tool, available from tech-
net.microsoft.com, provides a process-tree tool.
3.11
explain the role of the init process on unix and linux systems in regard
to process termination.
3.12
including the initial parent process, how many processes are created by
the program shown in figure 3.32?
3.13
explain the circumstances under which which the line of code marked
printf("line j") in figure 3.33 will be reached.
3.14
using the program in figure 3.34, identify the values of pid at lines a, b,
c, and d. (assume that the actual pids of the parent and child are 2600
and 2603, respectively.)
#include <sys/types.h>
#include <stdio.h>
#include <unistd.h>
int main()
{
pid t pid, pid1;
/* fork a child process */
pid = fork();
if (pid < 0) { /* error occurred */
fprintf(stderr, "fork failed");
return 1;
}
else if (pid == 0) { /* child process */
pid1 = getpid();
printf("child: pid = %d",pid); /* a */
printf("child: pid1 = %d",pid1); /* b */
}
else { /* parent process */
pid1 = getpid();
printf("parent: pid = %d",pid); /* c */
printf("parent: pid1 = %d",pid1); /* d */
wait(null);
}
return 0;
}
figure 3.34
what are the pid values?
exercises
153
#include <sys/types.h>
#include <stdio.h>
#include <unistd.h>
#define size 5
int nums[size] = {0,1,2,3,4};
int main()
{
int i;
pid t pid;
pid = fork();
if (pid == 0) {
for (i = 0; i < size; i++) {
nums[i] *= -i;
printf("child: %d ",nums[i]); /* line x */
}
}
else if (pid > 0) {
wait(null);
for (i = 0; i < size; i++)
printf("parent: %d ",nums[i]); /* line y */
}
return 0;
}
figure 3.35
what output will be at line x and line y?
3.15
give an example of a situation in which ordinary pipes are more suitable
than named pipes and an example of a situation in which named pipes
are more suitable than ordinary pipes.
3.16
consider the rpc mechanism. describe the undesirable consequences
that could arise from not enforcing either the at most once or exactly
once semantic. describe possible uses for a mechanism that has neither
of these guarantees.
3.17
using the program shown in figure 3.35, explain what the output will
be at lines x and y.
3.18
what are the benets and the disadvantages of each of the following?
consider both the system level and the programmer level.
a.
synchronous and asynchronous communication
b.
automatic and explicit buffering
c.
send by copy and send by reference
d.
fixed-sized and variable-sized messages
154
chapter 3
processes
programming problems
3.19
using either a unix or a linux system, write a c program that forks
a child process that ultimately becomes a zombie process. this zombie
process must remain in the system for at least 10 seconds. process states
can be obtained from the command
ps -l
the process states are shown below the s column; processes with a state
of z are zombies. the process identier (pid) of the child process is listed
in the pid column, and that of the parent is listed in the ppid column.
perhaps the easiest way to determine that the child process is indeed
a zombie is to run the program that you have written in the background
(using the &) and then run the command ps -l to determine whether
the child is a zombie process. because you do not want too many zombie
processes existing in the system, you will need to remove the one that
you have created. the easiest way to do that is to terminate the parent
process using the kill command. for example, if the process id of the
parent is 4884, you would enter
kill -9 4884
3.20
an operating systems pid manager is responsible for managing process
identiers. when a process is rst created, it is assigned a unique pid
by the pid manager. the pid is returned to the pid manager when the
process completes execution, and the manager may later reassign this
pid. process identiers are discussed more fully in section 3.3.1. what
is most important here is to recognize that process identiers must be
unique; no two active processes can have the same pid.
use the following constants to identify the range of possible pid values:
#define min pid 300
#define max pid 5000
you may use any data structure of your choice to represent the avail-
ability of process identiers. one strategy is to adopt what linux has
done and use a bitmap in which a value of 0 at position i indicates that
a process id of value i is available and a value of 1 indicates that the
process id is currently in use.
implement the following api for obtaining and releasing a pid:
int allocate map(void)creates and initializes a data structure
for representing pids; returns1 if unsuccessful, 1 if successful
int allocate pid(void)allocates and returns a pid; returns
1 if unable to allocate a pid (all pids are in use)
void release pid(int pid)releases a pid
this programming problem will be modied later on in chpaters 4 and
5.
programming problems
155
3.21
the collatz conjecture concerns what happens when we take any
positive integer n and apply the following algorithm:
n =

n/2,
if n is even
3 n + 1,
if n is odd
the conjecture states that when this algorithm is continually applied,
all positive integers will eventually reach 1. for example, if n = 35, the
sequence is
35, 106, 53, 160, 80, 40, 20, 10, 5, 16, 8, 4, 2, 1
write a c program using the fork() system call that generates this
sequence in the child process. the starting number will be provided
from the command line. for example, if 8 is passed as a parameter on
the command line, the child process will output 8, 4, 2, 1. because the
parent and child processes have their own copies of the data, it will be
necessary for the child to output the sequence. have the parent invoke
the wait() call to wait for the child process to complete before exiting
the program. perform necessary error checking to ensure that a positive
integer is passed on the command line.
3.22
in exercise 3.21, the child process must output the sequence of numbers
generated from the algorithm specied by the collatz conjecture because
the parent and child have their own copies of the data. another
approach to designing this program is to establish a shared-memory
object between the parent and child processes. this technique allows the
child to write the contents of the sequence to the shared-memory object.
the parent can then output the sequence when the child completes.
because the memory is shared, any changes the child makes will be
reected in the parent process as well.
this program will be structured using posix shared memory as
described in section 3.5.1. the parent process will progress through the
following steps:
a.
establish the shared-memory object (shm open(), ftruncate(),
and mmap()).
b.
create the child process and wait for it to terminate.
c.
output the contents of shared memory.
d.
remove the shared-memory object.
one area of concern with cooperating processes involves synchro-
nization issues. in this exercise, the parent and child processes must be
coordinated so that the parent does not output the sequence until the
child nishes execution. these two processes will be synchronized using
the wait() system call: the parent process will invoke wait(), which
will suspend it until the child process exits.
3.23
section 3.6.1 describes port numbers below 1024 as being well known
that is, they provide standard services. port 17 is known as the quote-of-
156
chapter 3
processes
the-day service. when a client connects to port 17 on a server, the server
responds with a quote for that day.
modify the date server shown in figure 3.21 so that it delivers a quote
of the day rather than the current date. the quotes should be printable
ascii characters and should contain fewer than 512 characters, although
multiple lines are allowed. since port 17 is well known and therefore
unavailable, have your server listen to port 6017. the date client shown
in figure 3.22 can be used to read the quotes returned by your server.
3.24
a haiku is a three-line poem in which the rst line contains ve syllables,
the second line contains seven syllables, and the third line contains ve
syllables. write a haiku server that listens to port 5575. when a client
connects to this port, the server responds with a haiku. the date client
shown in figure 3.22 can be used to read the quotes returned by your
haiku server.
3.25
an echo server echoes back whatever it receives from a client. for
example, if a client sends the server the string hello there!, the server
will respond with hello there!
write an echo server using the java networking api described in
section 3.6.1. this server will wait for a client connection using the
accept() method. when a client connection is received, the server will
loop, performing the following steps:
read data from the socket into a buffer.
write the contents of the buffer back to the client.
the server will break out of the loop only when it has determined that
the client has closed the connection.
the
date
server
shown
in
figure
3.21
uses
the
java.io.bufferedreader
class.
bufferedreader
extends
the
java.io.reader class, which is used for reading character streams.
however,
the
echo
server
cannot
guarantee
that
it
will
read
characters from clients; it may receive binary data as well. the
class java.io.inputstream deals with data at the byte level rather
than the character level. thus, your echo server must use an object
that extends java.io.inputstream. the read() method in the
java.io.inputstream class returns 1 when the client has closed its
end of the socket connection.
3.26
design a program using ordinary pipes in which one process sends a
string message to a second process, and the second process reverses
the case of each character in the message and sends it back to the rst
process. for example, if the rst process sends the messagehi there, the
second process will return hi there. this will require using two pipes,
one for sending the original message from the rst to the second process
and the other for sending the modied message from the second to the
rst process. you can write this program using either unix or windows
pipes.
3.27
design a le-copying program named filecopy using ordinary pipes.
this program will be passed two parameters: the name of the le to be
programming projects
157
copied and the name of the copied le. the program will then create
an ordinary pipe and write the contents of the le to be copied to the
pipe. the child process will read this le from the pipe and write it to
the destination le. for example, if we invoke the program as follows:
filecopy input.txt copy.txt
the le input.txt will be written to the pipe. the child process will
read the contents of this le and write it to the destination le copy.txt.
you may write this program using either unix or windows pipes.
programming projects
project 1unix shell and history feature
this project consists of designing a c program to serve as a shell interface
that accepts user commands and then executes each command in a separate
process. this project can be completed on any linux, unix, or mac os x system.
a shell interface gives the user a prompt, after which the next command
is entered. the example below illustrates the prompt osh> and the users
next command: cat prog.c. (this command displays the le prog.c on the
terminal using the unix cat command.)
osh> cat prog.c
one technique for implementing a shell interface is to have the parent process
rst read what the user enters on the command line (in this case, cat
prog.c), and then create a separate child process that performs the command.
unless otherwise specied, the parent process waits for the child to exit
before continuing. this is similar in functionality to the new process creation
illustrated in figure 3.10. however, unix shells typically also allow the child
process to run in the background, or concurrently. to accomplish this, we add
an ampersand (&) at the end of the command. thus, if we rewrite the above
command as
osh> cat prog.c &
the parent and child processes will run concurrently.
the separate child process is created using the fork() system call, and the
users command is executed using one of the system calls in the exec() family
(as described in section 3.3.1).
a c program that provides the general operations of a command-line shell
is supplied in figure 3.36. the main() function presents the prompt osh->
and outlines the steps to be taken after input from the user has been read. the
main() function continually loops as long as should run equals 1; when the
user enters exit at the prompt, your program will set should run to 0 and
terminate.
this project is organized into two parts: (1) creating the child process and
executing the command in the child, and (2) modifying the shell to allow a
history feature.
158
chapter 3
processes
#include <stdio.h>
#include <unistd.h>
#define max line 80 /* the maximum length command */
int main(void)
{
char *args[max line/2 + 1]; /* command line arguments */
int should run = 1; /* flag to determine when to exit program */
while (should run) {
printf("osh>");
fflush(stdout);
/**
* after reading user input, the steps are:
* (1) fork a child process using fork()
* (2) the child process will invoke execvp()
* (3) if command included &, parent will invoke wait()
*/
}
return 0;
}
figure 3.36
outline of simple shell.
part i creating a child process
the rst task is to modify the main() function in figure 3.36 so that a child
process is forked and executes the command specied by the user. this will
require parsing what the user has entered into separate tokens and storing the
tokens in an array of character strings (args in figure 3.36). for example, if the
user enters the command ps -ael at the osh> prompt, the values stored in the
args array are:
args[0] = "ps"
args[1] = "-ael"
args[2] = null
this args array will be passed to the execvp() function, which has the
following prototype:
execvp(char *command, char *params[]);
here, command represents the command to be performed and params stores the
parameters to this command. for this project, the execvp() function should
be invoked as execvp(args[0], args). be sure to check whether the user
included an & to determine whether or not the parent process is to wait for the
child to exit.
programming projects
159
part iicreating a history feature
the next task is to modify the shell interface program so that it provides
a history feature that allows the user to access the most recently entered
commands. the user will be able to access up to 10 commands by using the
feature. the commands will be consecutively numbered starting at 1, and
the numbering will continue past 10. for example, if the user has entered 35
commands, the 10 most recent commands will be numbered 26 to 35.
the user will be able to list the command history by entering the command
history
at the osh> prompt. as an example, assume that the history consists of the
commands (from most to least recent):
ps, ls -l, top, cal, who, date
the command history will output:
6 ps
5 ls -l
4 top
3 cal
2 who
1 date
your program should support two techniques for retrieving commands
from the command history:
1. when the user enters !!, the most recent command in the history is
executed.
2. when the user enters a single ! followed by an integer n, the nth
command in the history is executed.
continuing our example from above, if the user enters !!, the ps command
will be performed; if the user enters !3, the command cal will be executed.
any command executed in this fashion should be echoed on the users screen.
the command should also be placed in the history buffer as the next command.
the program should also manage basic error handling. if there are
no commands in the history, entering !! should result in a message no
commands in history. if there is no command corresponding to the number
entered with the single !, the program should output "no such command in
history."
project 2linux kernel module for listing tasks
in this project, you will write a kernel module that lists all current tasks in a
linux system. be sure to review the programming project in chapter 2, which
deals with creating linux kernel modules, before you begin this project. the
project can be completed using the linux virtual machine provided with this
text.
160
chapter 3
processes
part iiterating over tasks linearly
as illustrated in section 3.1, the pcb in linux is represented by the structure
task struct, which is found in the <linux/sched.h> include le. in linux,
the for each process() macro easily allows iteration over all current tasks
in the system:
#include <linux/sched.h>
struct task struct *task;
for each process(task) {
/* on each iteration task points to the next task */
}
the various elds in task struct can then be displayed as the program loops
through the for each process() macro.
part i assignment
design a kernel module that iterates through all tasks in the system using the
for each process() macro. in particular, output the task name (known as
executable name), state, and process id of each task. (you will probably have
to read through the task struct structure in <linux/sched.h> to obtain the
names of these elds.) write this code in the module entry point so that its
contents will appear in the kernel log buffer, which can be viewed using the
dmesg command. to verify that your code is working correctly, compare the
contents of the kernel log buffer with the output of the following command,
which lists all tasks in the system:
ps -el
the two values should be very similar. because tasks are dynamic, however, it
is possible that a few tasks may appear in one listing but not the other.
part iiiterating over tasks with a depth-first search tree
the second portion of this project involves iterating over all tasks in the system
using a depth-rst search (dfs) tree. (as an example: the dfs iteration of the
processes in figure 3.8 is 1, 8415, 8416, 9298, 9204, 2, 6, 200, 3028, 3610, 4005.)
linux maintains its process tree as a series of lists. examining the
task struct in <linux/sched.h>, we see two struct list head objects:
children
and
sibling
bibliographical notes
161
these objects are pointers to a list of the tasks children, as well as its sib-
lings. linux also maintains references to the init task (struct task struct
init task). using this information as well as macro operations on lists, we
can iterate over the children of init as follows:
struct task struct *task;
struct list head *list;
list for each(list, &init task->children) {
task = list entry(list, struct task struct, sibling);
/* task points to the next child in the list */
}
the list for each() macro is passed two parameters, both of type struct
list head:
a pointer to the head of the list to be traversed
a pointer to the head node of the list to be traversed
at each iteration of list for each(), the rst parameter is set to the list
structure of the next child. we then use this value to obtain each structure in
the list using the list entry() macro.
part ii assignment
beginning from the init task, design a kernel module that iterates over all tasks
in the system using a dfs tree. just as in the rst part of this project, output
the name, state, and pid of each task. perform this iteration in the kernel entry
module so that its output appears in the kernel log buffer.
if you output all tasks in the system, you may see many more tasks than
appear with the ps -ael command. this is because some threads appear as
children but do not show up as ordinary processes. therefore, to check the
output of the dfs tree, use the command
ps -elf
this command lists all tasksincluding threadsin the system. to verify
that you have indeed performed an appropriate dfs iteration, you will have to
examine the relationships among the various tasks output by the ps command.
bibliographical notes
process creation, management, and ipc in unix and windows systems,
respectively, are discussed in [robbins and robbins (2003)] and [russinovich
and solomon (2009)]. [love (2010)] covers support for processes in the linux
kernel, and [hart (2005)] covers windows systems programming in detail.
coverage of the multiprocess model used in googles chrome can be found at
http://blog.chromium.org/2008/09/multi-process-architecture.html.
162
chapter 3
processes
message passing for multicore systems is discussed in [holland and
seltzer (2011)]. [baumann et al. (2009)] describe performance issues in shared-
memory and message-passing systems. [vahalia (1996)] describes interprocess
communication in the mach system.
the implementation of rpcs is discussed by [birrell and nelson (1984)].
[staunstrup (1982)] discusses procedure calls versus message-passing com-
munication. [harold (2005)] provides coverage of socket programming in
java.
[hart (2005)] and [robbins and robbins (2003)] cover pipes in windows
and unix systems, respectively.
bibliography
[baumann et al. (2009)]
a. baumann, p. barham, p.-e. dagand, t. harris,
r. isaacs, p. simon, t. roscoe, a. sch
upbach, and a. singhania, the multikernel:
a new os architecture for scalable multicore systems (2009), pages 2944.
[birrell and nelson (1984)]
a. d. birrell and b. j. nelson, implementing
remote procedure calls, acm transactions on computer systems, volume 2,
number 1 (1984), pages 3959.
[harold (2005)]
e. r. harold, java network programming, third edition, oreilly
& associates (2005).
[hart (2005)]
j. m. hart, windows system programming, third edition, addison-
wesley (2005).
[holland and seltzer (2011)]
d. holland and m. seltzer, multicore oses: look-
ing forward from 1991, er, 2011, proceedings of the 13th usenix conference on
hot topics in operating systems (2011), pages 3333.
[love (2010)]
r. love, linux kernel development, third edition, developers
library (2010).
[robbins and robbins (2003)]
k. robbins and s. robbins, unix systems pro-
gramming: communication, concurrency and threads, second edition, prentice
hall (2003).
[russinovich and solomon (2009)]
m. e. russinovich and d. a. solomon, win-
dows internals: including windows server 2008 and windows vista, fifth edition,
microsoft press (2009).
[staunstrup (1982)]
j. staunstrup, message passing communication versus
procedure call communication, softwarepractice and experience, volume 12,
number 3 (1982), pages 223234.
[vahalia (1996)]
u. vahalia, unix internals: the new frontiers, prentice hall
(1996).
4
c h a p t e r
threads
the process model introduced in chapter 3 assumed that a process was
an executing program with a single thread of control. virtually all modern
operating systems, however, provide features enabling a process to contain
multiple threads of control. in this chapter, we introduce many concepts
associated with multithreaded computer systems, including a discussion of
the apis for the pthreads, windows, and java thread libraries. we look at a
number of issues related to multithreaded programming and its effect on the
design of operating systems. finally, we explore how the windows and linux
operating systems support threads at the kernel level.
chapter objectives
to introduce the notion of a threada fundamental unit of cpu utilization
that forms the basis of multithreaded computer systems.
to discuss the apis for the pthreads, windows, and java thread libraries.
to explore several strategies that provide implicit threading.
to examine issues related to multithreaded programming.
to cover operating system support for threads in windows and linux.
4.1
overview
a thread is a basic unit of cpu utilization; it comprises a thread id, a program
counter, a register set, and a stack. it shares with other threads belonging
to the same process its code section, data section, and other operating-system
resources, such as open les and signals. a traditional (or heavyweight) process
has a single thread of control. if a process has multiple threads of control, it
can perform more than one task at a time. figure 4.1 illustrates the difference
between a traditional single-threaded process and a multithreaded process.
4.1.1
motivation
most software applications that run on modern computers are multithreaded.
an application typically is implemented as a separate process with several
163
164
chapter 4
threads
registers
code
data
files
stack
registers
registers
registers
code
data
files
stack
stack
stack
thread
thread
single-threaded process
multithreaded process
figure 4.1
single-threaded and multithreaded processes.
threads of control. a web browser might have one thread display images or
text while another thread retrieves data from the network, for example. a
word processor may have a thread for displaying graphics, another thread for
responding to keystrokes from the user, and a third thread for performing
spelling and grammar checking in the background. applications can also
be designed to leverage processing capabilities on multicore systems. such
applications can perform several cpu-intensive tasks in parallel across the
multiple computing cores.
in certain situations, a single application may be required to perform
several similar tasks. for example, a web server accepts client requests for
web pages, images, sound, and so forth. a busy web server may have several
(perhaps thousands of) clients concurrently accessing it. if the web server ran
as a traditional single-threaded process, it would be able to service only one
client at a time, and a client might have to wait a very long time for its request
to be serviced.
one solution is to have the server run as a single process that accepts
requests. when the server receives a request, it creates a separate process
to service that request. in fact, this process-creation method was in common
use before threads became popular. process creation is time consuming and
resource intensive, however. if the new process will perform the same tasks as
the existing process, why incur all that overhead? it is generally more efcient
to use one process that contains multiple threads. if the web-server process is
multithreaded, the server will create a separate thread that listens for client
requests. when a request is made, rather than creating another process, the
server creates a new thread to service the request and resume listening for
additional requests. this is illustrated in figure 4.2.
threads also play a vital role in remote procedure call (rpc) systems. recall
from chapter 3 that rpcs allow interprocess communication by providing a
communication mechanism similar to ordinary function or procedure calls.
typically, rpc servers are multithreaded. when a server receives a message, it
4.1
overview
165
client
(1) request
(2) create new
thread to service
the request
(3) resume listening
for additional
client requests
server
thread
figure 4.2
multithreaded server architecture.
services the message using a separate thread. this allows the server to service
several concurrent requests.
finally, most operating-system kernels are now multithreaded. several
threads operate in the kernel, and each thread performs a specic task, such
as managing devices, managing memory, or interrupt handling. for example,
solaris has a set of threads in the kernel specically for interrupt handling;
linux uses a kernel thread for managing the amount of free memory in the
system.
4.1.2
benets
the benets of multithreaded programming can be broken down into four
major categories:
1. responsiveness. multithreading an interactive application may allow
a program to continue running even if part of it is blocked or is
performing a lengthy operation, thereby increasing responsiveness to
the user. this quality is especially useful in designing user interfaces. for
instance, consider what happens when a user clicks a button that results
in the performance of a time-consuming operation. a single-threaded
application would be unresponsive to the user until the operation had
completed. in contrast, if the time-consuming operation is performed in
a separate thread, the application remains responsive to the user.
2. resource sharing. processes can only share resources through techniques
such as shared memory and message passing. such techniques must
be explicitly arranged by the programmer. however, threads share the
memory and the resources of the process to which they belong by default.
the benet of sharing code and data is that it allows an application to
have several different threads of activity within the same address space.
3. economy. allocating memory and resources for process creation is costly.
because threads share the resources of the process to which they belong,
it is more economical to create and context-switch threads. empirically
gauging the difference in overhead can be difcult, but in general it is
signicantly more time consuming to create and manage processes than
threads. in solaris, for example, creating a process is about thirty times
166
chapter 4
threads
t1
t2
t3
t4
t1
t2
t3
t4
t1
single core
time
figure 4.3
concurrent execution on a single-core system.
slower than is creating a thread, and context switching is about ve times
slower.
4. scalability. the benets of multithreading can be even greater in a
multiprocessor architecture, where threads may be running in parallel
on different processing cores. a single-threaded process can run on only
one processor, regardless how many are available. we explore this issue
further in the following section.
4.2
multicore programming
earlier in the history of computer design, in response to the need for more
computing performance, single-cpu systems evolved into multi-cpu systems.
a more recent, similar trend in system design is to place multiple computing
cores on a single chip. each core appears as a separate processor to the
operating system (section 1.3.2). whether the cores appear across cpu chips or
within cpu chips, we call these systems multicore or multiprocessor systems.
multithreaded programming provides a mechanism for more efcient use
of these multiple computing cores and improved concurrency. consider an
application with four threads. on a system with a single computing core,
concurrency merely means that the execution of the threads will be interleaved
over time (figure 4.3), because the processing core is capable of executing only
one thread at a time. on a system with multiple cores, however, concurrency
means that the threads can run in parallel, because the system can assign a
separate thread to each core (figure 4.4).
notice the distinction between parallelism and concurrency in this discus-
sion. a system is parallel if it can perform more than one task simultaneously.
in contrast, a concurrent system supports more than one task by allowing all
the tasks to make progress. thus, it is possible to have concurrency without
parallelism. before the advent of smp and multicore architectures, most com-
puter systems had only a single processor. cpu schedulers were designed to
provide the illusion of parallelism by rapidly switching between processes in
t1
t3
t1
t3
t1
core 1
t2
t4
t2
t4
t2
core 2
time
figure 4.4
parallel execution on a multicore system.
4.2
multicore programming
167
amdahls law
amdahls law is a formula that identies potential performance gains from
adding additional computing cores to an application that has both serial
(nonparallel) and parallel components. if s is the portion of the application
that must be performed serially on a system with n processing cores, the
formula appears as follows:
speedup
1
s + (1s)
n
as an example, assume we have an application that is 75 percent parallel and
25 percent serial. if we run this application on a system with two processing
cores, we can get a speedup of 1.6 times. if we add two additional cores (for
a total of four), the speedup is 2.28 times.
one interesting fact about amdahls law is that as n approaches innity,
the speedup converges to 1/s. for example, if 40 percent of an application
is performed serially, the maximum speedup is 2.5 times, regardless of
the number of processing cores we add. this is the fundamental principle
behind amdahls law: the serial portion of an application can have a
disproportionate effect on the performance we gain by adding additional
computing cores.
some argue that amdahls law does not take into account the hardware
performance enhancements used in the design of contemporary multicore
systems. such arguments suggest amdahls law may cease to be applicable
as the number of processing cores continues to increase on modern computer
systems.
the system, thereby allowing each process to make progress. such processes
were running concurrently, but not in parallel.
as systems have grown from tens of threads to thousands of threads, cpu
designers have improved system performance by adding hardware to improve
thread performance. modern intel cpus frequently support two threads per
core, while the oracle t4 cpu supports eight threads per core. this support
means that multiple threads can be loaded into the core for fast switching.
multicore computers will no doubt continue to increase in core counts and
hardware thread support.
4.2.1
programming challenges
the trend towards multicore systems continues to place pressure on system
designers and application programmers to make better use of the multiple
computing cores. designers of operating systems must write scheduling
algorithms that use multiple processing cores to allow the parallel execution
shown in figure 4.4. for application programmers, the challenge is to modify
existing programs as well as design new programs that are multithreaded.
in general, ve areas present challenges in programming for multicore
systems:
168
chapter 4
threads
1. identifying tasks. this involves examining applications to nd areas
that can be divided into separate, concurrent tasks. ideally, tasks are
independent of one another and thus can run in parallel on individual
cores.
2. balance. while identifying tasks that can run in parallel, programmers
must also ensure that the tasks perform equal work of equal value. in
some instances, a certain task may not contribute as much value to the
overall process as other tasks. using a separate execution core to run that
task may not be worth the cost.
3. data splitting. just as applications are divided into separate tasks, the
data accessed and manipulated by the tasks must be divided to run on
separate cores.
4. data dependency. the data accessed by the tasks must be examined for
dependencies between two or more tasks. when one task depends on
data from another, programmers must ensure that the execution of the
tasks is synchronized to accommodate the data dependency. we examine
such strategies in chapter 5.
5. testing and debugging. when a program is running in parallel on
multiple cores, many different execution paths are possible. testing and
debugging such concurrent programs is inherently more difcult than
testing and debugging single-threaded applications.
because of these challenges, many software developers argue that the advent of
multicore systems will require an entirely new approach to designing software
systems in the future. (similarly, many computer science educators believe that
software development must be taught with increased emphasis on parallel
programming.)
4.2.2
types of parallelism
in general, there are two types of parallelism: data parallelism and task
parallelism. data parallelism focuses on distributing subsets of the same data
across multiple computing cores and performing the same operation on each
core. consider, for example, summing the contents of an array of size n. on a
single-core system, one thread would simply sum the elements [0] . . . [n 1].
on a dual-core system, however, thread a, running on core 0, could sum the
elements [0] . . . [n/2 1] while thread b, running on core 1, could sum the
elements [n/2] . . . [n 1]. the two threads would be running in parallel on
separate computing cores.
task parallelism involves distributing not data but tasks (threads) across
multiple computing cores. each thread is performing a unique operation.
different threads may be operating on the same data, or they may be operating
on different data. consider again our example above. in contrast to that
situation, an example of task parallelism might involve two threads, each
performing a unique statistical operation on the array of elements. the threads
again are operating in parallel on separate computing cores, but each is
performing a unique operation.
4.3
multithreading models
169
fundamentally, then, data parallelism involves the distribution of data
across multiple cores and task parallelism on the distribution of tasks across
multiple cores. in practice, however, few applications strictly follow either data
or task parallelism. in most instances, applications use a hybrid of these two
strategies.
4.3
multithreading models
our discussion so far has treated threads in a generic sense. however, support
for threads may be provided either at the user level, for user threads, or by the
kernel, for kernel threads. user threads are supported above the kernel and
are managed without kernel support, whereas kernel threads are supported
and managed directly by the operating system. virtually all contemporary
operating systemsincluding windows, linux, mac os x, and solaris
support kernel threads.
ultimately, a relationship must exist between user threads and kernel
threads. in this section, we look at three common ways of establishing such a
relationship: the many-to-one model, the one-to-one model, and the many-to-
many model.
4.3.1
many-to-one model
the many-to-one model (figure 4.5) maps many user-level threads to one
kernel thread. thread management is done by the thread library in user space,
so it is efcient (we discuss thread libraries in section 4.4). however, the entire
process will block if a thread makes a blocking system call. also, because only
one thread can access the kernel at a time, multiple threads are unable to run in
parallel on multicore systems. green threadsa thread library available for
solaris systems and adopted in early versions of javaused the many-to-one
model. however, very few systems continue to use the model because of its
inability to take advantage of multiple processing cores.
user thread
kernel thread
k
figure 4.5
many-to-one model.
170
chapter 4
threads
user thread
kernel thread
k
k
k
k
figure 4.6
one-to-one model.
4.3.2
one-to-one model
the one-to-one model (figure 4.6) maps each user thread to a kernel thread. it
provides more concurrency than the many-to-one model by allowing another
thread to run when a thread makes a blocking system call. it also allows
multiple threads to run in parallel on multiprocessors. the only drawback to
this model is that creating a user thread requires creating the corresponding
kernel thread. because the overhead of creating kernel threads can burden the
performance of an application, most implementations of this model restrict the
number of threads supported by the system. linux, along with the family of
windows operating systems, implement the one-to-one model.
4.3.3
many-to-many model
the many-to-many model (figure 4.7) multiplexes many user-level threads to
a smaller or equal number of kernel threads. the number of kernel threads
may be specic to either a particular application or a particular machine (an
application may be allocated more kernel threads on a multiprocessor than on
a single processor).
lets consider the effect of this design on concurrency. whereas the many-
to-one model allows the developer to create as many user threads as she wishes,
it does not result in true concurrency, because the kernel can schedule only
one thread at a time. the one-to-one model allows greater concurrency, but the
developer has to be careful not to create too many threads within an application
(and in some instances may be limited in the number of threads she can
user thread
kernel thread
k
k
k
figure 4.7
many-to-many model.
4.4
thread libraries
171
user thread
kernel thread
k
k
k
k
figure 4.8
two-level model.
create). the many-to-many model suffers from neither of these shortcomings:
developers can create as many user threads as necessary, and the corresponding
kernel threads can run in parallel on a multiprocessor. also, when a thread
performs a blocking system call, the kernel can schedule another thread for
execution.
one variation on the many-to-many model still multiplexes many user-
level threads to a smaller or equal number of kernel threads but also allows a
user-level thread to be bound to a kernel thread. this variation is sometimes
referred to as the two-level model (figure 4.8). the solaris operating system
supported the two-level model in versions older than solaris 9. however,
beginning with solaris 9, this system uses the one-to-one model.
4.4
thread libraries
a thread library provides the programmer with an api for creating and
managing threads. there are two primary ways of implementing a thread
library. the rst approach is to provide a library entirely in user space with no
kernel support. all code and data structures for the library exist in user space.
this means that invoking a function in the library results in a local function
call in user space and not a system call.
the second approach is to implement a kernel-level library supported
directly by the operating system. in this case, code and data structures for
the library exist in kernel space. invoking a function in the api for the library
typically results in a system call to the kernel.
three main thread libraries are in use today: posix pthreads, windows, and
java. pthreads, the threads extension of the posix standard, may be provided
as either a user-level or a kernel-level library. the windows thread library
is a kernel-level library available on windows systems. the java thread api
allows threads to be created and managed directly in java programs. however,
because in most instances the jvm is running on top of a host operating system,
the java thread api is generally implemented using a thread library available
on the host system. this means that on windows systems, java threads are
typically implemented using the windows api; unix and linux systems often
use pthreads.
172
chapter 4
threads
for posix and windows threading, any data declared globallythat is,
declared outside of any functionare shared among all threads belonging to
the same process. because java has no notion of global data, access to shared
data must be explicitly arranged between threads. data declared local to a
function are typically stored on the stack. since each thread has its own stack,
each thread has its own copy of local data.
in the remainder of this section, we describe basic thread creation using
these three thread libraries. as an illustrative example, we design a multi-
threaded program that performs the summation of a non-negative integer in a
separate thread using the well-known summation function:
sum =
n

i=0
i
for example, if n were 5, this function would represent the summation of
integers from 0 to 5, which is 15. each of the three programs will be run with
the upper bounds of the summation entered on the command line. thus, if the
user enters 8, the summation of the integer values from 0 to 8 will be output.
before we proceed with our examples of thread creation, we introduce
two general strategies for creating multiple threads: asynchronous threading
and synchronous threading. with asynchronous threading, once the parent
creates a child thread, the parent resumes its execution, so that the parent
and child execute concurrently. each thread runs independently of every other
thread, and the parent thread need not know when its child terminates. because
the threads are independent, there is typically little data sharing between
threads. asynchronous threading is the strategy used in the multithreaded
server illustrated in figure 4.2.
synchronous threading occurs when the parent thread creates one or more
children and then must wait for all of its children to terminate before it resumes
the so-called fork-join strategy. here, the threads created by the parent
perform work concurrently, but the parent cannot continue until this work
has been completed. once each thread has nished its work, it terminates
and joins with its parent. only after all of the children have joined can the
parent resume execution. typically, synchronous threading involves signicant
data sharing among threads. for example, the parent thread may combine the
results calculated by its various children. all of the following examples use
synchronous threading.
4.4.1
pthreads
pthreads refers to the posix standard (ieee 1003.1c) dening an api for thread
creation and synchronization. this is a specication for thread behavior,
not an implementation. operating-system designers may implement the
specication in any way they wish. numerous systems implement the pthreads
specication; most are unix-type systems, including linux, mac os x, and
solaris. although windows doesnt support pthreads natively, some third-
party implementations for windows are available.
the c program shown in figure 4.9 demonstrates the basic pthreads api for
constructing a multithreaded program that calculates the summation of a non-
negative integer in a separate thread. in a pthreads program, separate threads
4.4
thread libraries
173
#include <pthread.h>
#include <stdio.h>
int sum; /* this data is shared by the thread(s) */
void *runner(void *param); /* threads call this function */
int main(int argc, char *argv[])
{
pthread t tid; /* the thread identifier */
pthread attr t attr; /* set of thread attributes */
if (argc != 2) {
fprintf(stderr,"usage: a.out <integer value>\n");
return -1;
}
if (atoi(argv[1]) < 0) {
fprintf(stderr,"%d must be >= 0\n",atoi(argv[1]));
return -1;
}
/* get the default attributes */
pthread attr init(&attr);
/* create the thread */
pthread create(&tid,&attr,runner,argv[1]);
/* wait for the thread to exit */
pthread join(tid,null);
printf("sum = %d\n",sum);
}
/* the thread will begin control in this function */
void *runner(void *param)
{
int i, upper = atoi(param);
sum = 0;
for (i = 1; i <= upper; i++)
sum += i;
pthread exit(0);
}
figure 4.9
multithreaded c program using the pthreads api.
begin execution in a specied function. in figure 4.9, this is the runner()
function. when this program begins, a single thread of control begins in
main(). after some initialization, main() creates a second thread that begins
control in the runner() function. both threads share the global data sum.
lets look more closely at this program. all pthreads programs must
include the pthread.h header le. the statement pthread t tid declares
174
chapter 4
threads
#define num threads 10
/* an array of threads to be joined upon */
pthread t workers[num threads];
for (int i = 0; i < num threads; i++)
pthread join(workers[i], null);
figure 4.10
pthread code for joining ten threads.
the identier for the thread we will create. each thread has a set of attributes,
including stack size and scheduling information. the pthread attr t attr
declaration represents the attributes for the thread. we set the attributes in the
function call pthread attr init(&attr). because we did not explicitly set
any attributes, we use the default attributes provided. (in chapter 6, we discuss
some of the scheduling attributes provided by the pthreads api.) a separate
thread is created with the pthread create() function call. in addition to
passing the thread identier and the attributes for the thread, we also pass the
name of the function where the new thread will begin executionin this case,
the runner() function. last, we pass the integer parameter that was provided
on the command line, argv[1].
at this point, the program has two threads: the initial (or parent) thread
in main() and the summation (or child) thread performing the summation
operation in the runner() function. this program follows the fork-join strategy
described earlier: after creating the summation thread, the parent thread
will wait for it to terminate by calling the pthread join() function. the
summation thread will terminate when it calls the function pthread exit().
once the summation thread has returned, the parent thread will output the
value of the shared data sum.
this example program creates only a single thread. with the growing
dominance of multicore systems, writing programs containing several threads
has become increasingly common. a simple method for waiting on several
threads using the pthread join() function is to enclose the operation within
a simple for loop. for example, you can join on ten threads using the pthread
code shown in figure 4.10.
4.4.2
windows threads
the technique for creating threads using the windows thread library is similar
to the pthreads technique in several ways. we illustrate the windows thread
api in the c program shown in figure 4.11. notice that we must include the
windows.h header le when using the windows api.
just as in the pthreads version shown in figure 4.9, data shared by the
separate threadsin this case, sumare declared globally (the dword data
type is an unsigned 32-bit integer). we also dene the summation() function
that is to be performed in a separate thread. this function is passed a pointer
to a void, which windows denes as lpvoid. the thread performing this
function sets the global data sum to the value of the summation from 0 to the
parameter passed to summation().
4.4
thread libraries
175
#include <windows.h>
#include <stdio.h>
dword sum; /* data is shared by the thread(s) */
/* the thread runs in this separate function */
dword winapi summation(lpvoid param)
{
dword upper = *(dword*)param;
for (dword i = 0; i <= upper; i++)
sum += i;
return 0;
}
int main(int argc, char *argv[])
{
dword threadid;
handle threadhandle;
int param;
if (argc != 2) {
fprintf(stderr,"an integer parameter is required\n");
return -1;
}
param = atoi(argv[1]);
if (param < 0) {
fprintf(stderr,"an integer >= 0 is required\n");
return -1;
}
/* create the thread */
threadhandle = createthread(
null, /* default security attributes */
0, /* default stack size */
summation, /* thread function */
&param, /* parameter to thread function */
0, /* default creation flags */
&threadid); /* returns the thread identifier */
if (threadhandle != null) {
/* now wait for the thread to finish */
waitforsingleobject(threadhandle,infinite);
/* close the thread handle */
closehandle(threadhandle);
printf("sum = %d\n",sum);
}
}
figure 4.11
multithreaded c program using the windows api.
176
chapter 4
threads
threads are created in the windows api using the createthread()
function, andjust as in pthreadsa set of attributes for the thread is passed
to this function. these attributes include security information, the size of the
stack, and a ag that can be set to indicate if the thread is to start in a suspended
state. in this program, we use the default values for these attributes. (the
default values do not initially set the thread to a suspended state and instead
make it eligible to be run by the cpu scheduler.) once the summation thread
is created, the parent must wait for it to complete before outputting the value
of sum, as the value is set by the summation thread. recall that the pthread
program (figure 4.9) had the parent thread wait for the summation thread
using the pthread join() statement. we perform the equivalent of this in the
windows api using the waitforsingleobject() function, which causes the
creating thread to block until the summation thread has exited.
in situations that require waiting for multiple threads to complete, the
waitformultipleobjects() function is used. this function is passed four
parameters:
1. the number of objects to wait for
2. a pointer to the array of objects
3. a ag indicating whether all objects have been signaled
4. a timeout duration (or infinite)
for example, if thandles is an array of thread handle objects of size n, the
parent thread can wait for all its child threads to complete with this statement:
waitformultipleobjects(n, thandles, true, infinite);
4.4.3
java threads
threads are the fundamental model of program execution in a java program,
and the java language and its api provide a rich set of features for the creation
and management of threads. all java programs comprise at least a single thread
of controleven a simple java program consisting of only a main() method
runs as a single thread in the jvm. java threads are available on any system that
provides a jvm including windows, linux, and mac os x. the java thread api
is available for android applications as well.
there are two techniques for creating threads in a java program. one
approach is to create a new class that is derived from the thread class and
to override its run() method. an alternativeand more commonly used
technique is to dene a class that implements the runnable interface. the
runnable interface is dened as follows:
public interface runnable
{
public abstract void run();
}
when a class implements runnable, it must dene a run() method. the code
implementing the run() method is what runs as a separate thread.
4.5
implicit threading
177
figure 4.12 shows the java version of a multithreaded program that
determines the summation of a non-negative integer. the summation class
implements the runnable interface. thread creation is performed by creating
an object instance of the thread class and passing the constructor a runnable
object.
creating a thread object does not specically create the new thread; rather,
the start() method creates the new thread. calling the start() method for
the new object does two things:
1. it allocates memory and initializes a new thread in the jvm.
2. it calls the run() method, making the thread eligible to be run by the jvm.
(note again that we never call the run() method directly. rather, we call
the start() method, and it calls the run() method on our behalf.)
when the summation program runs, the jvm creates two threads. the rst
is the parent thread, which starts execution in the main() method. the second
thread is created when the start() method on the thread object is invoked.
this child thread begins execution in the run() method of the summation class.
after outputting the value of the summation, this thread terminates when it
exits from its run() method.
data sharing between threads occurs easily in windows and pthreads, since
shared data are simply declared globally. as a pure object-oriented language,
java has no such notion of global data. if two or more threads are to share
data in a java program, the sharing occurs by passing references to the shared
object to the appropriate threads. in the java program shown in figure 4.12,
the main thread and the summation thread share the object instance of the sum
class. this shared object is referenced through the appropriate getsum() and
setsum() methods. (you might wonder why we dont use an integer object
rather than designing a new sum class. the reason is that the integer class is
immutablethat is, once its value is set, it cannot change.)
recall that the parent threads in the pthreads and windows libraries
use pthread join() and waitforsingleobject() (respectively) to wait
for the summation threads to nish before proceeding. the join() method
in java provides similar functionality. (notice that join() can throw an
interruptedexception, which we choose to ignore.) if the parent must wait
for several threads to nish, the join() method can be enclosed in a for loop
similar to that shown for pthreads in figure 4.10.
4.5
implicit threading
with the continued growth of multicore processing, applications containing
hundredsor even thousandsof threads are looming on the horizon.
designing such applications is not a trivial undertaking: programmers must
addressnotonlythe challengesoutlined insection4.2butadditional difculties
as well. these difculties, which relate to program correctness, are covered in
chapters 5 and 7.
one way to address these difculties and better support the design of
multithreaded applications is to transfer the creation and management of
178
chapter 4
threads
class sum
{
private int sum;
public int getsum() {
return sum;
}
public void setsum(int sum) {
this.sum = sum;
}
}
class summation implements runnable
{
private int upper;
private sum sumvalue;
public summation(int upper, sum sumvalue) {
this.upper = upper;
this.sumvalue = sumvalue;
}
public void run() {
int sum = 0;
for (int i = 0; i <= upper; i++)
sum += i;
sumvalue.setsum(sum);
}
}
public class driver
{
public static void main(string[] args) {
if (args.length > 0) {
if (integer.parseint(args[0]) < 0)
system.err.println(args[0] + " must be >= 0.");
else {
sum sumobject = new sum();
int upper = integer.parseint(args[0]);
thread thrd = new thread(new summation(upper, sumobject));
thrd.start();
try {
thrd.join();
system.out.println
("the sum of "+upper+" is "+sumobject.getsum());
} catch (interruptedexception ie) { }
}
}
else
system.err.println("usage: summation <integer value>"); }
}
figure 4.12
java program for the summation of a non-negative integer.
4.5
implicit threading
179
the jvm and the host operating system
the jvm is typically implemented on top of a host operating system (see
figure 16.10). this setup allows the jvm to hide the implementation details
of the underlying operating system and to provide a consistent, abstract
environment that allows java programs to operate on any platform that
supports a jvm. the specication for the jvm does not indicate how java
threads are to be mapped to the underlying operating system, instead leaving
that decision to the particular implementation of the jvm. for example, the
windows xp operating system uses the one-to-one model; therefore, each
java thread for a jvm running on such a system maps to a kernel thread. on
operating systems that use the many-to-many model (such as tru64 unix), a
java thread is mapped according to the many-to-many model. solaris initially
implemented the jvmusing the many-to-one model (the green threads library,
mentioned earlier). later releases of the jvm were implemented using the
many-to-many model. beginning with solaris 9, java threads were mapped
using the one-to-one model. in addition, there may be a relationship between
the java thread library and the thread library on the host operating system.
for example, implementations of a jvm for the windows family of operating
systems might use the windows api when creating java threads; linux,
solaris, and mac os x systems might use the pthreads api.
threading from application developers to compilers and run-time libraries.
this strategy, termed implicit threading, is a popular trend today. in this
section, we explore three alternative approaches for designing multithreaded
programs that can take advantage of multicore processors through implicit
threading.
4.5.1
thread pools
in section 4.1, we described a multithreaded web server. in this situation,
whenever the server receives a request, it creates a separate thread to service
the request. whereas creating a separate thread is certainly superior to creating
a separate process, a multithreaded server nonetheless has potential problems.
the rst issue concerns the amount of time required to create the thread,
together with the fact that the thread will be discarded once it has completed
its work. the second issue is more troublesome. if we allow all concurrent
requests to be serviced in a new thread, we have not placed a bound on the
number of threads concurrently active in the system. unlimited threads could
exhaust system resources, such as cpu time or memory. one solution to this
problem is to use a thread pool.
the general idea behind a thread pool is to create a number of threads at
process startup and place them into a pool, where they sit and wait for work.
when a server receives a request, it awakens a thread from this poolif one
is availableand passes it the request for service. once the thread completes
its service, it returns to the pool and awaits more work. if the pool contains no
available thread, the server waits until one becomes free.
180
chapter 4
threads
thread pools offer these benets:
1. servicing a request with an existing thread is faster than waiting to create
a thread.
2. a thread pool limits the number of threads that exist at any one point.
this is particularly important on systems that cannot support a large
number of concurrent threads.
3. separating the task to be performed from the mechanics of creating the
task allows us to use different strategies for running the task. for example,
the task could be scheduled to execute after a time delay or to execute
periodically.
the number of threads in the pool can be set heuristically based on factors
such as the number of cpus in the system, the amount of physical memory,
and the expected number of concurrent client requests. more sophisticated
thread-pool architectures can dynamically adjust the number of threads in the
pool according to usage patterns. such architectures provide the further benet
of having a smaller poolthereby consuming less memorywhen the load
on the system is low. we discuss one such architecture, apples grand central
dispatch, later in this section.
the windows api provides several functions related to thread pools. using
the thread pool api is similar to creating a thread with the thread create()
function, as described in section 4.4.2. here, a function that is to run as a
separate thread is dened. such a function may appear as follows:
dword winapi poolfunction(avoid param) {
/*
* this function runs as a separate thread.
*/
}
a pointer to poolfunction() is passed to one of the functions in the thread
pool api, and a thread from the pool executes this function. one such member
in the thread pool api is the queueuserworkitem() function, which is passed
three parameters:
lpthread start routine functiona pointer to the function that is to
run as a separate thread
pvoid paramthe parameter passed to function
ulong flagsags indicating how the thread pool is to create and
manage execution of the thread
an example of invoking a function is the following:
queueuserworkitem(&poolfunction, null, 0);
this causes a thread from the thread pool to invoke poolfunction() on behalf
of the programmer. in this instance, we pass no parameters to poolfunc-
4.5
implicit threading
181
tion(). because we specify 0 as a ag, we provide the thread pool with no
special instructions for thread creation.
other members in the windows thread pool api include utilities that invoke
functions at periodic intervals or when an asynchronous i/o request completes.
the java.util.concurrent package in the java api provides a thread-pool
utility as well.
4.5.2
openmp
openmp is a set of compiler directives as well as an api for programs written
in c, c++, or fortran that provides support for parallel programming in
shared-memory environments. openmp identies parallel regions as blocks
of code that may run in parallel. application developers insert compiler
directives into their code at parallel regions, and these directives instruct the
openmp run-time library to execute the region in parallel. the following c
program illustrates a compiler directive above the parallel region containing
the printf() statement:
#include <omp.h>
#include <stdio.h>
int main(int argc, char *argv[])
{
/* sequential code */
#pragma omp parallel
{
printf("i am a parallel region.");
}
/* sequential code */
return 0;
}
when openmp encounters the directive
#pragma omp parallel
it creates as many threads are there are processing cores in the system. thus, for
a dual-core system, two threads are created, for a quad-core system, four are
created; and so forth. all the threads then simultaneously execute the parallel
region. as each thread exits the parallel region, it is terminated.
openmp provides several additional directives for running code regions
in parallel, including parallelizing loops. for example, assume we have two
arrays a and b of size n. we wish to sum their contents and place the results
in array c. we can have this task run in parallel by using the following code
segment, which contains the compiler directive for parallelizing for loops:
182
chapter 4
threads
#pragma omp parallel for
for (i = 0; i < n; i++) {
c[i] = a[i] + b[i];
}
openmp divides the work contained in the for loop among the threads it has
created in response to the directive
#pragma omp parallel for
in addition to providing directives for parallelization, openmp allows devel-
opers to choose among several levels of parallelism. for example, they can set
the number of threads manually. it also allows developers to identify whether
data are shared between threads or are private to a thread. openmp is available
on several open-source and commercial compilers for linux, windows, and
mac os x systems. we encourage readers interested in learning more about
openmp to consult the bibliography at the end of the chapter.
4.5.3
grand central dispatch
grand central dispatch (gcd)a technology for apples mac os x and ios
operating systemsis a combination of extensions to the c language, an api,
and a run-time library that allows application developers to identify sections
of code to run in parallel. like openmp, gcd manages most of the details of
threading.
gcd identies extensions to the c and c++ languages known as blocks. a
block is simply a self-contained unit of work. it is specied by a caret inserted
in front of a pair of braces { }. a simple example of a block is shown below:
{ printf("i am a block"); }
gcd schedules blocks for run-time execution by placing them on a dispatch
queue. when it removes a block from a queue, it assigns the block to an
available thread from the thread pool it manages. gcd identies two types of
dispatch queues: serial and concurrent.
blocks placed on a serial queue are removed in fifo order. once a block has
been removed from the queue, it must complete execution before another block
is removed. each process has its own serial queue (known as its main queue).
developers can create additional serial queues that are local to particular
processes. serial queues are useful for ensuring the sequential execution of
several tasks.
blocks placed on a concurrent queue are also removed in fifo order, but
several blocks may be removed at a time, thus allowing multiple blocks to
execute in parallel. there are three system-wide concurrent dispatch queues,
and they are distinguished according to priority: low, default, and high.
priorities represent an approximation of the relative importance of blocks.
quite simply, blocks with a higher priority should be placed on the high-
priority dispatch queue.
the following code segment illustrates obtaining the default-priority
concurrent
queue
and
submitting
a
block
to
the
queue
using
the
dispatch async() function:
4.6
threading issues
183
dispatch queue t queue = dispatch get global queue
(dispatch queue priority default, 0);
dispatch async(queue, { printf("i am a block."); });
internally, gcds thread pool is composed of posix threads. gcd actively
manages the pool, allowing the number of threads to grow and shrink
according to application demand and system capacity.
4.5.4
other approaches
thread pools, openmp, and grand central dispatch are just a few of many
emerging technologies for managing multithreaded applications. other com-
mercial approaches include parallel and concurrent libraries, such as intels
threading building blocks (tbb) and several products from microsoft. the java
language and api have seen signicant movement toward supporting concur-
rent programming as well. a notable example is the java.util.concurrent
package, which supports implicit thread creation and management.
4.6
threading issues
in this section, we discuss some of the issues to consider in designing
multithreaded programs.
4.6.1
the fork() and exec() system calls
in chapter 3, we described how the fork() system call is used to create a
separate, duplicate process. the semantics of the fork() and exec() system
calls change in a multithreaded program.
if one thread in a program calls fork(), does the new process duplicate
all threads, or is the new process single-threaded? some unix systems have
chosen to have two versions of fork(), one that duplicates all threads and
another that duplicates only the thread that invoked the fork() system call.
the exec() system call typically works in the same way as described
in chapter 3. that is, if a thread invokes the exec() system call, the program
specied in the parameter to exec() will replace the entire processincluding
all threads.
which of the two versions of fork() to use depends on the application.
if exec() is called immediately after forking, then duplicating all threads is
unnecessary, as the program specied in the parameters to exec() will replace
the process. in this instance, duplicating only the calling thread is appropriate.
if, however, the separate process does not call exec() after forking, the separate
process should duplicate all threads.
4.6.2
signal handling
a signal is used in unix systems to notify a process that a particular event has
occurred. a signal may be received either synchronously or asynchronously,
184
chapter 4
threads
depending on the source of and the reason for the event being signaled. all
signals, whether synchronous or asynchronous, follow the same pattern:
1. a signal is generated by the occurrence of a particular event.
2. the signal is delivered to a process.
3. once delivered, the signal must be handled.
examples of synchronous signal include illegal memory access and divi-
sion by 0. if a running program performs either of these actions, a signal
is generated. synchronous signals are delivered to the same process that
performed the operation that caused the signal (that is the reason they are
considered synchronous).
when a signal is generated by an event external to a running process, that
process receives the signal asynchronously. examples of such signals include
terminating a process with specic keystrokes (such as <control><c>) and
having a timer expire. typically, an asynchronous signal is sent to another
process.
a signal may be handled by one of two possible handlers:
1. a default signal handler
2. a user-dened signal handler
every signal has a default signal handler that the kernel runs when
handling that signal. this default action can be overridden by a user-dened
signal handler that is called to handle the signal. signals are handled in
different ways. some signals (such as changing the size of a window) are
simply ignored; others (such as an illegal memory access) are handled by
terminating the program.
handling signals in single-threaded programs is straightforward: signals
are always delivered to a process. however, delivering signals is more
complicated in multithreaded programs, where a process may have several
threads. where, then, should a signal be delivered?
in general, the following options exist:
1. deliver the signal to the thread to which the signal applies.
2. deliver the signal to every thread in the process.
3. deliver the signal to certain threads in the process.
4. assign a specic thread to receive all signals for the process.
the method for delivering a signal depends on the type of signal generated.
for example, synchronous signals need to be delivered to the thread causing
the signal and not to other threads in the process. however, the situation with
asynchronous signals is not as clear. some asynchronous signalssuch as a
signal that terminates a process (<control><c>, for example)should be
sent to all threads.
4.6
threading issues
185
the standard unix function for delivering a signal is
kill(pid t pid, int signal)
this function species the process (pid) to which a particular signal (signal) is
to be delivered. most multithreaded versions of unix allow a thread to specify
which signals it will accept and which it will block. therefore, in some cases,
an asynchronous signal may be delivered only to those threads that are not
blocking it. however, because signals need to be handled only once, a signal is
typically delivered only to the rst thread found that is not blocking it. posix
pthreads provides the following function, which allows a signal to be delivered
to a specied thread (tid):
pthread kill(pthread t tid, int signal)
although windows does not explicitly provide support for signals, it
allows us to emulate them using asynchronous procedure calls (apcs). the
apc facility enables a user thread to specify a function that is to be called
when the user thread receives notication of a particular event. as indicated
by its name, an apc is roughly equivalent to an asynchronous signal in unix.
however, whereas unix must contend with how to deal with signals in a
multithreaded environment, the apc facility is more straightforward, since an
apc is delivered to a particular thread rather than a process.
4.6.3
thread cancellation
thread cancellation involves terminating a thread before it has completed. for
example, if multiple threads are concurrently searching through a database and
one thread returns the result, the remaining threads might be canceled. another
situation might occur when a user presses a button on a web browser that stops
a web page from loading any further. often, a web page loads using several
threadseach image is loaded in a separate thread. when a user presses the
stop button on the browser, all threads loading the page are canceled.
a thread that is to be canceled is often referred to as the target thread.
cancellation of a target thread may occur in two different scenarios:
1. asynchronous cancellation. one thread immediately terminates the
target thread.
2. deferred cancellation. the target thread periodically checks whether it
should terminate, allowing it an opportunity to terminate itself in an
orderly fashion.
the difculty with cancellation occurs in situations where resources have
been allocated to a canceled thread or where a thread is canceled while in
the midst of updating data it is sharing with other threads. this becomes
especially troublesome with asynchronous cancellation. often, the operating
system will reclaim system resources from a canceled thread but will not
reclaim all resources. therefore, canceling a thread asynchronously may not
free a necessary system-wide resource.
186
chapter 4
threads
with deferred cancellation, in contrast, one thread indicates that a target
thread is to be canceled, but cancellation occurs only after the target thread has
checked a ag to determine whether or not it should be canceled. the thread
can perform this check at a point at which it can be canceled safely.
in pthreads, thread cancellation is initiated using the pthread cancel()
function. the identier of the target thread is passed as a parameter to
the function. the following code illustrates creatingand then canceling
a thread:
pthread t tid;
/* create the thread */
pthread create(&tid, 0, worker, null);
. . .
/* cancel the thread */
pthread cancel(tid);
invoking pthread cancel()indicates only a request to cancel the target
thread, however; actual cancellation depends on how the target thread is set
up to handle the request. pthreads supports three cancellation modes. each
mode is dened as a state and a type, as illustrated in the table below. a thread
may set its cancellation state and type using an api.
mode
state
type
off
disabled
deferred
enabled
deferred
asynchronous
enabled
asynchronous
as the table illustrates, pthreads allows threads to disable or enable
cancellation. obviously, a thread cannot be canceled if cancellation is disabled.
however, cancellation requests remain pending, so the thread can later enable
cancellation and respond to the request.
the default cancellation type is deferred cancellation. here, cancellation
occurs only when a thread reaches a cancellation point. one technique for
establishing a cancellation point is to invoke the pthread testcancel()
function. if a cancellation request is found to be pending, a function known
as a cleanup handler is invoked. this function allows any resources a thread
may have acquired to be released before the thread is terminated.
the following code illustrates how a thread may respond to a cancellation
request using deferred cancellation:
while (1) {
/* do some work for awhile */
/* . . . */
/* check if there is a cancellation request */
pthread testcancel();
}
4.6
threading issues
187
because of the issues described earlier, asynchronous cancellation is not
recommended in pthreads documentation. thus, we do not cover it here. an
interesting note is that on linux systems, thread cancellation using the pthreads
api is handled through signals (section 4.6.2).
4.6.4
thread-local storage
threads belonging to a process share the data of the process. indeed, this
data sharing provides one of the benets of multithreaded programming.
however, in some circumstances, each thread might need its own copy of
certain data. we will call such data thread-local storage (or tls.) for example,
in a transaction-processing system, we might service each transaction in a
separate thread. furthermore, each transaction might be assigned a unique
identier. to associate each thread with its unique identier, we could use
thread-local storage.
it is easy to confuse tls with local variables. however, local variables
are visible only during a single function invocation, whereas tls data are
visible across function invocations. in some ways, tls is similar to static
data. the difference is that tls data are unique to each thread. most thread
librariesincluding windows and pthreadsprovide some form of support
for thread-local storage; java provides support as well.
4.6.5
scheduler activations
a nal issue to be considered with multithreaded programs concerns com-
munication between the kernel and the thread library, which may be required
by the many-to-many and two-level models discussed in section 4.3.3. such
coordination allows the number of kernel threads to be dynamically adjusted
to help ensure the best performance.
many systems implementing either the many-to-many or the two-level
model place an intermediate data structure between the user and kernel
threads. this data structuretypically known as a lightweight process, or
lwpis shown in figure 4.13. to the user-thread library, the lwp appears to
be a virtual processor on which the application can schedule a user thread to
run. each lwp is attached to a kernel thread, and it is kernel threads that the
lwp
user thread
kernel thread
k
lightweight process
figure 4.13
lightweight process (lwp).
188
chapter 4
threads
operating system schedules to run on physical processors. if a kernel thread
blocks (such as while waiting for an i/o operation to complete), the lwp blocks
as well. up the chain, the user-level thread attached to the lwp also blocks.
an application may require any number of lwps to run efciently. consider
a cpu-bound application running on a single processor. in this scenario, only
one thread can run at at a time, so one lwp is sufcient. an application that is
i/o-intensive may require multiple lwps to execute, however. typically, an lwp
is required for each concurrent blocking system call. suppose, for example, that
ve different le-read requests occur simultaneously. five lwps are needed,
because all could be waiting for i/o completion in the kernel. if a process has
only four lwps, then the fth request must wait for one of the lwps to return
from the kernel.
one scheme for communication between the user-thread library and the
kernel is known as scheduler activation. it works as follows: the kernel
provides an application with a set of virtual processors (lwps), and the
application can schedule user threads onto an available virtual processor.
furthermore, the kernel must inform an application about certain events. this
procedure is known as an upcall. upcalls are handled by the thread library
with an upcall handler, and upcall handlers must run on a virtual processor.
one event that triggers an upcall occurs when an application thread is about to
block. in this scenario, the kernel makes an upcall to the application informing
it that a thread is about to block and identifying the specic thread. the kernel
then allocates a new virtual processor to the application. the application runs
an upcall handler on this new virtual processor, which saves the state of the
blocking thread and relinquishes the virtual processor on which the blocking
thread is running. the upcall handler then schedules another thread that is
eligible to run on the new virtual processor. when the event that the blocking
thread was waiting for occurs, the kernel makes another upcall to the thread
library informing it that the previously blocked thread is now eligible to run.
the upcall handler for this event also requires a virtual processor, and the kernel
may allocate a new virtual processor or preempt one of the user threads and
run the upcall handler on its virtual processor. after marking the unblocked
thread as eligible to run, the application schedules an eligible thread to run on
an available virtual processor.
4.7
operating-system examples
at this point, we have examined a number of concepts and issues related to
threads. we conclude the chapter by exploring how threads are implemented
in windows and linux systems.
4.7.1
windows threads
windows implements the windows api, which is the primary api for the
family of microsoft operating systems (windows 98, nt, 2000, and xp, as well
as windows 7). indeed, much of what is mentioned in this section applies to
this entire family of operating systems.
a windows application runs as a separate process, and each process may
contain one or more threads. the windows api for creating threads is covered in
4.7
operating-system examples
189
section 4.4.2. additionally, windows uses the one-to-one mapping described
in section 4.3.2, where each user-level thread maps to an associated kernel
thread.
the general components of a thread include:
a thread id uniquely identifying the thread
a register set representing the status of the processor
a user stack, employed when the thread is running in user mode, and a
kernel stack, employed when the thread is running in kernel mode
a private storage area used by various run-time libraries and dynamic link
libraries (dlls)
the register set, stacks, and private storage area are known as the context of
the thread.
the primary data structures of a thread include:
ethreadexecutive thread block
kthreadkernel thread block
tebthread environment block
the key components of the ethread include a pointer to the process
to which the thread belongs and the address of the routine in which the
thread starts control. the ethread also contains a pointer to the corresponding
kthread.
the kthread includes scheduling and synchronization information for
the thread. in addition, the kthread includes the kernel stack (used when the
thread is running in kernel mode) and a pointer to the teb.
the ethread and the kthread exist entirely in kernel space; this means
that only the kernel can access them. the teb is a user-space data structure
that is accessed when the thread is running in user mode. among other elds,
the teb contains the thread identier, a user-mode stack, and an array for
thread-local storage. the structure of a windows thread is illustrated in figure
4.14.
4.7.2
linux threads
linux provides the fork() system call with the traditional functionality of
duplicating a process, as described in chapter 3. linux also provides the ability
to create threads using the clone() system call. however, linux does not
distinguish between processes and threads. in fact, linux uses the term task
rather than process or thread when referring to a ow of control within a
program.
when clone() is invoked, it is passed a set of ags that determine how
much sharing is to take place between the parent and child tasks. some of these
ags are listed in figure 4.15. for example, suppose that clone() is passed
the ags clone fs, clone vm, clone sighand, and clone files. the parent
and child tasks will then share the same le-system information (such as the
current working directory), the same memory space, the same signal handlers,
190
chapter 4
threads
user space
kernel space
pointer to
parent process
thread start
address
ethread
kthread
kernel
stack
scheduling
and
synchronization
information
user
stack
thread-local
storage
thread identifier
teb
figure 4.14
data structures of a windows thread.
and the same set of open les. using clone() in this fashion is equivalent to
creating a thread as described in this chapter, since the parent task shares most
of its resources with its child task. however, if none of these ags is set when
clone() is invoked, no sharing takes place, resulting in functionality similar
to that provided by the fork() system call.
the varying level of sharing is possible because of the way a task is
represented in the linux kernel. a unique kernel data structure (specically,
struct task struct) exists for each task in the system. this data structure,
instead of storing data for the task, contains pointers to other data structures
where these data are storedfor example, data structures that represent the list
of open les, signal-handling information, and virtual memory. when fork()
is invoked, a new task is created, along with a copy of all the associated data
flag
meaning
clone_fs
clone_vm
clone_sighand
clone_files
file-system information is shared.
the same memory space is shared.
signal handlers are shared.
the set of open files is shared.
figure 4.15
some of the ags passed when clone() is invoked.
practice exercises
191
structures of the parent process. a new task is also created when the clone()
system call is made. however, rather than copying all data structures, the new
task points to the data structures of the parent task, depending on the set of
ags passed to clone().
4.8
summary
a thread is a ow of control within a process. a multithreaded process contains
several different ows of control within the same address space. the benets of
multithreading include increased responsiveness to the user, resource sharing
within the process, economy, and scalability factors, such as more efcient use
of multiple processing cores.
user-level threads are threads that are visible to the programmer and are
unknown to the kernel. the operating-system kernel supports and manages
kernel-level threads. in general, user-level threads are faster to create and
manage than are kernel threads, because no intervention from the kernel is
required.
three different types of models relate user and kernel threads. the many-
to-one model maps many user threads to a single kernel thread. the one-to-one
model maps each user thread to a corresponding kernel thread. the many-to-
many model multiplexes many user threads to a smaller or equal number of
kernel threads.
most modern operating systems provide kernel support for threads. these
include windows, mac os x, linux, and solaris.
thread libraries provide the application programmer with an api for
creating and managing threads. three primary thread libraries are in common
use: posix pthreads, windows threads, and java threads.
in addition to explicitly creating threads using the api provided by a
library, we can use implicit threading, in which the creation and management
of threading is transferred to compilers and run-time libraries. strategies for
implicit threading include thread pools, openmp, and grand central dispatch.
multithreaded programs introduce many challenges for programmers,
including the semantics of the fork() and exec() system calls. other
issues include signal handling, thread cancellation, thread-local storage, and
scheduler activations.
practice exercises
4.1
provide two programming examples in which multithreading provides
better performance than a single-threaded solution.
4.2
what are two differences between user-level threads and kernel-level
threads? under what circumstances is one type better than the other?
4.3
describe the actions taken by a kernel to context-switch between kernel-
level threads.
4.4
what resources are used when a thread is created? how do they differ
from those used when a process is created?
192
chapter 4
threads
4.5
assume that an operating system maps user-level threads to the kernel
using the many-to-many model and that the mapping is done through
lwps. furthermore, the system allows developers to create real-time
threads for use in real-time systems. is it necessary to bind a real-time
thread to an lwp? explain.
exercises
4.6
provide two programming examples in which multithreading does not
provide better performance than a single-threaded solution.
4.7
under what circumstances does a multithreaded solution using multi-
ple kernel threads provide better performance than a single-threaded
solution on a single-processor system?
4.8
which of the following components of program state are shared across
threads in a multithreaded process?
a.
register values
b.
heap memory
c.
global variables
d.
stack memory
4.9
can a multithreaded solution using multiple user-level threads achieve
better performance on a multiprocessor system than on a single-
processor system? explain.
4.10
in chapter 3, we discussed googles chrome browser and its practice
of opening each new website in a separate process. would the same
benets have been achieved if instead chrome had been designed to
open each new website in a separate thread? explain.
4.11
is it possible to have concurrency but not parallelism? explain.
4.12
using amdahls law, calculate the speedup gain of an application that
has a 60 percent parallel component for (a) two processing cores and (b)
four processing cores.
4.13
determine if the following problems exhibit task or data parallelism:
the multithreaded statistical program described in exercise 4.21
the multithreaded sudoku validator described in project 1 in this
chapter
the multithreaded sorting program described in project 2 in this
chapter
the multithreaded web server described in section 4.1
4.14
a system with two dual-core processors has four processors available
for scheduling. a cpu-intensive application is running on this system.
all input is performed at program start-up, when a single le must
be opened. similarly, all output is performed just before the program
exercises
193
terminates, when the program results must be written to a single
le. between startup and termination, the program is entirely cpu-
bound. your task is to improve the performance of this application
by multithreading it. the application runs on a system that uses the
one-to-one threading model (each user thread maps to a kernel thread).
how many threads will you create to perform the input and output?
explain.
how many threads will you create for the cpu-intensive portion of
the application? explain.
4.15
consider the following code segment:
pid t pid;
pid = fork();
if (pid == 0) { /* child process */
fork();
thread create( . . .);
}
fork();
a.
how many unique processes are created?
b.
how many unique threads are created?
4.16
as described in section 4.7.2, linux does not distinguish between
processes and threads. instead, linux treats both in the same way,
allowing a task to be more akin to a process or a thread depending on the
set of ags passed to the clone() system call. however, other operating
systems, such as windows, treat processes and threads differently.
typically, such systems use a notation in which the data structure for
a process contains pointers to the separate threads belonging to the
process. contrast these two approaches for modeling processes and
threads within the kernel.
4.17
the program shown in figure 4.16 uses the pthreads api. what would
be the output from the program at line c and line p?
4.18
consider a multicore system and a multithreaded program written
using the many-to-many threading model. let the number of user-level
threads in the program be greater than the number of processing cores
in the system. discuss the performance implications of the following
scenarios.
a.
the number of kernel threads allocated to the program is less than
the number of processing cores.
b.
the number of kernel threads allocated to the program is equal to
the number of processing cores.
c.
the number of kernel threads allocated to the program is greater
than the number of processing cores but less than the number of
user-level threads.
194
chapter 4
threads
#include <pthread.h>
#include <stdio.h>
#include <types.h>
int value = 0;
void *runner(void *param); /* the thread */
int main(int argc, char *argv[])
{
pid t pid;
pthread t tid;
pthread attr t attr;
pid = fork();
if (pid == 0) { /* child process */
pthread attr init(&attr);
pthread create(&tid,&attr,runner,null);
pthread join(tid,null);
printf("child: value = %d",value); /* line c */
}
else if (pid > 0) { /* parent process */
wait(null);
printf("parent: value = %d",value); /* line p */
}
}
void *runner(void *param) {
value = 5;
pthread exit(0);
}
figure 4.16
c program for exercise 4.17.
4.19
pthreads provides an api for managing thread cancellation. the
pthread setcancelstate() function is used to set the cancellation
state. its prototype appears as follows:
pthread setcancelstate(int state, int *oldstate)
the two possible values for the state are pthread cancel enable and
pthread cancel disable.
using the code segment shown in figure 4.17, provide examples of
two operations that would be suitable to perform between the calls to
disable and enable thread cancellation.
programming problems
195
int oldstate;
pthread setcancelstate(pthread cancel disable, &oldstate);
/* what operations would be performed here? */
pthread setcancelstate(pthread cancel enable, &oldstate);
figure 4.17
c program for exercise 4.19.
programming problems
4.20
modify programming problem exercise 3.20 from chapter 3, which asks
you to design a pid manager. this modication will consist of writing
a multithreaded program that tests your solution to exercise 3.20. you
will create a number of threadsfor example, 100and each thread will
request a pid, sleep for a random period of time, and then release the pid.
(sleeping for a random period of time approximates the typical pid usage
in which a pid is assigned to a new process, the process executes and
then terminates, and the pid is released on the processs termination.) on
unix and linux systems, sleeping is accomplished through the sleep()
function, which is passed an integer value representing the number of
seconds to sleep. this problem will be modied in chapter 5.
4.21
write a multithreaded program that calculates various statistical values
for a list of numbers. this program will be passed a series of numbers on
the command line and will then create three separate worker threads.
one thread will determine the average of the numbers, the second
will determine the maximum value, and the third will determine the
minimum value. for example, suppose your program is passed the
integers
90 81 78 95 79 72 85
the program will report
the average value is 82
the minimum value is 72
the maximum value is 95
the variables representing the average, minimum, and maximum values
will be stored globally. the worker threads will set these values, and the
parent thread will output the values once the workers have exited. (we
could obviously expand this program by creating additional threads
that determine other statistical values, such as median and standard
deviation.)
4.22
an interesting way of calculating  is to use a technique known as monte
carlo, which involves randomization. this technique works as follows:
suppose you have a circle inscribed within a square, as shown in figure
196
chapter 4
threads
(1, 1)
(1, 1)
(1, 1)
(1, 1)
(0, 0)
figure 4.18
monte carlo technique for calculating pi.
4.18. (assume that the radius of this circle is 1.) first, generate a series of
random points as simple (x, y) coordinates. these points must fall within
the cartesian coordinates that bound the square. of the total number of
random points that are generated, some will occur within the circle.
next, estimate  by performing the following calculation:
 = 4 (number of points in circle) / (total number of points)
write a multithreaded version of this algorithm that creates a separate
thread to generate a number of random points. the thread will count
the number of points that occur within the circle and store that result
in a global variable. when this thread has exited, the parent thread will
calculate and output the estimated value of . it is worth experimenting
with the number of random points generated. as a general rule, the
greater the number of points, the closer the approximation to .
in the source-code download for this text, we provide a sample program
that provides a technique for generating random numbers, as well as
determining if the random (x, y) point occurs within the circle.
readers interested in the details of the monte carlo method for esti-
mating  should consult the bibliography at the end of this chapter. in
chapter 5, we modify this exercise using relevant material from that
chapter.
4.23
repeat exercise 4.22, but instead of using a separate thread to generate
random points, use openmp to parallelize the generation of points. be
careful not to place the calculcation of  in the parallel region, since you
want to calculcate  only once.
4.24
write a multithreaded program that outputs prime numbers. this
program should work as follows: the user will run the program and
will enter a number on the command line. the program will then create
a separate thread that outputs all the prime numbers less than or equal
to the number entered by the user.
4.25
modify the socket-based date server (figure 3.21) in chapter 3 so that
the server services each client request in a separate thread.
programming projects
197
4.26
the fibonacci sequence is the series of numbers 0, 1, 1, 2, 3, 5, 8, ....
formally, it can be expressed as:
f ib0 = 0
f ib1 = 1
f ibn = f ibn1 + f ibn2
write a multithreaded program that generates the fibonacci sequence.
this program should work as follows: on the command line, the user
will enter the number of fibonacci numbers that the program is to
generate. the program will then create a separate thread that will
generate the fibonacci numbers, placing the sequence in data that can
be shared by the threads (an array is probably the most convenient
data structure). when the thread nishes execution, the parent thread
will output the sequence generated by the child thread. because the
parent thread cannot begin outputting the fibonacci sequence until the
child thread nishes, the parent thread will have to wait for the child
thread to nish. use the techniques described in section 4.4 to meet this
requirement.
4.27
exercise 3.25 in chapter 3 involves designing an echo server using the
java threading api. this server is single-threaded, meaning that the
server cannot respond to concurrent echo clients until the current client
exits. modify the solution to exercise 3.25 so that the echo server services
each client in a separate request.
programming projects
project 1sudoku solution validator
a sudoku puzzle uses a 9 9 grid in which each column and row, as well as
each of the nine 3 3 subgrids, must contain all of the digits 1	9. figure
4.19 presents an example of a valid sudoku puzzle. this project consists of
designing a multithreaded application that determines whether the solution to
a sudoku puzzle is valid.
there are several different ways of multithreading this application. one
suggested strategy is to create threads that check the following criteria:
a thread to check that each column contains the digits 1 through 9
a thread to check that each row contains the digits 1 through 9
nine threads to check that each of the 3 3 subgrids contains the digits 1
through 9
this would result in a total of eleven separate threads for validating a
sudoku puzzle. however, you are welcome to create even more threads for
this project. for example, rather than creating one thread that checks all nine
chapter 4
threads
figure 4.19
solution to a 9 9 sudoku puzzle.
columns, you could create nine separate threads and have each of them check
one column.
passing parameters to each thread
the parent thread will create the worker threads, passing each worker the
location that it must check in the sudoku grid. this step will require passing
several parameters to each thread. the easiest approach is to create a data
structure using a struct. for example, a structure to pass the row and column
where a thread must begin validating would appear as follows:
/* structure for passing data to threads */
typedef struct
{
int row;
int column;
} parameters;
both pthreads and windows programs will create worker threads using a
strategy similar to that shown below:
parameters *data = (parameters *) malloc(sizeof(parameters));
data->row = 1;
data->column = 1;
/* now create the thread passing it data as a parameter */
the data pointer will be passed to either the pthread create() (pthreads)
function or the createthread() (windows) function, which in turn will pass
it as a parameter to the function that is to run as a separate thread.
returning results to the parent thread
each worker thread is assigned the task of determining the validity of a
particular region of the sudoku puzzle. once a worker has performed this
bibliographical notes
199
7, 12, 19, 3, 18
7, 12, 19, 3, 18, 4, 2, 6, 15, 8
original list
2, 3, 4, 6, 7, 8, 12, 15, 18, 19
merge thread
sorted list
sorting
thread0
sorting
thread1
4, 2, 6, 15, 8
figure 4.20
multithreaded sorting.
check, it must pass its results back to the parent. one good way to handle this
is to create an array of integer values that is visible to each thread. the ith
index in this array corresponds to the ith worker thread. if a worker sets its
corresponding value to 1, it is indicating that its region of the sudoku puzzle
is valid. a value of 0 would indicate otherwise. when all worker threads have
completed, the parent thread checks each entry in the result array to determine
if the sudoku puzzle is valid.
project 2multithreaded sorting application
write a multithreaded sorting program that works as follows: a list of integers
is divided into two smaller lists of equal size. two separate threads (which we
will term sorting threads) sort each sublist using a sorting algorithm of your
choice. the two sublists are then merged by a third threada merging thread
which merges the two sublists into a single sorted list.
because global data are shared cross all threads, perhaps the easiest way
to set up the data is to create a global array. each sorting thread will work on
one half of this array. a second global array of the same size as the unsorted
integer array will also be established. the merging thread will then merge
the two sublists into this second array. graphically, this program is structured
according to figure 4.20.
this programming project will require passing parameters to each of the
sorting threads. in particular, it will be necessary to identify the starting index
from which each thread is to begin sorting. refer to the instructions in project
1 for details on passing parameters to a thread.
the parent thread will output the sorted array once all sorting threads have
exited.
bibliographical notes
threads have had a long evolution, starting as cheap concurrency in
programming languages and moving to lightweight processes, with early
examples that included the thoth system ([cheriton et al. (1979)]) and the pilot