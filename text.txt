An operating system is a program that manages a computer’s hardware. It also provides a basis for application programs and acts as an intermediary between the computer user and the computer hardware. An amazing aspect of operating systems is how they vary in accomplishing these tasks. Mainframe operating systems are designed primarily to optimize utilization of hardware. Personal computer (PC) operating systems support complex games, business applications, and everything in between. Operating systems for mobile computers provide an environment in which a user can easily interface with the computer to execute programs. Thus, some operating systems are designed to be convenient, others to be efficient, and others to be some combination of the two. Before we can explore the details of computer system operation, we need to know something about system structure. We thus discuss the basic functions of system startup, I/O, and storage early in this chapter. We also describe the basic computer architecture that makes it possible to write a functional operating system. Because an operating system is large and complex, it must be created piece by piece. Each of these pieces should be a well-delineated portion of the system, with carefully de ned inputs, outputs, and functions. In this chapter, we provide a general overview of the major components of a contemporary computer system as well as the functions provided by the operating system. Additionally, we cover several other topics to help set the stage for the remainder of this text: data structures used in operating systems, computing environments, and open-source operating systems. CHAPTER OBJECTIVES To describe the basic organization of computer systems. To provide a grand tour of the major components of operating systems. To give an overview of the many types of computing environments. To explore several open-source operating systems. 1.1 What Operating Systems Do We begin our discussion by looking at the operating system’s role in the overall computer system. A computer system can be divided roughly into four components: the hardware, the operating system, the application programs, and the users. The hardware—the central processing unit (CPU), the memory, and the input/output (I/O) devices—provides the basic computing resources for the system. The application programs—such as word processors, spreadsheets, compilers, and Web browsers—de ne the ways in which these resources are used to solve users’ computing problems. The operating system controls the hardware and coordinates its use among the various application programs for the various users. We can also view a computer system as consisting of hardware, software, and data. The operating system provides the means for proper use of these resources in the operation of the computer system. An operating system is similar to a government. Like a government, it performs no useful function by itself. It simply provides an environment within which other programs can do useful work. To understand more fully the operating system’s role, we next explore operating systems from two viewpoints: that of the user and that of the system. 1.1.1 User View The user’s view of the computer varies according to the interface being used. Most computer users sit in front of a PC, consisting of a monitor, keyboard, mouse, and system unit. Such a system is designed for one user to monopolize its resources. The goal is to maximize the work (or play) that the user is performing. In this case, the operating system is designed mostly for ease of use, with some attention paid to performance and none paid to resource utilization—how various hardware and software resources are shared. Performance is, of course, important to the user; but such systems are optimized for the single-user experience rather than the requirements of multiple users. In other cases, a user sits at a terminal connected to a mainframe or a minicomputer. Other users are accessing the same computer through other terminals. These users share resources and may exchange information. The operating system in such cases is designed to maximize resource utilization— to assure that all available CPU time, memory, and I/O are used ef ciently and that no individual user takes more than her fair share. In still other cases, users sit at workstations connected to networks of other workstations and servers. These users have dedicated resources at their disposal, but they also share resources such as networking and servers, including  le, compute, and print servers. Therefore, their operating system is designed to compromise between individual usability and resource utilization. Recently, many varieties of mobile computers, such as smartphones and tablets, have come into fashion. Most mobile computers are standalone units for individual users. Quite often, they are connected to networks through cellular or other wireless technologies. Increasingly, these mobile devices are replacing desktop and laptop computers for people who are primarily interested in using computers for e-mail and web browsing. The user interface for mobile computers generally features a touch screen, where the user interacts with the system by pressing and swiping  ngers across the screen rather than using a physical keyboard and mouse. Some computers have little or no user view. For example, embedded computers in home devices and automobiles may have numeric keypads and may turn indicator lights on or off to show status, but they and their operating systems are designed primarily to run without user intervention.
1.1.2 System View From the computer’s point of view, the operating system is the program most intimately involved with the hardware. In this context, we can view an operating system as a resource allocator. A computer system has many resources that may be required to solve a problem: CPU time, memory space,  le-storage space, I/O devices, and so on. The operating system acts as the manager of these resources. Facing numerous and possibly conﬂicting requests for resources, the operating system must decide how to allocate them to speci c programs and users so that it can operate the computer system ef ciently and fairly. As we have seen, resource allocation is especially important where many users access the same mainframe or minicomputer. A slightly different view of an operating system emphasizes the need to control the various I/O devices and user programs. An operating system is a control program. A control program manages the execution of user programs to prevent errors and improper use of the computer. It is especially concerned with the operation and control of I/O devices. 1.1.3 De ning Operating Systems By now, you can probably see that the term operating system covers many roles and functions. That is the case, at least in part, because of the myriad designs and uses of computers. Computers are present within toasters, cars, ships, spacecraft, homes, and businesses. They are the basis for game machines, music players, cable TV tuners, and industrial control systems. Although computers have a relatively short history, they have evolved rapidly. Computing started as an experiment to determine what could be done and quickly moved to  xed-purpose systems for military uses, such as code breaking and trajectory plotting, and governmental uses, such as census calculation. Those early computers evolved into general-purpose, multifunction mainframes, and that’s when operating systems were born. In the 1960s, Moore’s Law predicted that the number of transistors on an integrated circuit would double every eighteen months, and that prediction has held true. Computers gained in functionality and shrunk in size, leading to a vast number of uses and a vast number and variety of operating systems. (See Chapter 20 for more details on the history of operating systems.) How, then, can we de ne what an operating system is? In general, we have no completely adequate de nition of an operating system. Operating systems exist because they offer a reasonable way to solve the problem of creating a usable computing system. The fundamental goal of computer systems is to execute user programs and to make solving user problems easier. Computer hardware is constructed toward this goal. Since bare hardware alone is not particularly easy to use, application programs are developed. These programs require certain common operations, such as those controlling the I/O devices. The common functions of controlling and allocating resources are then brought together into one piece of software: the operating system. In addition, we have no universally accepted de nition of what is part of the operating system. A simple viewpoint is that it includes everything a vendor ships when you order “the operating system.  The features included, however, vary greatly across systems. Some systems take up less than a megabyte of space and lack even a full-screen editor, whereas others require gigabytes of space and are based entirely on graphical windowing systems. A more common de nition, and the one that we usually follow, is that the operating system is the one program running at all times on the computer—usually called the kernel. (Along with the kernel, there are two other types of programs: system programs, which are associated with the operating system but are not necessarily part of the kernel, and application programs, which include all programs not associated with the operation of the system.) The matter of what constitutes an operating system became increasingly important as personal computers became more widespread and operating systems grew increasingly sophisticated. In 1998, the United States Department of Justice  led suit against Microsoft, in essence claiming that Microsoft included too much functionality in its operating systems and thus prevented application vendors from competing. (For example, a Web browser was an integral part of the operating systems.) As a result, Microsoft was found guilty of using its operating-system monopoly to limit competition. Today, however, if we look at operating systems for mobile devices, we see that once again the number of features constituting the operating system is increasing. Mobile operating systems often include not only a core kernel but also middleware—a set of software frameworks that provide additional services to application developers. For example, each of the two most prominent mobile operating systems—Apple’s iOS and Google’s Android—features a core kernel along with middleware that supports databases, multimedia, and graphics (to name a only few). 1.2 Computer-System Organization Before we can explore the details of how computer systems operate, we need general knowledge of the structure of a computer system. In this section, we look at several parts of this structure. The section is mostly concerned with computer-system organization, so you can skim or skip it if you already understand the concepts. 1.2.1 Computer-System Operation A modern general-purpose computer system consists of one or more CPUs and a number of device controllers connected through a common bus that provides access to shared memory. Each device controller is in charge of a specific type of device. The CPU and the device controllers can execute in parallel, competing for memory cycles. To ensure orderly access to the shared memory, a memory controller synchronizes access to the memory. For a computer to start running—for instance, when it is powered up or rebooted—it needs to have an initial program to run. This initial program, or bootstrap program, tends to be simple. Typically, it is stored within the computer hardware in read-only memory (ROM) or electrically erasable programmable read-only memory (EEPROM), known by the general term firmware. It initializes all aspects of the system, from CPU registers to device controllers to memory contents. The bootstrap program must know how to load the operating system and how to start executing that system. To accomplish that, the bootstrap program locates the operating-system kernel and loads it into memory.
Once the kernel is loaded and executing, it can start providing services to the system and its users. Some services are provided outside of the kernel, by system programs that are loaded into memory at boot time to become system processes, or system daemons that run the entire time the kernel is running. On UNIX, the first system process is “init,  and it starts many other daemons. Once this phase is complete, the system is fully booted, and the system waits for some event to occur. The occurrence of an event is usually signaled by an interrupt from either the hardware or the software. Hardware may trigger an interrupt at any time by sending a signal to the CPU, usually by way of the system bus. Software may trigger an interrupt by executing a special operation called a system call (also called a monitor call). When the CPU is interrupted, it stops what it is doing and immediately transfers execution to a fixed location. The fixed location usually contains the starting address where the service routine for the interrupt is located. The interrupt service routine executes; on completion, the CPU resumes the interrupted computation. A timeline of this operation is shown in Figure 1.3. Interrupts are an important part of a computer architecture. Each computer design has its own interrupt mechanism, but several functions are common. The interrupt must transfer control to the appropriate interrupt service routine. The straightforward method for handling this transfer would be to invoke a generic routine to examine the interrupt information. The routine, in turn, would call the interrupt-specific handler. However, interrupts must be handled quickly. Since only a predefined number of interrupts is possible, a table of pointers to interrupt routines can be used instead to provide the necessary speed. The interrupt routine is called indirectly through the table, with no intermediate routine needed. Generally, the table of pointers is stored in low memory (the first hundred or so locations). These locations hold the addresses of the interrupt service routines for the various devices. This array, or interrupt vector, of addresses is then indexed by a unique device number, given with the interrupt request, to provide the address of the interrupt service routine for the interrupting device. Operating systems as different as Windows and UNIX dispatch interrupts in this manner. The interrupt architecture must also save the address of the interrupted instruction. Many old designs simply stored the interrupt address in a fixed location or in a location indexed by the device number. More recent architectures store the return address on the system stack. If the interrupt routine needs to modify the processor state—for instance, by modifying register values—it must explicitly save the current state and then restore that state before returning. After the interrupt is serviced, the saved return address is loaded into the program counter, and the interrupted computation resumes as though the interrupt had not occurred. 1.2.2 Storage Structure The CPU can load instructions only from memory, so any programs to run must be stored there. General-purpose computers run most of their programs from rewritable memory, called main memory (also called random-access memory, or RAM). Main memory commonly is implemented in a semiconductor technology called dynamic random-access memory (DRAM). Computers use other forms of memory as well. We have already mentioned read-only memory, ROM) and electrically erasable programmable read-only memory, EEPROM). Because ROM cannot be changed, only static programs, such as the bootstrap program described earlier, are stored there. The immutability of ROM is of use in game cartridges. EEPROM can be changed but cannot be changed frequently and so contains mostly static programs. For example, smartphones have EEPROM to store their factory-installed programs.
All forms of memory provide an array of bytes. Each byte has its own address. Interaction is achieved through a sequence of load or store instructions to specific memory addresses. The load instruction moves a byte or word from main memory to an internal register within the CPU, whereas the store instruction moves the content of a register to main memory. Aside from explicit loads and stores, the CPU automatically loads instructions from main memory for execution. A typical instruction–execution cycle, as executed on a system with a von Neumann architecture, first fetches an instruction from memory and stores that instruction in the instruction register. The instruction is then decoded and may cause operands to be fetched from memory and stored in some internal register. After the instruction on the operands has been executed, the result may be stored back in memory. Notice that the memory unit sees only a stream of memory addresses. It does not know how they are generated (by the instruction counter, indexing, indirection, literal addresses, or some other means) or what they are for (instructions or data). Accordingly, we can ignore how a memory address is generated by a program. We are interested only in the sequence of memory addresses generated by the running program. Ideally, we want the programs and data to reside in main memory permanently. This arrangement usually is not possible for the following two reasons: Main memory is usually too small to store all needed programs and data permanently. Main memory is a volatile storage device that loses its contents when power is turned off or otherwise lost. Thus, most computer systems provide secondary storage as an extension of main memory. The main requirement for secondary storage is that it be able to hold large quantities of data permanently. The most common secondary-storage device is a magnetic disk, which provides storage for both programs and data. Most programs (system and application) are stored on a disk until they are loaded into memory. Many programs then use the disk as both the source and the destination of their processing. Hence, the proper management of disk storage is of central importance to a computer system, as we discuss in Chapter 10. In a larger sense, however, the storage structure that we have described—consisting of registers, main memory, and magnetic disks—is only one of many possible storage systems. Others include cache memory, CD-ROM, magnetic tapes, and so on. Each storage system provides the basic functions of storing a datum and holding that datum until it is retrieved at a later time. The main differences among the various storage systems lie in speed, cost, size, and volatility. The wide variety of storage systems can be organized in a hierarchy according to speed and cost. The higher levels are expensive, but they are fast. As we move down the hierarchy, the cost per bit generally decreases, whereas the access time generally increases. This trade-off is reasonable; if a given storage system were both faster and less expensive than another—other properties being the same—then there would be no reason to use the slower, more expensive memory. In fact, many early storage devices, including paper registers, tape, and core memories, are relegated to museums now that magnetic tape and semiconductor memory have become faster and cheaper. The top four levels of memory may be constructed using semiconductor memory. In addition to differing in speed and cost, the various storage systems are either volatile or nonvolatile. As mentioned earlier, volatile storage loses its contents when the power to the device is removed. In the absence of expensive battery and generator backup systems, data must be written to nonvolatile storage for safekeeping. In the hierarchy shown, the storage systems above the solid-state disk are volatile, whereas those including the solid-state disk and below are nonvolatile. Solid-state disks have several variants but in general are faster than magnetic disks and are nonvolatile. One type of solid-state disk stores data in a large DRAM array during normal operation but also contains a hidden magnetic hard disk and a battery for backup power. If external power is interrupted, this solid-state disk’s controller copies the data from RAM to the magnetic disk. When external power is restored, the controller copies the data back into RAM. Another form of solid-state disk is flash memory, which is popular in cameras and personal digital assistants (PDAs), in robots, and increasingly for storage on general-purpose computers. Flash memory is slower than DRAM but needs no power to retain its contents. Another form of nonvolatile storage is NVRAM, which is DRAM with battery backup power. This memory can be as fast as DRAM and (as long as the battery lasts) is nonvolatile. The design of a complete memory system must balance all the factors just discussed: it must use only as much expensive memory as necessary while providing as much inexpensive, nonvolatile memory as possible. Caches can be installed to improve performance where a large disparity in access time or transfer rate exists between two components.
Storage is only one of many types of I/O devices within a computer. A large portion of operating system code is dedicated to managing I/O, both because of its importance to the reliability and performance of a system and because of the varying nature of the devices. Next, we provide an overview of I/O. A general-purpose computer system consists of CPUs and multiple device controllers that are connected through a common bus. Each device controller is in charge of a specific type of device. Depending on the controller, more than one device may be attached. For instance, seven or more devices can be attached to the small computer-systems interface (SCSI) controller. A device controller maintains some local buffer storage and a set of special-purpose registers. The device controller is responsible for moving the data between the peripheral devices that it controls and its local buffer storage. Typically, operating systems have a device driver for each device controller. This device driver understands the device controller and provides the rest of the operating system with a uniform interface to the device. To start an I/O operation, the device driver loads the appropriate registers within the device controller. The device controller, in turn, examines the contents of these registers to determine what action to take (such as "read a character from the keyboard"). The controller starts the transfer of data from the device to its local buffer. Once the transfer of data is complete, the device controller informs the device driver via an interrupt that it has finished its operation. The device driver then returns control to the operating system, possibly returning the data or a pointer to the data if the operation was a read. For other operations, the device driver returns status information. This form of interrupt-driven I/O is fine for moving small amounts of data but can produce high overhead when used for bulk data movement such as disk I/O. To solve this problem, direct memory access (DMA) is used. After setting up buffers, pointers, and counters for the I/O device, the device controller transfers an entire block of data directly to or from its own buffer storage to memory, with no intervention by the CPU. Only one interrupt is generated per block, to tell the device driver that the operation has completed, rather than the one interrupt per byte generated for low-speed devices. While the device controller is performing these operations, the CPU is available to accomplish other work. Some high-end systems use switch rather than bus architecture. On these systems, multiple components can talk to other components concurrently, rather than competing for cycles on a shared bus. In this case, DMA is even more effective. Figure 1.5 shows the interplay of all components of a computer system.
Increased reliability is a critical aspect of many computer systems. The ability to continue providing service proportionally to the level of surviving hardware is called graceful degradation. Systems that go beyond graceful degradation and can continue operation despite the failure of any single component are called fault-tolerant systems. Fault tolerance requires mechanisms to detect, diagnose, and, if possible, correct failures. The HP NonStop (formerly Tandem) system, for example, achieves fault tolerance through hardware and software duplication. The system consists of multiple pairs of CPUs working in lockstep, where both processors execute each instruction and compare the results. If the results differ, indicating a fault in one CPU, both are halted, and the process is moved to another pair of CPUs. This solution, however, is expensive due to the required special hardware and considerable duplication.
Multiprocessor systems come in two types: asymmetric multiprocessing (ASMP) and symmetric multiprocessing (SMP). ASMP assigns specific tasks to each processor, with one processor acting as the boss that schedules and allocates work to the others. On the other hand, SMP systems treat all processors as peers, with each processor capable of performing all tasks within the operating system. Virtually all modern operating systems, including Windows, Mac OS X, and Linux, provide support for SMP.
Multiprocessing can cause a system to transition from uniform memory access (UMA) to non-uniform memory access (NUMA). In UMA, access to any RAM from any CPU takes the same amount of time, while in NUMA, some parts of memory may take longer to access than others, leading to a performance penalty. Operating systems can minimize this penalty through resource management.
A recent trend in CPU design is multicore processors, where multiple computing cores are integrated onto a single chip. Multicore systems can be more efficient and consume less power than multiple single-core chips. Multicore CPUs appear to the operating system as multiple standard processors, putting pressure on operating system designers and application programmers to effectively utilize these processing cores.
Blade servers represent a newer development where multiple processor boards, I/O boards, and networking boards are housed in the same chassis. Each blade-processor board boots independently and runs its own operating system, making blade servers consist of multiple independent multiprocessor systems.
Clustered systems represent another type of multiprocessor system where multiple CPUs are gathered together. Unlike traditional multiprocessor systems, clustered systems consist of two or more individual systems or nodes connected via a LAN or a faster interconnect. These systems are considered loosely coupled, with each node being capable of running its own operating system. Clustering is often used to provide high-availability service, ensuring that service continues even if one or more systems in the cluster fail. Cluster software runs on each node, allowing them to monitor each other, detect failures, and redistribute workload as needed.
There are two main types of clustering: asymmetric and symmetric. Asymmetric clustering involves one machine in hot-standby mode while the other runs applications, with the standby machine taking over if the active server fails. In symmetric clustering, multiple hosts run applications and monitor each other, making more efficient use of available hardware.
Clusters can also be used to provide high-performance computing environments by running applications concurrently on all computers in the cluster. However, applications must be specifically designed to take advantage of parallelization, dividing tasks into separate components that run in parallel on individual computers in the cluster.
Other forms of clusters include parallel clusters, which allow multiple hosts to access the same data on shared storage, and wide-area network (WAN) clusters, which provide clustering over a wider geographical area. The structure of a clustered system typically involves multiple computers interconnected via a network, with shared storage allowing for redundancy, increased performance, and fault tolerance.
Beowulf clusters are a specific type of clustered system designed for high-performance computing tasks. They consist of commodity hardware connected via a simple local-area network and use open-source software libraries for communication between nodes. Beowulf clusters offer a low-cost strategy for building high-performance computing clusters, with some clusters built from discarded personal computers containing hundreds of nodes.
Multiprogrammed systems are designed to efficiently utilize system resources like CPU, memory, and peripheral devices. However, they do not allow for user interaction with the system. Time sharing, also known as multitasking, extends the concept of multiprogramming by allowing the CPU to switch rapidly among multiple jobs, enabling users to interact with each program while it is running.
Time sharing requires an interactive computer system that facilitates direct communication between users and the system. Users input instructions via devices like keyboards, mice, touchpads, or touch screens and expect immediate results. Response time in a time-shared system should typically be less than one second.
In a time-shared operating system, multiple users can share the computer simultaneously. Each user is provided with a small portion of CPU time, and the system switches rapidly between users' programs, giving the impression that each user has dedicated access to the entire system.
To support time sharing, the operating system uses CPU scheduling and multiprogramming techniques. Each user has at least one separate program, or process, loaded into memory. Processes execute for short durations before either completing or requiring I/O operations. The operating system manages these processes and ensures that each user gets a fair share of CPU time.
In addition to CPU scheduling and multiprogramming, time-sharing systems require memory management to handle multiple programs in memory simultaneously. Techniques like swapping and virtual memory are used to ensure reasonable response times and efficient use of memory resources.
Furthermore, time-sharing systems must provide mechanisms for file management, disk management, resource protection, job synchronization, communication, and deadlock avoidance. These mechanisms are essential for maintaining system stability, security, and performance in a multi-user environment.
Operating systems in modern computers operate in different modes: kernel mode and user mode. Kernel mode allows the operating system to execute privileged instructions and perform critical system tasks, while user mode is used for executing user applications. Transitions between these modes are managed by the hardware, and system calls are used to request services from the operating system while in user mode.
Hardware-supported dual-mode operation provides protection against errant user programs and facilitates the proper execution of the operating system. Privileged instructions can only be executed in kernel mode, preventing user programs from accessing critical system resources. When an error occurs, the hardware traps to the operating system, allowing it to handle the error gracefully and maintain system stability.
In summary, time-sharing operating systems enable multiple users to share a computer simultaneously by rapidly switching among user programs. These systems require mechanisms for CPU scheduling, multiprogramming, memory management, file management, and resource protection to ensure efficient and secure operation. Hardware-supported dual-mode operation provides protection against errors and facilitates proper system execution.
1.5.2 Timer
To ensure that the operating system maintains control over the CPU, a timer is used. The timer can be set to interrupt the computer after a specified period. A variable timer is generally implemented by a fixed-rate clock and a counter. The operating system sets the counter. When the counter reaches 0, an interrupt occurs.
Before turning over control to the user, the operating system ensures that the timer is set to interrupt. If the timer interrupts, control transfers automatically to the operating system. The operating system may treat the interrupt as a fatal error or may give the program more time. Instructions that modify the content of the timer are privileged.
We can use the timer to prevent a user program from running too long. A simple technique is to initialize a counter with the amount of time that a program is allowed to run. Every second, the timer interrupts, and the counter is decremented by 1. When the counter becomes negative, the operating system terminates the program for exceeding the assigned time limit.
1.6 Process Management
A program does nothing unless its instructions are executed by a CPU. A program in execution, as mentioned, is a process. A time-shared user program such as a compiler is a process. A word-processing program being run by an individual user on a PC is a process. A system task, such as sending output to a printer, can also be a process (or at least part of one). A process needs certain resources—including CPU time, memory, files, and I/O devices—to accomplish its task.
We emphasize that a program by itself is not a process. A program is a passive entity, like the contents of a file stored on disk, whereas a process is an active entity. A single-threaded process has one program counter specifying the next instruction to execute. A multithreaded process has multiple program counters, each pointing to the next instruction to execute for a given thread.
A process is the unit of work in a system. A system consists of a collection of processes, some of which are operating-system processes (those that execute system code) and the rest of which are user processes (those that execute user code). The operating system is responsible for tasks such as scheduling processes and threads on the CPUs, creating and deleting processes, suspending and resuming processes, providing mechanisms for process synchronization, and providing mechanisms for process communication.
1.7 Memory Management
Main memory is central to the operation of a modern computer system. It is a large array of bytes, ranging in size from hundreds of thousands to billions. Each byte has its own address. Main memory is a repository of quickly accessible data shared by the CPU and I/O devices. For a program to be executed, it must be mapped to absolute addresses and loaded into memory. As the program executes, it accesses program instructions and data from memory.
Memory management is necessary to optimize both CPU utilization and the speed of the computer’s response to its users. The operating system is responsible for tasks such as keeping track of which parts of memory are currently being used and by whom, deciding which processes and data to move into and out of memory, and allocating and deallocating memory space as needed.
1.8 Storage Management
The operating system provides a uniform, logical view of information storage by abstracting from the physical properties of storage devices to define a logical storage unit, the file. Files are mapped onto physical media by the operating system and accessed via storage devices. Storage management is crucial for organizing and accessing data efficiently.
The operating system ensures a uniform interface for users to interact with storage devices, handling tasks such as file allocation, access control, and data retrieval.
1.8.1 File-System Management
File management is one of the most visible components of an operating system. Computers can store information on several different types of physical media such as magnetic disk, optical disk, and magnetic tape. Each medium has its own characteristics and physical organization, controlled by devices like disk drives or tape drives.
A file is a collection of related information defined by its creator, commonly representing programs (both source and object forms) and data. Files may be numeric, alphabetic, alphanumeric, or binary, and they can be free-form or formatted rigidly. The concept of a file is extremely general.
The operating system implements the abstract concept of a file by managing mass-storage media and the devices that control them. Files are organized into directories to facilitate their use. Additionally, the operating system may control access to files by multiple users, specifying permissions such as read, write, or append.
The operating system is responsible for activities related to file management, including creating and deleting files, creating and deleting directories, providing primitives for manipulating files and directories, mapping files onto secondary storage, and backing up files on stable (nonvolatile) storage media.
1.8.2 Mass-Storage Management
Since main memory is insufficient to hold all data and programs, and data in main memory are volatile, secondary storage is necessary to back up main memory. Disks are commonly used as the principal online storage medium for programs and data. Proper management of disk storage is essential to computer system operation.
The operating system handles activities related to disk management, including free-space management, storage allocation, and disk scheduling. Efficient use of secondary storage is crucial for overall system performance.
Tertiary storage, which is slower and lower in cost than secondary storage, is used for purposes such as backups, storage of seldom-used data, and long-term archival storage. Tertiary storage devices include magnetic tape drives, CD and DVD drives, and optical platters.
Some operating systems manage tertiary storage, handling functions like mounting and unmounting media, allocating and freeing devices for exclusive use, and migrating data from secondary to tertiary storage. Techniques for managing secondary and tertiary storage are discussed in Chapter 10.
1.8.3 Caching
Caching is an important principle in computer systems, involving the temporary storage of frequently accessed information in a faster storage system called a cache. Caches can be implemented in hardware or software, and they improve system performance by reducing access time to frequently used data.
Main memory can be viewed as a fast cache for secondary storage, as data must be copied into main memory before use. Various levels of the storage hierarchy, including registers, cache, main memory, solid-state disks, and magnetic disks, are managed by the operating system to optimize performance.
Cache management involves careful selection of cache size and replacement policy to maximize performance. Software-controlled caches use replacement algorithms to decide which data to keep in the cache. Main memory, acting as a cache for secondary storage, ensures efficient data access for running programs.
1.8.4 I/O Systems
One of the primary functions of an operating system is to shield users from the intricacies of specific hardware devices. The I/O subsystem accomplishes this task by abstracting and managing various components:
- Memory-management component: This component handles buffering, caching, and spooling, optimizing data transfer between main memory and I/O devices.
- General device-driver interface: The interface allows device drivers to communicate with the operating system and facilitates the management of hardware devices.
- Device drivers: These drivers are responsible for controlling specific hardware devices, possessing knowledge of the device's unique characteristics.
The I/O subsystem efficiently coordinates interrupt handlers, device drivers, and other system components to ensure seamless data transfer and device management. Chapter 13 explores how the I/O subsystem interacts with other system elements, manages devices, transfers data, and handles I/O completion.
1.9 Protection and Security
In a multi-user and multi-process computing environment, access to data and resources must be regulated to maintain system integrity and security. Various mechanisms ensure that only authorized processes can operate on system resources, such as files, memory segments, and the CPU.
Protection mechanisms control access to resources and enforce access controls specified by the operating system. They play a critical role in improving system reliability and preventing unauthorized usage. Chapter 14 delves into the design and implementation of protection mechanisms, highlighting their significance in maintaining system integrity.
Despite adequate protection mechanisms, systems remain vulnerable to external and internal threats, including viruses, denial-of-service attacks, identity theft, and unauthorized system usage. Security measures aim to defend against such attacks, safeguarding the system and its resources from malicious activities. Chapter 15 explores security features and strategies to mitigate security risks in operating systems.
In the course of normal system use, the user ID and group ID for a user are generally sufficient. However, there are instances when a user needs to elevate privileges to gain extra permissions for specific activities, such as accessing restricted devices. Operating systems offer various methods for privilege escalation. For example, on UNIX systems, the setuid attribute on a program enables it to run with the user ID of the file owner instead of the current user's ID. The process retains this effective UID until it either relinquishes the additional privileges or terminates.
1.10 Kernel Data Structures
Next, we explore a central topic in operating-system implementation: the organization of data within the system. This section provides a brief overview of several fundamental data structures extensively used in operating systems. Readers seeking further details on these structures can refer to the bibliography at the end of the chapter.
1.10.1 Lists, Stacks, and Queues
Arrays, while simple, may not always suffice for storing data, especially when item sizes vary or when preserving relative item positions is crucial. Lists offer a solution by representing a sequence of data values. Linked lists, a common implementation of lists, come in various forms:
- Singly linked lists: Each item points to its successor.
- Doubly linked lists: Each item can refer to its predecessor and successor.
- Circularly linked lists: The last element in the list refers to the first element instead of null.
Linked lists accommodate variable-sized items and facilitate easy insertion and deletion. However, retrieving a specified item in a list of size n may require linear time (O(n)), as all elements may need traversal. Lists are often used directly in kernel algorithms or as building blocks for more complex structures like stacks and queues.
A stack follows the last in, first out (LIFO) principle, where the last item added is the first to be removed. It is commonly used in function call invocation in operating systems.
In contrast, a queue adheres to the first in, first out (FIFO) principle, where items are removed in the order of insertion. Queues find applications in scenarios such as job scheduling and task management.
1.10.2 Trees
Trees represent hierarchical data structures, where data values are linked through parent-child relationships. Binary trees, a type of tree structure, limit each parent to at most two children. Binary search trees impose an ordering between a parent's left and right children, facilitating efficient searching.
1.10.3 Hash Functions and Maps
Hash functions transform input data into numeric values, which serve as indices for quick data retrieval from tables. This approach offers efficient searching, with worst-case performance often approaching O(1), depending on implementation. Operating systems leverage hash functions extensively for various tasks.
One potential difficulty with hash functions is the possibility of hash collisions, where two inputs produce the same output value, leading to the same table location. Hash collisions can be accommodated by using a linked list at the table location, containing all items with the same hash value. However, the efficiency of the hash function decreases with more collisions.
Hash functions find application in implementing hash maps, which associate key-value pairs using a hash function. For example, a key like "operating" can be mapped to the value "system" using a hash map. Once established, applying the hash function to the key retrieves the corresponding value from the hash map. For instance, in password authentication, the hash function is applied to the user name to retrieve the associated password for comparison.
1.10.4 Bitmaps
A bitmap is a string of binary digits used to represent the status of items. Each binary digit in the bitmap corresponds to an item's availability, with 0 indicating availability and 1 indicating unavailability. Bitmaps are space-efficient and are commonly used to represent the availability of a large number of resources. For example, in disk drives, a bitmap can indicate the availability of each disk block, where each bit represents the availability status of a block.
Data structures like lists, stacks, queues, trees, and bitmaps are integral to operating system implementations. These structures are utilized in various kernel algorithms and implementations, contributing to the efficient functioning of the operating system.
LINUX KERNEL DATA STRUCTURES
The Linux kernel utilizes various data structures available in the kernel source code. For instance, the linked-list data structure is extensively used throughout the kernel, with details provided in the <linux/list.h> include file. Linux implements queues using kfifo, while a balanced binary search tree implementation using red-black trees can be found in the <linux/rbtree.h> include file.
1.11 Computing Environments
Operating systems are used across diverse computing environments, each with its unique characteristics and requirements. Traditional computing environments have evolved significantly over time, with boundaries between them becoming increasingly blurred. For instance, the typical office environment now incorporates various technologies, including networked PCs, servers, web portals, and mobile devices.
At home, advancements in network connectivity have enabled users to access data and services more efficiently. High-speed data connections have facilitated the setup of home networks, allowing users to share resources like printers and servers. Firewalls are commonly used to protect home networks from security threats.
Historically, computing resources were scarce, leading to batch and interactive systems. Batch systems processed jobs in bulk, while interactive systems waited for user input. Time-sharing systems optimized resource utilization by allowing multiple users to share computing resources through rapid cycling of processes.
1.11.5 Peer-to-Peer Computing
Another model for distributed systems is the peer-to-peer (P2P) system, where all nodes are considered peers and can act as both clients and servers. Unlike traditional client-server systems, where the server can be a bottleneck, P2P systems distribute services across multiple nodes, improving scalability and fault tolerance.
In P2P systems, nodes must first join the network to provide and request services from other peers. There are two general approaches for discovering services:
- Centralized Lookup Service: When a node joins the network, it registers its services with a centralized lookup service. Other nodes can then query this service to find the provider of a desired service.